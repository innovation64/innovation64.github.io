---
tags: LLM
---

# 伯克利实验室系列
## 现实字节：如何在LLMs中衡量幻觉

>这篇文章是 2023 年 10 月由 Justin Wong 撰写

想象你向你的祖母询问她著名的草莓大黄派食谱。她偶尔会口误，告诉你“打破草莓”，而她可能实际上想说“切草莓”。与关于她大学时代的轻松故事不同，你需要准确的信息才能在这个周末真正地烤派，所以你意识到她口误，并要求她澄清。语言模型有类似的“口误”或幻觉倾向，这导致在 API 使用上存在挑战，因为必须采取具体行动。在 Gorilla 中，想象我们向LLM“生成一个用于识别成熟草莓的视觉模型的 API 调用”。以下情况之一可能发生

- 我们很幸运！API 调用选择了最先进的分类器：CLIP。
- 我们调用了一个有效的 API，但却是错误的:(LLM是不准确的。这意味着它没有准确地理解用户的指定。例如，模型可能是一个文本嵌入模型：`torch.hub.load('pytorch/vision:v0.10.0', 'RoBERTa')`
- 愿望式思考！LLM幻想出一个不存在的模型：BerryPicker。`torch.hub.load('pytorch/vision:v0.10.0', 'BerryPicker')`


对于非任务驱动型应用，只有在提出可证伪的声明时才能检测到幻觉，例如“查尔斯·狄更斯写了《麦克白》”。相比之下，LLM生成的 API 调用必须完成特定任务，因此是可证伪的。这使我们能够衡量幻觉！我们在本博客文章中讨论了如何通过识别当LLM提供未定义或不受 API 支持的 API 调用或参数的情况来衡量幻觉

## 计算机如何表示 API 调用

API 调用是一种函数调用。关于如何表示程序的问题可以追溯到 20 世纪 60 年代，当时唐纳德·克努特（TeX 的发明者）引入了抽象语法树（AST）。从那时起，ASTs 已经成为一种强大的程序表示，用于检查程序是否等价以及应用自动程序优化。编译器设计者可以在 AST 上定义重写，并确信程序的正确性不会受到写等价证明的影响。我们通过简化默认参数来阐述这个过程。假设`pretrained`参数默认为 true。那么，`API_Function(my_model, pretrained=True)`和`API_Function(my_model)`在直观上是等价的。然而，计算机看到的是两个明显不同的字符序列。通过将其表示为 AST，我们可以应用一个保持等价的重写，将默认参数填充到树中。现在，很容易检查表示函数调用的 AST 是否等价，这意味着原始函数调用是等价的。

## 测量幻觉在LLMs

任何单个 API 调用都可以唯一地解析或表示为抽象语法树（AST）。在 AST 中，节点代表每个参数或类函数。例如，下面的图像显示了使用`torch.hub.load`加载`ResNet50`生成的 AST：```torch.hub.load(‘pytorch/vision:v0.10.0’，‘ResNet50’，pretrained=True)```然而，我们注意到`pretrained`是一个辅助参数，对 API 的有效性不是至关重要的。我们可以通过仔细整理所有基本 API 函数调用（如`torch.hub.load(‘pytorch/vision:v0.10.0’，‘ResNet50’)`）来检查幻觉。这些基本 API 函数调用可以标准化为```API_name(API_arg1, API_arg2, ..., API_argk)```标准化使我们能够检查生成的 AST 是否调用了基本 API 函数调用之一。特别是对于深度学习 API，字符串/路径用于表示模型和存储库。这种自由文本格式降低了添加新模型的摩擦，但限制了可以进行的程序分析。对于深度学习 API，一个关键的有效性检查是模型是否存在于存储库中。在支持 94 个模型列表的 torch.hub API 的情况下，我们检查 API 调用是否调用了现有的模型之一。 AST 子树匹配可以轻松检查模型和存储库是否是 API 调用精选列表中的有效配对。ASTs 是一种标准且强大的结构化表示，它允许我们进行子树匹配，以高效地检查 API 调用是否是幻觉。

![](https://gorilla.cs.berkeley.edu/assets/img/blog_post_1_result.png)

## 现实字节：当LLMs看到不存在的事物

幻觉是所有关于LLMs讨论的中心。在 API 生成的情况下，幻觉可以被定义为生成不存在的 API 调用的模型。LLM生成可能是不准确的，也可能是幻觉。一个并不意味着另一个。例如，如果用户请求用于医学图像的分类器，如果模型生成 Stripe API 调用的图像分类器，那么这就是幻觉，因为它不存在！另一方面，如果模型建议使用 Stripe API 来检查您的余额，那么这是 API 的错误使用，但至少不是虚构的（没有幻觉）。在我们的博客中，我们描述了 Gorilla 使用抽象语法树（AST）来衡量生成 API 调用幻觉的创新方法。虽然不能推广到所有任务，但据我们所知，Gorilla 是第一个衡量和量化LLM生成幻觉的！

## Gorilla Hallucinations 大猩猩幻觉


优秀的检索器可以减少幻觉。概述的 AST 子树匹配方法突出了在减少幻觉中质量检索的重要性。我们在三个检索设置上评估了 Gorilla：1）BM-25，一种经典的基于关键词的检索器，2）GPT-Index，在 gpt 文档嵌入上的最近邻搜索，以及 3）Oracle，真实情况。每个检索器都将一个 API 调用的文档添加到上下文中。随着检索精度从 BM-25 到 GPT-index 的提高，我们看到 Gorilla 几乎消除了幻觉。GPT-4 也从检索质量中获得了强大的益处，导致幻觉减少了 91%。值得注意的是，GPT-4 比 Claude 从检索中获益更多，这表明模型可以被优化以更好地利用检索到的文档！在此了解更多关于我们的检索增强生成方法。

## Takeaways 收获

语言模型就像人类一样，会产生幻觉并无意中做出错误陈述。尽管我们的重点是LLM API，但据我们所知，Gorilla 是第一个为LLM代测量和量化幻觉的！“尽管我们的重点是LLM API，但据我们所知，Gorilla 是第一个为LLM代测量和量化幻觉的！”以下是关于测量幻觉的一些要点：

- 使用 AST 来衡量 API 幻觉。对于LLM API，我们引入了一种新的方法，通过抽象语法树（AST）的子树匹配来检查幻觉。检索器减少了幻觉。我们发现提高检索器可以减少幻觉！
- 解决问题的第一步是认识到问题的存在。我们对自动修复和自我修复的潜力感到兴奋，当 API 调用出现幻觉时。就像你会问奶奶是否真的建议切草莓一样，我们设想未来的语言模型驱动的 API 调用将提出有针对性的澄清问题。

