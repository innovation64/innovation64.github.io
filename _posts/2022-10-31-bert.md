---
tags: GLM
---
# Bertå®æˆ˜
>æœ¬æ–‡å¤§éƒ¨åˆ†å‚ç…§äºå†¬çš„åšå®¢ ä»…ä¾›è‡ªå·±å‚è€ƒ
https://ifwind.github.io/2021/08/26/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%881%EF%BC%89%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/
## æ–‡æœ¬åˆ†ç±»

>å¼•è¨€ï¼šæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ ğŸ¤— [Transformers](https://github.com/huggingface/transformers)ä»£ç åº“ä¸­çš„æ¨¡å‹æ¥è§£å†³æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œä»»åŠ¡æ¥æºäº[GLUE Benchmark](https://gluebenchmark.com/).
- ä»»åŠ¡ä»‹ç»
  æœ¬è´¨å°±æ˜¯åˆ†ç±»é—®é¢˜ï¼Œæ¯”å¦‚å¯¹ä¸€å¥è¯çš„æƒ…æ„Ÿææ€§åˆ†ç±»ï¼ˆæ­£å‘1æˆ–è´Ÿå‘-1æˆ–ä¸­æ€§0ï¼‰ï¼š

### æ­¥éª¤
- æ•°æ®åŠ è½½
- æ•°æ®é¢„å¤„ç†
- å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼šä½¿ç”¨transformerä¸­çš„Traineræ¥å£å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒ
- è¶…å‚æ•°æœç´¢

### å‰æœŸå‡†å¤‡
å®‰è£…ä»¥ä¸‹ä¸¤ä¸ªåº“
```python
pip install datasets transformers
```
### æ•°æ®åŠ è½½
#### æ•°æ®é›†ä»‹ç»
- GLUE
```python 
GLUE_TASKS = ["cola", "mnli", "mnli-mm", "mrpc", "qnli", "qqp", "rte", "sst2", "stsb", "wnli"]
```
|åˆ†ç±»ä»»åŠ¡|ä»»åŠ¡ç›®æ ‡|
|--|--|
|CoLA (Corpus of Linguistic Acceptability)|é‰´åˆ«ä¸€ä¸ªå¥å­æ˜¯å¦è¯­æ³•æ­£ç¡®.|
|MNLI (Multi-Genre Natural Language Inference)|ç»™å®šä¸€ä¸ªå‡è®¾ï¼Œåˆ¤æ–­å¦ä¸€ä¸ªå¥å­ä¸è¯¥å‡è®¾çš„å…³ç³»ï¼šentails, contradicts æˆ–è€… unrelatedã€‚|
|MRPC (Microsoft Research Paraphrase Corpus)|åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯å¦äº’ä¸ºparaphrasesæ”¹å†™.|
|QNLI (Question-answering Natural Language Inference)|	åˆ¤æ–­ç¬¬2å¥æ˜¯å¦åŒ…å«ç¬¬1å¥é—®é¢˜çš„ç­”æ¡ˆã€‚|
|QQP (Quora Question Pairs2)|	åˆ¤æ–­ä¸¤ä¸ªé—®å¥æ˜¯å¦è¯­ä¹‰ç›¸åŒã€‚|
|RTE (Recognizing Textual Entailment)|	åˆ¤æ–­ä¸€ä¸ªå¥å­æ˜¯å¦ä¸å‡è®¾æˆentailå…³ç³»ã€‚|
|SST-2 (Stanford Sentiment Treebank)|	åˆ¤æ–­ä¸€ä¸ªå¥å­çš„æƒ…æ„Ÿæ­£è´Ÿå‘.|
|STS-B (Semantic Textual Similarity Benchmark)|åˆ¤æ–­ä¸¤ä¸ªå¥å­çš„ç›¸ä¼¼æ€§ï¼ˆåˆ†æ•°ä¸º1-5åˆ†ï¼‰ã€‚|
|WNLI (Winograd Natural Language Inference)|åˆ¤æ–­ä¸€ä¸ªæœ‰åŒ¿åä»£è¯çš„å¥å­å’Œä¸€ä¸ªæœ‰è¯¥ä»£è¯è¢«æ›¿æ¢çš„å¥å­æ˜¯å¦åŒ…å«ã€‚Determine if a sentence with an anonymous pronoun and a sentence with this pronoun replaced are entailed or not.|

### æ•°æ®åŠ è½½
[å®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/datasets/loading_datasets.html#from-local-files)
- åŠ è½½å®˜æ–¹æ•°æ®
é™¤äº†`mbli-mm`å¤–,å…¶ä»–ä»»åŠ¡éƒ½å¯ä»¥ç›´æ¥é€šè¿‡ä»»åŠ¡åå­—è¿›è¡ŒåŠ è½½ã€‚æ•°æ®åŠ è½½ä¹‹åä¼šè‡ªåŠ¨ç¼“å­˜
```python
from datasets import load_dataset
actual_task = "mnli" if task == "mnli-mm" else task
dataset = load_dataset("glue", actual_task)
metric = load_metric('glue', actual_task)
```
è¿™ä¸ªdatasetså¯¹è±¡æœ¬èº«æ˜¯ä¸€ç§DatasetDictæ•°æ®ç»“æ„. å¯¹äºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼Œåªéœ€è¦ä½¿ç”¨å¯¹åº”çš„keyï¼ˆtrainï¼Œvalidationï¼Œtestï¼‰å³å¯å¾—åˆ°ç›¸åº”çš„æ•°æ®ã€‚
ç»™å®šä¸€ä¸ªæ•°æ®åˆ‡åˆ†çš„keyï¼ˆtrainã€validationæˆ–è€…testï¼‰å’Œä¸‹æ ‡å³å¯æŸ¥çœ‹æ•°æ®ï¼šdataset["train"][0]

ä¸‹é¢çš„å‡½æ•°å°†ä»æ•°æ®é›†é‡Œéšæœºé€‰æ‹©å‡ ä¸ªä¾‹å­è¿›è¡Œå±•ç¤ºï¼š
```
import datasets
import random
import pandas as pd
from IPython.display import display, HTML

def show_random_elements(dataset, num_examples=10):
    assert num_examples <= len(dataset), "Can't pick more elements than there are in the dataset."
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset)-1)
        while pick in picks:
            pick = random.randint(0, len(dataset)-1)
        picks.append(pick)
    
    df = pd.DataFrame(dataset[picks])
    for column, typ in dataset.features.items():
        if isinstance(typ, datasets.ClassLabel):
            df[column] = df[column].transform(lambda i: typ.names[i])
    display(HTML(df.to_html()))
    
show_random_elements(dataset["train"])
```
- åŠ è½½è‡ªå·±çš„æ•°æ®
  - csvæ ¼å¼
  data_filesä¸ºæœ¬åœ°æ–‡ä»¶åæˆ–ç½‘ç»œæ•°æ®é“¾æ¥ï¼Œå¦‚æœæ²¡æœ‰ç”¨å­—å…¸æŒ‡å®šè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†ï¼Œé»˜è®¤éƒ½ä¸ºè®­ç»ƒé›†ã€‚
    ```python
    from datasets import load_dataset
    dataset = load_dataset('csv', data_files='my_file.csv')
    dataset = load_dataset('csv', data_files=['my_file_1.csv', 'my_file_2.csv', 'my_file_3.csv'])
    dataset = load_dataset('csv', data_files={'train': ['my_train_file_1.csv', 'my_train_file_2.csv'],
    base_url = 'https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/'
    dataset = load_dataset('csv', data_files={'train': base_url + 'train.csv', 'test': base_url + 'test.csv'})
    ```
  - jsonæ ¼å¼
    æƒ…å†µ1ï¼šjsonæ•°æ®ä¸åŒ…æ‹¬åµŒå¥—çš„jsonï¼Œæ¯”å¦‚ï¼š
    ```
    {"a": 1, "b": 2.0, "c": "foo", "d": false}
    {"a": 4, "b": -5.5, "c": null, "d": true}
    ```
    æ¬¡æ•°å¯ä»¥ç›´æ¥åŠ è½½æ•°æ®
    ```python
    from datasets import load_dataset
    dataset = load_dataset('json', data_files={'train': ['my_text_1.json', 'my_text_2.json'], 'test': 'my_test_file.json'})

    dataset = load_dataset('text', data_files={'train': 'https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.json'})
    ```
    æƒ…å†µäºŒï¼šjsonæ•°æ®åŒ…æ‹¬åµŒå¥—çš„jsonï¼Œæ¯”å¦‚ï¼š
    ```
    {"version": "0.1.0",
    "data": [{"a": 1, "b": 2.0, "c": "foo", "d": false},
            {"a": 4, "b": -5.5, "c": null, "d": true}]
        }
    ```
    æ­¤æ—¶éœ€è¦ä½¿ç”¨ field å‚æ•°æŒ‡å®šå“ªä¸ªå­—æ®µåŒ…å«æ•°æ®é›†ï¼š
    ```python
    from datasets import load_dataset
    dataset = load_dataset('json', data_files='my_file.json', field='data')
    ```
  - txtæ ¼å¼
    ```python
    from datasets import load_dataset
        dataset = load_dataset('text', data_files={'train': ['my_text_1.txt', 'my_text_2.txt'], 'test': 'my_test_file.txt'})

        dataset = load_dataset('text', data_files={'train': 'https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt'})
    ```
  - dictæ ¼å¼
    ```python
    my_dict = {'id': [0, 1, 2],
            'name': ['mary', 'bob', 'eve'],
            'age': [24, 53, 19]}
        from datasets import Dataset
        dataset = Dataset.from_dict(my_dict)
    ```
  - pandas.DataFrameæ ¼å¼
    ```python
        from datasets import Dataset
        import pandas as pd
        df = pd.DataFrame({"a": [1, 2, 3]})
        dataset = Dataset.from_pandas(df)
    ```
### æ•°æ®é¢„å¤„ç†
åœ¨å°†æ•°æ®å–‚å…¥æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€‚ä¹‹å‰æˆ‘ä»¬å·²ç»çŸ¥é“äº†æ•°æ®é¢„å¤„ç†çš„åŸºæœ¬æµç¨‹ï¼š
- åˆ†è¯
- è½¬åŒ–æˆå¯¹åº”ä»»åŠ¡è¾“å…¥æ¨¡å‹çš„æ ¼å¼
`Tokenizer`ç”¨äºä¸Šé¢ä¸¤æ­¥æ•°æ®é¢„å¤„ç†å·¥ä½œï¼š`Tokenizer`é¦–å…ˆå¯¹è¾“å…¥è¿›è¡Œ`tokenize`ï¼Œç„¶åå°†`tokens`è½¬åŒ–ä¸ºé¢„æ¨¡å‹ä¸­éœ€è¦å¯¹åº”çš„`token ID`ï¼Œå†è½¬åŒ–ä¸ºæ¨¡å‹éœ€è¦çš„è¾“å…¥æ ¼å¼ã€‚
#### åˆå§‹åŒ–Tokenizer
ä½¿ç”¨`AutoTokenizer.from_pretrained`æ–¹æ³•æ ¹æ®æ¨¡å‹æ–‡ä»¶å®ä¾‹åŒ–tokenizerï¼Œè¿™æ ·å¯ä»¥ç¡®ä¿ï¼š
- å¾—åˆ°ä¸€ä¸ªä¸é¢„è®­ç»ƒæ¨¡å‹ä¸€ä¸€å¯¹åº”çš„tokenizerã€‚
- ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹checkpointå¯¹åº”çš„tokenizeræ—¶ï¼ŒåŒæ—¶ä¸‹è½½äº†æ¨¡å‹éœ€è¦çš„è¯è¡¨åº“vocabularyï¼Œå‡†ç¡®æ¥è¯´æ˜¯tokens vocabularyã€‚
  ```
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)
  ```
>æ³¨æ„ï¼šuse_fast=Trueè¦æ±‚tokenizerå¿…é¡»æ˜¯transformers.PreTrainedTokenizerFastç±»å‹ï¼Œä»¥ä¾¿åœ¨é¢„å¤„ç†çš„æ—¶å€™éœ€è¦ç”¨åˆ°fast tokenizerçš„ä¸€äº›ç‰¹æ®Šç‰¹æ€§ï¼ˆæ¯”å¦‚å¤šçº¿ç¨‹å¿«é€Ÿtokenizerï¼‰ã€‚å¦‚æœå¯¹åº”çš„æ¨¡å‹æ²¡æœ‰fast tokenizerï¼Œå»æ‰è¿™ä¸ªé€‰é¡¹å³å¯ã€‚
å‡ ä¹æ‰€æœ‰æ¨¡å‹å¯¹åº”çš„tokenizeréƒ½æœ‰å¯¹åº”çš„fast tokenizerï¼Œå¯ä»¥åœ¨æ¨¡å‹tokenizerå¯¹åº”è¡¨é‡ŒæŸ¥çœ‹æ‰€æœ‰é¢„è®­ç»ƒæ¨¡å‹å¯¹åº”çš„tokenizeræ‰€æ‹¥æœ‰çš„ç‰¹ç‚¹ã€‚

#### Tokenizeråˆ†è¯ç¤ºä¾‹
é¢„è®­ç»ƒçš„Tokenizeré€šå¸¸åŒ…å«äº†åˆ†å•å¥å’Œåˆ†ä¸€å¯¹å¥å­çš„å‡½æ•°ã€‚å¦‚ï¼š

```python
#åˆ†å•å¥ï¼ˆä¸€ä¸ªbatchï¼‰
batch_sentences = ["Hello I'm a single sentence",
                   "And another sentence",
                   "And the very very last one"]
encoded_inputs = tokenizer(batch_sentences)
print(encoded_inputs)
#{'input_ids': [[101, 8667, 146, 112, 182, 170, 1423, 5650, 102],
#               [101, 1262, 1330, 5650, 102],
#               [101, 1262, 1103, 1304, 1304, 1314, 1141, 102]],
# 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0],
#                    [0, 0, 0, 0, 0],
#                    [0, 0, 0, 0, 0, 0, 0, 0]],
# 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1],
#                    [1, 1, 1, 1, 1],
#                    [1, 1, 1, 1, 1, 1, 1, 1]]}
```

```python
#åˆ†ä¸€å¯¹å¥å­
encoded_input = tokenizer("How old are you?", "I'm 6 years old")
print(encoded_input)
#{'input_ids': [101, 1731, 1385, 1132, 1128, 136, 102, 146, 112, 182, 127, #1201, 1385, 102],
# 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
# 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

æˆ‘ä»¬ä¹‹å‰ä¹Ÿæåˆ°å¦‚æœæ˜¯è‡ªå·±é¢„è®­ç»ƒçš„tokenizerså¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¸ºtokenizerså¢åŠ å¤„ç†ä¸€å¯¹å¥å­çš„æ–¹æ³•ï¼š


```python
from tokenizers.processors import TemplateProcessing

tokenizer.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
    special_tokens=[
        ("[CLS]", tokenizer.token_to_id("[CLS]")),
        ("[SEP]", tokenizer.token_to_id("[SEP]")),
    ],
)
#è®¾ç½®å¥å­æœ€å¤§é•¿åº¦
tokenizer.enable_truncation(max_length=512)
#ä½¿ç”¨tokenizer.save()ä¿å­˜æ¨¡å‹
tokenizer.save("data/tokenizer-wiki.json")
```

#### è½¬åŒ–æˆå¯¹åº”ä»»åŠ¡è¾“å…¥æ¨¡å‹çš„æ ¼å¼
tokenizeræœ‰ä¸åŒçš„è¿”å›å–å†³äºé€‰æ‹©çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œtokenizerå’Œé¢„è®­ç»ƒæ¨¡å‹æ˜¯ä¸€ä¸€å¯¹åº”çš„ï¼Œæ›´å¤šä¿¡æ¯å¯ä»¥åœ¨[è¿™é‡Œ](https://huggingface.co/transformers/preprocessing.html)è¿›è¡Œå­¦ä¹ ã€‚
ä¸åŒæ•°æ®å’Œå¯¹åº”çš„æ•°æ®æ ¼å¼ï¼Œä¸ºäº†é¢„å¤„ç†æˆ‘ä»¬çš„æ•°æ®ï¼Œå®šä¹‰ä¸‹é¢è¿™ä¸ªdictï¼Œä»¥ä¾¿åˆ†åˆ«ç”¨tokenizerå¤„ç†è¾“å…¥æ˜¯å•å¥æˆ–å¥å­å¯¹çš„æƒ…å†µã€‚

```python
task_to_keys = {
    "cola": ("sentence", None),
    "mnli": ("premise", "hypothesis"),
    "mnli-mm": ("premise", "hypothesis"),
    "mrpc": ("sentence1", "sentence2"),
    "qnli": ("question", "sentence"),
    "qqp": ("question1", "question2"),
    "rte": ("sentence1", "sentence2"),
    "sst2": ("sentence", None),
    "stsb": ("sentence1", "sentence2"),
    "wnli": ("sentence1", "sentence2"),
}
```
å°†é¢„å¤„ç†çš„ä»£ç æ”¾åˆ°ä¸€ä¸ªå‡½æ•°ä¸­ï¼š
```python
def preprocess_function(examples):
    if sentence2_key is None:
        return tokenizer(examples[sentence1_key], truncation=True)
    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)
```
å‰é¢æˆ‘ä»¬å·²ç»å±•ç¤ºäº†tokenizerå¤„ç†ä¸€ä¸ªå°batchçš„æ¡ˆä¾‹ã€‚datasetç±»ç›´æ¥ç”¨ç´¢å¼•å°±å¯ä»¥å–å¯¹åº”ä¸‹æ ‡çš„å¥å­1å’Œå¥å­2ï¼Œå› æ­¤ä¸Šé¢çš„é¢„å¤„ç†å‡½æ•°æ—¢å¯ä»¥å¤„ç†å•ä¸ªæ ·æœ¬ï¼Œä¹Ÿå¯ä»¥å¯¹å¤šä¸ªæ ·æœ¬è¿›è¡Œå¤„ç†ã€‚å¦‚æœè¾“å…¥æ˜¯å¤šä¸ªæ ·æœ¬ï¼Œé‚£ä¹ˆè¿”å›çš„æ˜¯ä¸€ä¸ªlistï¼š
```python
preprocess_function(dataset['train'][:5])
#{'input_ids': [[101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102], [101, 2028, 2062, 18404, 2236, 3989, 1998, 1045, 1005, 1049, 3228, 2039, 1012, 102], [101, 2028, 2062, 18404, 2236, 3989, 2030, 1045, 1005, 1049, 3228, 2039, 1012, 102], [101, 1996, 2062, 2057, 2817, 16025, 1010, 1996, 13675, 16103, 2121, 2027, 2131, 1012, 102], [101, 2154, 2011, 2154, 1996, 8866, 2024, 2893, 14163, 8024, 3771, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}
```
æ¥ä¸‹æ¥ä½¿ç”¨mapå‡½æ•°å¯¹æ•°æ®é›†datasetsé‡Œé¢ä¸‰ä¸ªæ ·æœ¬é›†åˆçš„æ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œå°†é¢„å¤„ç†å‡½æ•°prepare_train_featuresåº”ç”¨åˆ°ï¼ˆmap)æ‰€æœ‰æ ·æœ¬ä¸Šã€‚
```python
encoded_dataset = dataset.map(preprocess_function, batched=True)
```
>è¿”å›çš„ç»“æœä¼šè‡ªåŠ¨è¢«ç¼“å­˜ï¼Œé¿å…ä¸‹æ¬¡å¤„ç†çš„æ—¶å€™é‡æ–°è®¡ç®—ï¼ˆä½†æ˜¯ä¹Ÿè¦æ³¨æ„ï¼Œå¦‚æœè¾“å…¥æœ‰æ”¹åŠ¨ï¼Œå¯èƒ½ä¼šè¢«ç¼“å­˜å½±å“ï¼ï¼‰ã€‚
>datasetsåº“å‡½æ•°ä¼šå¯¹è¾“å…¥çš„å‚æ•°è¿›è¡Œæ£€æµ‹ï¼Œåˆ¤æ–­æ˜¯å¦æœ‰å˜åŒ–ï¼Œå¦‚æœæ²¡æœ‰å˜åŒ–å°±ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼Œå¦‚æœæœ‰å˜åŒ–å°±é‡æ–°å¤„ç†ã€‚ä½†å¦‚æœè¾“å…¥å‚æ•°ä¸å˜ï¼Œæƒ³æ”¹å˜è¾“å…¥çš„æ—¶å€™ï¼Œæœ€å¥½æ¸…ç†è°ƒè¿™ä¸ªç¼“å­˜ï¼ˆä½¿ç”¨load_from_cache_file=Falseå‚æ•°ï¼‰ã€‚å¦å¤–ï¼Œä¸Šé¢ä½¿ç”¨åˆ°çš„batched=Trueè¿™ä¸ªå‚æ•°æ˜¯tokenizerçš„ç‰¹ç‚¹ï¼Œè¿™ä¼šä½¿ç”¨å¤šçº¿ç¨‹åŒæ—¶å¹¶è¡Œå¯¹è¾“å…¥è¿›è¡Œå¤„ç†ã€‚
### å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹
æ•°æ®å·²ç»å‡†å¤‡å¥½äº†ï¼Œæˆ‘ä»¬éœ€è¦ä¸‹è½½å¹¶åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œç„¶åå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚
#### åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
æ—¢ç„¶æ˜¯åšseq2seqä»»åŠ¡ï¼Œé‚£ä¹ˆéœ€è¦ä¸€ä¸ªèƒ½è§£å†³è¿™ä¸ªä»»åŠ¡çš„æ¨¡å‹ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨`AutoModelForSequenceClassification` è¿™ä¸ªç±»ã€‚
å’Œtokenizerç›¸ä¼¼ï¼Œfrom_pretrainedæ–¹æ³•åŒæ ·å¯ä»¥å¸®åŠ©ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹ï¼ŒåŒæ—¶ä¹Ÿä¼šå¯¹æ¨¡å‹è¿›è¡Œç¼“å­˜ï¼Œä¹Ÿå¯ä»¥å¡«å…¥ä¸€ä¸ªåŒ…æ‹¬æ¨¡å‹ç›¸å…³æ–‡ä»¶çš„æ–‡ä»¶å¤¹ï¼ˆæ¯”å¦‚è‡ªå·±é¢„è®­ç»ƒçš„æ¨¡å‹ï¼‰ï¼Œè¿™æ ·ä¼šä»æœ¬åœ°ç›´æ¥åŠ è½½ã€‚ç†è®ºä¸Šå¯ä»¥ä½¿ç”¨å„ç§å„æ ·çš„transformeræ¨¡å‹ï¼ˆæ¨¡å‹é¢æ¿ï¼‰ï¼Œè§£å†³ä»»ä½•æ–‡æœ¬åˆ†ç±»åˆ†ç±»ä»»åŠ¡ã€‚
éœ€è¦æ³¨æ„çš„æ˜¯ï¼šSTS-Bæ˜¯ä¸€ä¸ªå›å½’é—®é¢˜ï¼ŒMNLIæ˜¯ä¸€ä¸ª3åˆ†ç±»é—®é¢˜ï¼š
```python
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

task = "cola"
model_checkpoint = "distilbert-base-uncased" #æ‰€é€‰æ‹©çš„é¢„è®­ç»ƒæ¨¡å‹

num_labels = 3 if task.startswith("mnli") else 1 if task=="stsb" else 2
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)
```
ç”±äºæˆ‘ä»¬çš„ä»»åŠ¡æ˜¯æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œè€Œæˆ‘ä»¬åŠ è½½çš„æ˜¯é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œæ‰€ä»¥ä¼šæç¤ºæˆ‘ä»¬åŠ è½½æ¨¡å‹çš„æ—¶å€™æ‰”æ‰äº†ä¸€äº›ä¸åŒ¹é…çš„ç¥ç»ç½‘ç»œå‚æ•°ï¼ˆæ¯”å¦‚ï¼šé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ç¥ç»ç½‘ç»œheadè¢«æ‰”æ‰äº†ï¼ŒåŒæ—¶éšæœºåˆå§‹åŒ–äº†æ–‡æœ¬åˆ†ç±»çš„ç¥ç»ç½‘ç»œheadï¼‰
#### è®¾å®šè®­ç»ƒå‚æ•°
ä¸ºäº†èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ª`Trainer`è®­ç»ƒå·¥å…·ï¼Œæˆ‘ä»¬è¿˜éœ€è¦è®­ç»ƒçš„è®¾å®š/å‚æ•° `TrainingArguments`ã€‚è¿™ä¸ªè®­ç»ƒè®¾å®šåŒ…å«äº†èƒ½å¤Ÿå®šä¹‰è®­ç»ƒè¿‡ç¨‹çš„æ‰€æœ‰å±æ€§ã€‚
```python
batch_size = 16
metric_name = "pearson" if task == "stsb" else "matthews_correlation" if task == "cola" else "accuracy"

args = TrainingArguments(
    "test-glue",
    evaluation_strategy = "epoch", #æ¯ä¸ªepcohä¼šåšä¸€æ¬¡éªŒè¯è¯„ä¼°ï¼›
    save_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model=metric_name, #æ ¹æ®å“ªä¸ªè¯„ä»·æŒ‡æ ‡é€‰æœ€ä¼˜æ¨¡å‹
)
```
#### å®šä¹‰è¯„ä¼°æ–¹æ³•
è¿˜æœ‰ä¸€ä»¶é‡è¦çš„äº‹ï¼Œæˆ‘ä»¬éœ€è¦é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„è¯„ä»·æŒ‡æ ‡å¼•å¯¼æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚
æˆ‘ä»¬ä½¿ç”¨ğŸ¤— Datasetsåº“æ¥åŠ è½½è¯„ä»·æŒ‡æ ‡è®¡ç®—åº“load_metricã€‚meticæ˜¯datasets.Metricçš„ä¸€ä¸ªå®ä¾‹:
```python
from datasets import load_metric
import numpy as np

fake_preds = np.random.randint(0, 2, size=(64,))
fake_labels = np.random.randint(0, 2, size=(64,))
metric.compute(predictions=fake_preds, references=fake_labels)
#{'matthews_correlation': 0.1513518081969605}
```
![](https://raw.githubusercontent.com/innovation64/Picimg/main/20221027235308.png)
ä¸ºTrainerå®šä¹‰å„ä¸ªä»»åŠ¡çš„è¯„ä¼°æ–¹æ³•compute_metricsï¼š
```python
from datasets import load_metric
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    if task != "stsb":
        predictions = np.argmax(predictions, axis=1)
    else:
        predictions = predictions[:, 0]
    return metric.compute(predictions=predictions, references=labels)
```
#### å¼€å§‹è®­ç»ƒ
å°†æ•°æ®/æ¨¡å‹/å‚æ•°ä¼ å…¥`Trainer`å³å¯ï¼š
```python
validation_key = "validation_mismatched" if task == "mnli-mm" else "validation_matched" if task == "mnli" else "validation"
trainer = Trainer(
    model,
    args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset[validation_key],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
```
å¼€å§‹è®­ç»ƒ:
```python
trainer.train()
```
æ¨¡å‹è¯„ä¼°
```python
trainer.evaluate()
```
### è¶…å‚æ•°æœç´¢
`Trainer`è¿˜æ”¯æŒè¶…å‚æœç´¢ï¼Œä½¿ç”¨optuna or Ray Tuneä»£ç åº“ã€‚
éœ€è¦å®‰è£…ä»¥ä¸‹ä¸¤ä¸ªä¾èµ–ï¼š
```python
pip install optuna
pip install ray[tune]
```
è¶…å‚æœç´¢æ—¶ï¼ŒTrainerå°†ä¼šè¿”å›å¤šä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œæ‰€ä»¥éœ€è¦ä¼ å…¥ä¸€ä¸ªå®šä¹‰å¥½çš„æ¨¡å‹ä»è€Œè®©Trainerå¯ä»¥ä¸æ–­é‡æ–°åˆå§‹åŒ–è¯¥ä¼ å…¥çš„æ¨¡å‹ï¼š
```python
def model_init():
    return AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)
```
å’Œä¹‹å‰è°ƒç”¨ Trainerç±»ä¼¼:
```python
trainer = Trainer(
    model_init=model_init,
    args=args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset[validation_key],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
```
è°ƒç”¨æ–¹æ³•`hyperparameter_search`è¿›è¡Œè¶…å‚æ•°æœç´¢ã€‚

æ³¨æ„ï¼Œè¿™ä¸ªè¿‡ç¨‹å¯èƒ½å¾ˆä¹…ï¼Œå¯ä»¥å…ˆç”¨éƒ¨åˆ†æ•°æ®é›†è¿›è¡Œè¶…å‚æœç´¢ï¼Œå†è¿›è¡Œå…¨é‡è®­ç»ƒã€‚ æ¯”å¦‚ä½¿ç”¨1/10çš„æ•°æ®è¿›è¡Œæœç´¢ï¼ˆåˆ©ç”¨n_trialsè®¾ç½®ï¼‰ï¼š
```python
best_run = trainer.hyperparameter_search(n_trials=10, direction="maximize")
```
hyperparameter_searchä¼šè¿”å›æ•ˆæœæœ€å¥½çš„æ¨¡å‹ç›¸å…³çš„å‚æ•°best_runï¼š

å°†Trainnerè®¾ç½®ä¸ºæœç´¢åˆ°çš„æœ€å¥½å‚æ•°best_runï¼Œå†å¯¹å…¨éƒ¨æ•°æ®è¿›è¡Œè®­ç»ƒï¼š
```python
for n, v in best_run.hyperparameters.items():
    setattr(trainer.args, n, v)

trainer.train()
```
### ä¸Šä¼ æ¨¡å‹åˆ°huggingface
[ä¸Šä¼ æ¨¡å‹](https://huggingface.co/docs/transformers/model_sharing)

## åºåˆ—æ ‡æ³¨
### ä»»åŠ¡ä»‹ç»
åºåˆ—æ ‡æ³¨ï¼Œé€šå¸¸ä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯tokençº§åˆ«çš„åˆ†ç±»é—®é¢˜ï¼šå¯¹æ¯ä¸€ä¸ªtokenè¿›è¡Œåˆ†ç±»ã€‚

tokençº§åˆ«çš„åˆ†ç±»ä»»åŠ¡é€šå¸¸æŒ‡çš„æ˜¯ä¸ºæ–‡æœ¬ä¸­çš„æ¯ä¸€ä¸ªtokené¢„æµ‹ä¸€ä¸ªæ ‡ç­¾ç»“æœã€‚æ¯”å¦‚å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ï¼š
```python
è¾“å…¥ï¼šæˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨
è¾“å‡ºï¼šO O B-LOC I-LOC B-POI B-POI I-POI
```
å¸¸è§çš„tokençº§åˆ«åˆ†ç±»ä»»åŠ¡:
- NER (Named-entity recognition åè¯-å®ä½“è¯†åˆ«) åˆ†è¾¨å‡ºæ–‡æœ¬ä¸­çš„åè¯å’Œå®ä½“ (personäººå, organizationç»„ç»‡æœºæ„å, locationåœ°ç‚¹å...).
- POS (Part-of-speech taggingè¯æ€§æ ‡æ³¨) æ ¹æ®è¯­æ³•å¯¹tokenè¿›è¡Œè¯æ€§æ ‡æ³¨ (nounåè¯, verbåŠ¨è¯, adjectiveå½¢å®¹è¯...)
- Chunk (ChunkingçŸ­è¯­ç»„å—) å°†åŒä¸€ä¸ªçŸ­è¯­çš„tokensç»„å—æ”¾åœ¨ä¸€èµ·ã€‚

ä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š
- æ•°æ®åŠ è½½
- æ•°æ®é¢„å¤„ç†
- å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼šä½¿ç”¨transformerä¸­çš„Traineræ¥å£å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚
#### å‰æœŸå‡†å¤‡
```python
pip install datasets transformers seqeval
#transformers==4.9.2
#datasets==1.11.0
#seqeval==1.2.2
```
### æ•°æ®åŠ è½½
#### æ•°æ®é›†ä»‹ç»
æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯CONLL 2003 datasetæ•°æ®é›†ã€‚

æ— è®ºæ˜¯åœ¨è®­ç»ƒé›†ã€éªŒè¯æœºè¿˜æ˜¯æµ‹è¯•é›†ä¸­ï¼Œdatasetséƒ½åŒ…å«äº†ä¸€ä¸ªåä¸ºtokensçš„åˆ—ï¼ˆä¸€èˆ¬æ¥è¯´æ˜¯å°†æ–‡æœ¬åˆ‡åˆ†æˆäº†å¤šä¸ªtokenï¼‰ï¼Œè¿˜åŒ…å«å®Œæˆä¸‰ä¸ªä¸åŒä»»åŠ¡çš„labelåˆ—ï¼ˆner_tag,pos_tagå’Œchunk_tagï¼‰ï¼Œå¯¹åº”äº†ä¸åŒä»»åŠ¡è¿™tokensçš„æ ‡æ³¨
#### åŠ è½½æ•°æ®
è¯¥æ•°æ®çš„åŠ è½½æ–¹å¼åœ¨transformersåº“ä¸­è¿›è¡Œäº†å°è£…ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹è¯­å¥è¿›è¡Œæ•°æ®åŠ è½½ï¼š
```python
from datasets import load_dataset
datasets = load_dataset("conll2003")
```
ç»™å®šä¸€ä¸ªæ•°æ®åˆ‡åˆ†çš„keyï¼ˆtrainã€validationæˆ–è€…testï¼‰å’Œä¸‹æ ‡å³å¯æŸ¥çœ‹æ•°æ®ã€‚
```python
datasets["train"][0]
#{'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],
# 'id': '0',
# 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0],
# 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],
# 'tokens': ['EU',
#  'rejects',
#  'German',
#  'call',
#  'to',
#  'boycott',
#  'British',
#  'lamb',
#  '.']}
```
æ‰€æœ‰çš„æ•°æ®æ ‡ç­¾éƒ½å·²ç»è¢«ç¼–ç æˆäº†æ•´æ•°ï¼Œå¯ä»¥ç›´æ¥è¢«é¢„è®­ç»ƒtransformeræ¨¡å‹ä½¿ç”¨ã€‚è¿™äº›æ•´æ•°çš„ç¼–ç æ‰€å¯¹åº”çš„å®é™…ç±»åˆ«å‚¨å­˜åœ¨featuresä¸­ã€‚
```python
datasets["train"].features[f"ner_tags"]
#Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], names_file=None, id=None), length=-1, id=None)
```
ä»¥NERä»»åŠ¡ä¸ºä¾‹ï¼Œ0å¯¹åº”çš„æ ‡ç­¾ç±»åˆ«æ˜¯â€Oâ€œï¼Œ 1å¯¹åº”çš„æ˜¯â€B-PERâ€œç­‰ç­‰ã€‚

â€Oâ€œè¡¨ç¤ºæ²¡æœ‰ç‰¹åˆ«å®ä½“ï¼ˆno special entity/otherï¼‰ã€‚æœ¬ä¾‹åŒ…å«4ç§æœ‰ä»·å€¼å®ä½“ç±»åˆ«åˆ†åˆ«æ˜¯ï¼ˆPERã€ORGã€LOCï¼ŒMISCï¼‰ï¼Œæ¯ä¸€ç§å®ä½“ç±»åˆ«åˆåˆ†åˆ«æœ‰B-ï¼ˆå®ä½“å¼€å§‹çš„tokenï¼‰å‰ç¼€å’ŒI-ï¼ˆå®ä½“ä¸­é—´çš„tokenï¼‰å‰ç¼€ã€‚
- 'PER' for person
- 'ORG' for organization
- 'LOC' for location
- 'MISC' for miscellaneous
```python
label_list = datasets["train"].features[f"{task}_tags"].feature.names
label_list
#['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']
```
ä¸‹é¢çš„å‡½æ•°å°†ä»æ•°æ®é›†é‡Œéšæœºé€‰æ‹©å‡ ä¸ªä¾‹å­è¿›è¡Œå±•ç¤ºï¼š
```python
from datasets import ClassLabel, Sequence
import random
import pandas as pd
from IPython.display import display, HTML

def show_random_elements(dataset, num_examples=4):
    assert num_examples <= len(dataset), "Can't pick more elements than there are in the dataset."
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset)-1)
        while pick in picks:
            pick = random.randint(0, len(dataset)-1)
        picks.append(pick)
    
    df = pd.DataFrame(dataset[picks])
    for column, typ in dataset.features.items():
        if isinstance(typ, ClassLabel):
            df[column] = df[column].transform(lambda i: typ.names[i])
        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):
            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])
    display(HTML(df.to_html()))

```
```
show_random_elements(datasets["train"])
```
![](https://raw.githubusercontent.com/innovation64/Picimg/main/20221028001708.png)
### æ•°æ®é¢„å¤„ç†
#### åˆå§‹åŒ–Tokenizer
```
from transformers import AutoTokenizer
model_checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```
#### è½¬åŒ–æˆå¯¹åº”ä»»åŠ¡è¾“å…¥æ¨¡å‹çš„æ ¼å¼
transformeré¢„è®­ç»ƒæ¨¡å‹åœ¨é¢„è®­ç»ƒçš„æ—¶å€™é€šå¸¸ä½¿ç”¨çš„æ˜¯subwordï¼Œå¦‚æœæˆ‘ä»¬çš„æ–‡æœ¬è¾“å…¥å·²ç»è¢«åˆ‡åˆ†æˆäº†wordï¼Œè¿™äº›wordè¿˜ä¼šè¢«tokenizerç»§ç»­åˆ‡åˆ†ä¸ºsubwordsï¼ŒåŒæ—¶ï¼Œç”±äºé¢„è®­ç»ƒæ¨¡å‹è¾“å…¥æ ¼å¼çš„è¦æ±‚ï¼Œå¾€å¾€è¿˜éœ€è¦åŠ ä¸Šä¸€äº›ç‰¹æ®Šç¬¦å·æ¯”å¦‚ï¼š [CLS] å’Œ [SEP]ã€‚æ¯”å¦‚ï¼š
```python
example = datasets["train"][4]
tokenized_input = tokenizer(example["tokens"], is_split_into_words=True)
tokens = tokenizer.convert_ids_to_tokens(tokenized_input["input_ids"])
print(len(example[f"{task}_tags"]), len(tokenized_input["input_ids"]))
#(31, 39)
print(example["tokens"])
print(tokens)
#['Germany', "'s", 'representative', 'to', 'the', 'European', 'Union', "'s", 'veterinary', 'committee', 'Werner', 'Zwingmann', 'said', 'on', 'Wednesday', 'consumers', 'should', 'buy', 'sheepmeat', 'from', 'countries', 'other', 'than', 'Britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.']
#['[CLS]', 'germany', "'", 's', 'representative', 'to', 'the', 'european', 'union', "'", 's', 'veterinary', 'committee', 'werner', 'z', '##wing', '##mann', 'said', 'on', 'wednesday', 'consumers', 'should', 'buy', 'sheep', '##me', '##at', 'from', 'countries', 'other', 'than', 'britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.', '[SEP]']
```
å¯ä»¥çœ‹åˆ°å•è¯"Zwingmann" ç»§ç»­è¢«åˆ‡åˆ†æˆäº†3ä¸ªsubtokensï¼š 'z', '##wing', '##mann'ã€‚

ç”±äºæ ‡æ³¨æ•°æ®é€šå¸¸æ˜¯åœ¨wordçº§åˆ«è¿›è¡Œæ ‡æ³¨çš„ï¼Œè€Œwordè¢«åˆ‡åˆ†æˆäº†subwordsï¼Œé‚£ä¹ˆæ„å‘³éœ€è¦å¯¹æ ‡æ³¨æ•°æ®è¿›è¡Œsubtokensçš„å¯¹é½ã€‚

tokenizerä¸­word_idsæ–¹æ³•å¯ä»¥å¸®åŠ©æˆ‘ä»¬è§£å†³è¿™ä¸ªé—®é¢˜ã€‚
```python
print(tokenized_input.word_ids())
#[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 11, 11, 11, 12, 13, 14, 15, 16, 17, 18, 18, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, None]
```
å¯ä»¥çœ‹åˆ°ï¼Œword_idså°†æ¯ä¸€ä¸ªsubtokensä½ç½®éƒ½å¯¹åº”äº†ä¸€ä¸ªwordçš„ä¸‹æ ‡ã€‚æ¯”å¦‚ç¬¬1ä¸ªä½ç½®å¯¹åº”ç¬¬0ä¸ªwordï¼Œç„¶åç¬¬2ã€3ä¸ªä½ç½®å¯¹åº”ç¬¬1ä¸ªwordã€‚ç‰¹æ®Šå­—ç¬¦å¯¹åº”äº†Noneã€‚

åˆ©ç”¨è¿™ä¸ªlistï¼Œæˆ‘ä»¬å°±èƒ½å°†subtokenså’Œwordsè¿˜æœ‰æ ‡æ³¨çš„labelså¯¹é½ã€‚
```python
word_ids = tokenized_input.word_ids()
aligned_labels = [-100 if i is None else example[f"{task}_tags"][i] for i in word_ids]
print(len(aligned_labels), len(tokenized_input["input_ids"]))
#39 39
```
é€šå¸¸å°†ç‰¹æ®Šå­—ç¬¦çš„labelè®¾ç½®ä¸º-100ï¼Œåœ¨æ¨¡å‹ä¸­-100é€šå¸¸ä¼šè¢«å¿½ç•¥æ‰ä¸è®¡ç®—lossã€‚
æœ‰ä¸¤ç§å¯¹é½labelçš„æ–¹å¼ï¼Œé€šè¿‡label_all_tokens = Trueåˆ‡æ¢ã€‚
- å¤šä¸ªsubtokenså¯¹é½ä¸€ä¸ªwordï¼Œå¯¹é½ä¸€ä¸ªlabelï¼›
- å¤šä¸ªsubtokensçš„ç¬¬ä¸€ä¸ªsubtokenå¯¹é½wordï¼Œå¯¹é½ä¸€ä¸ªlabelï¼Œå…¶ä»–subtokensç›´æ¥èµ‹äºˆ-100.

æ‰€æœ‰å†…å®¹åˆèµ·æ¥å˜æˆæˆ‘ä»¬çš„é¢„å¤„ç†å‡½æ•°ã€‚is_split_into_words=Trueå› ä¸ºè¾“å…¥çš„æ•°æ®å·²ç»æ˜¯æŒ‰ç©ºæ ¼åˆ‡åˆ†æˆwordçš„æ ¼å¼äº†ã€‚
```python
label_all_tokens = True

def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)

    labels = []
    for i, label in enumerate(examples[f"{task}_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            # Special tokens have a word id that is None. We set the label to -100 so they are automatically
            # ignored in the loss function.
            if word_idx is None:
                label_ids.append(-100)
            # We set the label for the first token of each word.
            elif word_idx != previous_word_idx:
                label_ids.append(label[word_idx])
            # For the other tokens in a word, we set the label to either the current label or -100, depending on
            # the label_all_tokens flag.
            else:
                label_ids.append(label[word_idx] if label_all_tokens else -100)
            previous_word_idx = word_idx

        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs
```
ä»¥ä¸Šçš„é¢„å¤„ç†å‡½æ•°å¯ä»¥å¤„ç†ä¸€ä¸ªæ ·æœ¬ï¼Œä¹Ÿå¯ä»¥å¤„ç†å¤šä¸ªæ ·æœ¬exapmlesã€‚å¦‚æœæ˜¯å¤„ç†å¤šä¸ªæ ·æœ¬ï¼Œåˆ™è¿”å›çš„æ˜¯å¤šä¸ªæ ·æœ¬è¢«é¢„å¤„ç†ä¹‹åçš„ç»“æœlistã€‚
```python
tokenize_and_align_labels(datasets['train'][:5])
#{'input_ids': [[101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102], [101, 2848, 13934, 102], [101, 9371, 2727, 1011, 5511, 1011, 2570, 102], [101, 1996, 2647, 3222, 2056, 2006, 9432, 2009, 18335, 2007, 2446, 6040, 2000, 10390, 2000, 18454, 2078, 2329, 12559, 2127, 6529, 5646, 3251, 5506, 11190, 4295, 2064, 2022, 11860, 2000, 8351, 1012, 102], [101, 2762, 1005, 1055, 4387, 2000, 1996, 2647, 2586, 1005, 1055, 15651, 2837, 14121, 1062, 9328, 5804, 2056, 2006, 9317, 10390, 2323, 4965, 8351, 4168, 4017, 2013, 3032, 2060, 2084, 3725, 2127, 1996, 4045, 6040, 2001, 24509, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100], [-100, 1, 2, -100], [-100, 5, 0, 0, 0, 0, 0, -100], [-100, 0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 5, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -100]]}
```
æ¥ä¸‹æ¥ä½¿ç”¨mapå‡½æ•°å¯¹æ•°æ®é›†datasetsé‡Œé¢ä¸‰ä¸ªæ ·æœ¬é›†åˆçš„æ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œå°†é¢„å¤„ç†å‡½æ•°prepare_train_featuresåº”ç”¨åˆ°ï¼ˆmap)æ‰€æœ‰æ ·æœ¬ä¸Šã€‚
```python
tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)
```
### å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹
#### åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
åšåºåˆ—æ ‡æ³¨token classificationä»»åŠ¡ï¼Œé‚£ä¹ˆéœ€è¦ä¸€ä¸ªèƒ½è§£å†³è¿™ä¸ªä»»åŠ¡çš„æ¨¡å‹ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨AutoModelForTokenClassification è¿™ä¸ªç±»ã€‚
å’Œtokenizerç›¸ä¼¼ï¼Œfrom_pretrainedæ–¹æ³•åŒæ ·å¯ä»¥å¸®åŠ©ä¸‹è½½å¹¶åŠ è½½æ¨¡å‹ï¼ŒåŒæ—¶ä¹Ÿä¼šå¯¹æ¨¡å‹è¿›è¡Œç¼“å­˜ï¼Œä¹Ÿå¯ä»¥å¡«å…¥ä¸€ä¸ªåŒ…æ‹¬æ¨¡å‹ç›¸å…³æ–‡ä»¶çš„æ–‡ä»¶å¤¹ï¼ˆæ¯”å¦‚è‡ªå·±é¢„è®­ç»ƒçš„æ¨¡å‹ï¼‰ï¼Œè¿™æ ·ä¼šä»æœ¬åœ°ç›´æ¥åŠ è½½ã€‚
```python
from transformers import AutoModelForTokenClassification
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))
```
#### è®¾å®šè®­ç»ƒå‚æ•°
```python
task='ner'
batch_size = 16

from transformers import  TrainingArguments

args = TrainingArguments(
    f"test-{task}",
    evaluation_strategy = "epoch",#æ¯ä¸ªepcohä¼šåšä¸€æ¬¡éªŒè¯è¯„ä¼°
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=3,
    weight_decay=0.01,
)
```
#### æ•°æ®æ”¶é›†å™¨ data collator
æ¥ä¸‹æ¥éœ€è¦å‘Šè¯‰Trainerå¦‚ä½•ä»é¢„å¤„ç†çš„è¾“å…¥æ•°æ®ä¸­æ„é€ batchã€‚æˆ‘ä»¬ä½¿ç”¨æ•°æ®æ”¶é›†å™¨data collatorï¼Œå°†ç»é¢„å¤„ç†çš„è¾“å…¥åˆ†batchå†æ¬¡å¤„ç†åå–‚ç»™æ¨¡å‹ã€‚
```python
from transformers import DataCollatorForTokenClassification
data_collator = DataCollatorForTokenClassification(tokenizer)
```
#### å®šä¹‰è¯„ä¼°æ–¹æ³•
ä½¿ç”¨seqeval metricæ¥å®Œæˆè¯„ä¼°
```python
from datasets import load_metric
metric = load_metric("seqeval")
```
è¯„ä¼°çš„è¾“å…¥æ˜¯é¢„æµ‹å’Œlabelçš„listï¼š
```python
labels = [label_list[i] for i in example[f"{task}_tags"]]
metric.compute(predictions=[labels], references=[labels])
#{'LOC': {'f1': 1.0, 'number': 2, 'precision': 1.0, 'recall': 1.0},
# 'ORG': {'f1': 1.0, 'number': 1, 'precision': 1.0, 'recall': 1.0},
# 'PER': {'f1': 1.0, 'number': 1, 'precision': 1.0, 'recall': 1.0},
# 'overall_accuracy': 1.0,
# 'overall_f1': 1.0,
# 'overall_precision': 1.0,
# 'overall_recall': 1.0}
```
å°†æ¨¡å‹é¢„æµ‹é€å…¥è¯„ä¼°ä¹‹å‰ï¼Œè¿˜éœ€è¦åšä¸€äº›æ•°æ®åå¤„ç†ï¼š
- é€‰æ‹©é¢„æµ‹åˆ†ç±»æœ€å¤§æ¦‚ç‡çš„ä¸‹æ ‡ï¼›
- å°†ä¸‹æ ‡è½¬åŒ–ä¸ºlabelï¼›
- å¿½ç•¥-100æ‰€åœ¨ä½ç½®ã€‚

ä¸‹é¢çš„å‡½æ•°å°†ä¸Šé¢çš„æ­¥éª¤åˆå¹¶äº†èµ·æ¥ï¼š
```python
import numpy as np

def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2) #é€‰æ‹©é¢„æµ‹åˆ†ç±»æœ€å¤§æ¦‚ç‡çš„ä¸‹æ ‡

    # Remove ignored index (special tokens)å¿½ç•¥-100æ‰€åœ¨ä½ç½®
    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    results = metric.compute(predictions=true_predictions, references=true_labels)
    #è®¡ç®—æ‰€æœ‰ç±»åˆ«æ€»çš„precision/recall/f1ï¼Œæ‰€ä»¥ä¼šæ‰”æ‰å•ä¸ªç±»åˆ«çš„precision/recall/f1 
    return {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"],
    }
```
### å¼€å§‹è®­ç»ƒ 
```python
from transformers import   Trainer
trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
trainer.train()
```
### æ¨¡å‹è¯„ä¼°
```python
trainer.evaluate()
predictions, labels, _ = trainer.predict(tokenized_datasets["validation"])
predictions = np.argmax(predictions, axis=2)

# Remove ignored index (special tokens)
true_predictions = [
    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
    for prediction, label in zip(predictions, labels)
]
true_labels = [
    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
    for prediction, label in zip(predictions, labels)
]

results = metric.compute(predictions=true_predictions, references=true_labels)
print(results)

```
## é—®ç­”ä»»åŠ¡-å¤šé€‰é—®ç­”
### ä»»åŠ¡ä»‹ç»
è™½ç„¶å«å¤šé€‰é—®ç­”ï¼Œä½†å®é™…ä¸Šæ˜¯æŒ‡ç»™å‡ºä¸€ä¸ªé—®é¢˜çš„å¤šä¸ªå¯èƒ½çš„ç­”æ¡ˆï¼ˆå¤‡é€‰é¡¹ï¼‰ï¼Œé€‰å‡ºå…¶ä¸­ä¸€ä¸ªæœ€åˆç†çš„ï¼Œå…¶å®ç±»ä¼¼äºæˆ‘ä»¬å¹³å¸¸åšçš„å•é€‰é¢˜ã€‚è¯¥ä»»åŠ¡çš„å®è´¨åŒæ ·æ˜¯åˆ†ç±»ä»»åŠ¡ï¼Œåœ¨å¤šä¸ªå¤‡é€‰é¡¹ä¸­è¿›è¡ŒäºŒåˆ†ç±»ï¼Œæ‰¾åˆ°ç­”æ¡ˆã€‚

æ¯”å¦‚è¾“å…¥ä¸€å¥è¯çš„ä¸ŠåŠå¥ï¼Œç»™å‡ºå‡ ä¸ªååŠå¥çš„å¤‡é€‰é¡¹ï¼Œé€‰å‡ºå“ªä¸ªé€‰é¡¹æ˜¯è¿™ä¸ªä¸ŠåŠå¥çš„ååŠå¥
```
è¾“å…¥ï¼š("ç¦»ç¦»åŸä¸Šè‰"ï¼Œ["å¤©å®‰é—¨ä¸€æ¸¸","ä¸€å²ä¸€æ¯è£","æ˜¥é£å¹åˆç”Ÿ"])
è¾“å‡ºï¼š1
```
ä¸»è¦åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š
- æ•°æ®åŠ è½½
- æ•°æ®é¢„å¤„ç†
- å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼šä½¿ç”¨transformerä¸­çš„Traineræ¥å£å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚
### æ•°æ®åŠ è½½
#### æ•°æ®é›†ä»‹ç»
æˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®é›†æ˜¯SWAGã€‚SWAGæ˜¯ä¸€ä¸ªå…³äºå¸¸è¯†æ¨ç†çš„æ•°æ®é›†ï¼Œæ¯ä¸ªæ ·æœ¬æè¿°ä¸€ç§æƒ…å†µï¼Œç„¶åç»™å‡ºå››ä¸ªå¯èƒ½çš„é€‰é¡¹ã€‚
#### åŠ è½½æ•°æ®
è¯¥æ•°æ®çš„åŠ è½½æ–¹å¼åœ¨transformersåº“ä¸­è¿›è¡Œäº†å°è£…ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹è¯­å¥è¿›è¡Œæ•°æ®åŠ è½½ï¼š
```python
from datasets import load_dataset
datasets = load_dataset("swag", "regular")
#ç»™å®šä¸€ä¸ªæ•°æ®åˆ‡åˆ†çš„keyï¼ˆtrainã€validationæˆ–è€…testï¼‰å’Œä¸‹æ ‡å³å¯æŸ¥çœ‹æ•°æ®ã€‚
datasets["train"][0]
#{'ending0': 'passes by walking down the street playing their instruments.',
# 'ending1': 'has heard approaching them.',
# 'ending2': "arrives and they're outside dancing and asleep.",
# 'ending3': 'turns the lead singer watches the performance.',
# 'fold-ind': '3416',
# 'gold-source': 'gold',
# 'label': 0,
# 'sent1': 'Members of the procession walk down the street holding small horn brass instruments.',
# 'sent2': 'A drum line',
# 'startphrase': 'Members of the procession walk down the street holding small horn brass instruments. A drum line',
# 'video-id': 'anetv_jkn6uvmqwh4'}


```
ä¸‹é¢çš„å‡½æ•°å°†ä»æ•°æ®é›†é‡Œéšæœºé€‰æ‹©å‡ ä¸ªä¾‹å­è¿›è¡Œå±•ç¤ºï¼š
```python
from datasets import ClassLabel
import random
import pandas as pd
from IPython.display import display, HTML

def show_random_elements(dataset, num_examples=3):
    assert num_examples <= len(dataset), "Can't pick more elements than there are in the dataset."
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset)-1)
        while pick in picks:
            pick = random.randint(0, len(dataset)-1)
        picks.append(pick)
    
    df = pd.DataFrame(dataset[picks])
    for column, typ in dataset.features.items():
        if isinstance(typ, ClassLabel):
            df[column] = df[column].transform(lambda i: typ.names[i])
    display(HTML(df.to_html()))

show_random_elements(datasets["train"])
```
![](https://raw.githubusercontent.com/innovation64/Picimg/main/20221028084948.png)
å¯ä»¥çœ‹åˆ°ï¼Œæ•°æ®é›†ä¸­çš„æ¯ä¸ªç¤ºä¾‹éƒ½æœ‰ä¸€ä¸ªä¸Šä¸‹æ–‡ï¼Œå®ƒæ˜¯ç”±ç¬¬ä¸€ä¸ªå¥å­(å­—æ®µsent1)å’Œç¬¬äºŒä¸ªå¥å­çš„ç®€ä»‹(å­—æ®µsent2)ç»„æˆï¼Œå¹¶ç»™å‡ºå››ç§ç»“å°¾å¥å­çš„å¤‡é€‰é¡¹(å­—æ®µending0ï¼Œ ending1ï¼Œ ending2å’Œending3)ï¼Œç„¶åè®©æ¨¡å‹ä»ä¸­é€‰æ‹©æ­£ç¡®çš„ä¸€ä¸ª(ç”±å­—æ®µlabelè¡¨ç¤º)ã€‚
### æ•°æ®é¢„å¤„ç†
```python
from transformers import AutoTokenizer
model_checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)

[("Members of the procession walk down the street holding small horn brass instruments.","A drum line passes by walking down the street playing their instruments."),
("Members of the procession walk down the street holding small horn brass instruments.","A drum line has heard approaching them."),
("Members of the procession walk down the street holding small horn brass instruments.","A drum line arrives and they're outside dancing and asleep."),
("Members of the procession walk down the street holding small horn brass instruments.","A drum line turns the lead singer watches the performance.")]
```
ä¹‹å‰å·²ç»ä»‹ç»è¿‡Tokenizerçš„è¾“å…¥å¯ä»¥æ˜¯ä¸€ä¸ªå•å¥ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸¤ä¸ªå¥å­ã€‚

é‚£ä¹ˆæ˜¾ç„¶åœ¨è°ƒç”¨tokenizerä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦é¢„å¤„ç†æ•°æ®é›†å…ˆç”Ÿæˆè¾“å…¥Tokenizerçš„æ ·æœ¬ã€‚

åœ¨preprocess_functionå‡½æ•°ä¸­ï¼š

- é¦–å…ˆå°†æ ·æœ¬ä¸­é—®é¢˜å’Œå¤‡é€‰é¡¹åˆ†åˆ«æ”¾åœ¨ä¸¤ä¸ªåµŒå¥—åˆ—è¡¨ï¼ˆä¸¤ä¸ªåµŒå¥—åˆ—è¡¨åˆ†åˆ«å­˜å‚¨äº†æ¯ä¸ªæ ·æœ¬çš„é—®é¢˜å’Œå¤‡é€‰é¡¹ï¼‰ä¸­ï¼›

    æ¯”å¦‚ï¼Œe1_sen1è¡¨ç¤ºæ ·æœ¬1çš„é—®é¢˜ï¼ˆç›¸å½“äºè¾“å…¥tokenizerçš„å¥å­1ï¼‰ï¼Œe1_sen2_1è¡¨ç¤ºæ ·æœ¬1çš„å¤‡é€‰é¡¹1ï¼ˆç›¸å½“äºè¾“å…¥tokenizerçš„å¥å­2ï¼‰.....
    ```
    [[e1_sen1,e1_sen1,e1_sen1,e1_sen1],
    [e2_sen1,e2_sen1,e2_sen1,e2_sen1],
    [e3_sen1,e3_sen1,e3_sen1,e3_sen1]]
    
    [[e1_sen2_1,e1_sen2_2,e1_sen2_3,e1_sen2_4],
    [e2_sen2_1,e2_sen2_2,e2_sen2_3,e2_sen2_4],
    [e3_sen2_1,e3_sen2_2,e3_sen2_3,e3_sen2_4]]
    ```
- ç„¶åå°†é—®é¢˜åˆ—è¡¨å’Œå¤‡é€‰é¡¹åˆ—è¡¨æ‹‰å¹³Flatten(ä¸¤ä¸ªåµŒå¥—åˆ—è¡¨å„è‡ªå»æ‰åµŒå¥—)ï¼Œä»¥ä¾¿tokenizerè¿›è¡Œæ‰¹å¤„ç†ï¼Œä»¥é—®é¢˜åˆ—è¡¨ä¸ºä¾‹ï¼š
  ```
  after flatten->
    [e1_sen1,e1_sen1,e1_sen1,e1_sen1,
    e2_sen1,e2_sen1,e2_sen1,e2_sen1,
    e3_sen1,e3_sen1,e3_sen1,e3_sen1]
    after Tokenize->
    [e1_tokens1,e1_tokens1,e1_tokens1,e1_tokens1,
    e2_tokens1,e2_tokens1,e2_tokens1,e2_tokens1,
    e3_tokens1,e3_tokens1,e3_tokens1]
    ```
- ç»è¿‡tokenizeråï¼Œå†è½¬å›æ¯ä¸ªæ ·æœ¬æœ‰å¤‡é€‰é¡¹ä¸ªæ•°è¾“å…¥idã€æ³¨æ„åŠ›æ©ç ç­‰ã€‚
```
after unflatten->
[[e1_tokens1,e1_tokens1,e1_tokens1,e1_tokens1],  
 [e2_tokens1,e2_tokens1,e2_tokens1,e2_tokens1]
 [e3_tokens1,e3_tokens1,e3_tokens1]]
```
å‚æ•°`truncation=True`ä½¿å¾—æ¯”æ¨¡å‹æ‰€èƒ½æ¥å—æœ€å¤§é•¿åº¦è¿˜é•¿çš„è¾“å…¥è¢«æˆªæ–­ã€‚
```python
ending_names = ["ending0", "ending1", "ending2", "ending3"]

def preprocess_function(examples):
    # é¢„å¤„ç†è¾“å…¥tokenizerçš„è¾“å…¥
    # Repeat each first sentence four times to go with the four possibilities of second sentences.
    first_sentences = [[context] * 4 for context in examples["sent1"]]#æ„é€ å’Œå¤‡é€‰é¡¹ä¸ªæ•°ç›¸åŒçš„é—®é¢˜å¥ï¼Œä¹Ÿæ˜¯tokenizerçš„ç¬¬ä¸€ä¸ªå¥å­
    # Grab all second sentences possible for each context.
    question_headers = examples["sent2"] #tokenizerçš„ç¬¬äºŒä¸ªå¥å­çš„ä¸ŠåŠå¥
    second_sentences = [[f"{header} {examples[end][i]}" for end in ending_names] for i, header in enumerate(question_headers)]#æ„é€ ä¸ŠåŠå¥æ‹¼æ¥ä¸‹åŠå¥ä½œä¸ºtokenizerçš„ç¬¬äºŒä¸ªå¥å­ï¼ˆä¹Ÿå°±æ˜¯å¤‡é€‰é¡¹ï¼‰
    
    # Flatten everything
    first_sentences = sum(first_sentences, []) #åˆå¹¶æˆä¸€ä¸ªåˆ—è¡¨æ–¹ä¾¿tokenizerä¸€æ¬¡æ€§å¤„ç†ï¼š[[e1_sen1,e1_sen1,e1_sen1,e1_sen1],[e2_sen1,e2_sen1,e2_sen1,e2_sen1],[e3_sen1,e3_sen1,e3_sen1,e3_sen1]]->[e1_sen1,e1_sen1,e1_sen1,e1_sen1,e2_sen1,e2_sen1,e2_sen1,e2_sen1,e3_sen1,e3_sen1,e3_sen1,e3_sen1]
    second_sentences = sum(second_sentences, [])#åˆå¹¶æˆä¸€ä¸ªåˆ—è¡¨æ–¹ä¾¿tokenizerä¸€æ¬¡æ€§å¤„ç†
    
    # Tokenize
    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)
    # Un-flatten
    # è½¬åŒ–æˆæ¯ä¸ªæ ·æœ¬ï¼ˆä¸€ä¸ªæ ·æœ¬ä¸­åŒ…æ‹¬äº†å››ä¸ªk=[é—®é¢˜1,é—®é¢˜1,é—®é¢˜1,é—®é¢˜1],v=[å¤‡é€‰é¡¹1,å¤‡é€‰é¡¹2,å¤‡é€‰é¡¹3,å¤‡é€‰é¡¹4]ï¼‰
    # [e1_tokens1,e1_tokens1,e1_tokens1,e1_tokens1,e2_tokens1,e2_tokens1,e2_tokens1,e2_tokens1,e3_tokens1,e3_tokens1,e3_tokens1,e3_tokens1]->[[e1_tokens1,e1_tokens1,e1_tokens1,e1_tokens1],[e2_tokens1,e2_tokens1,e2_tokens1,e2_tokens1],[e3_tokens1,e3_tokens1,e3_tokens1]]
    return {k: [v[i:i+4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}
```
ä»¥ä¸Šçš„é¢„å¤„ç†å‡½æ•°å¯ä»¥å¤„ç†ä¸€ä¸ªæ ·æœ¬ï¼Œä¹Ÿå¯ä»¥å¤„ç†å¤šä¸ªæ ·æœ¬exapmlesã€‚å¦‚æœæ˜¯å¤„ç†å¤šä¸ªæ ·æœ¬ï¼Œåˆ™è¿”å›çš„æ˜¯å¤šä¸ªæ ·æœ¬è¢«é¢„å¤„ç†ä¹‹åçš„ç»“æœlistã€‚

è®©æˆ‘ä»¬è§£ç ä¸€ä¸‹ç»™å®šç¤ºä¾‹çš„è¾“å…¥ï¼Œå¯ä»¥çœ‹åˆ°ä¸€ä¸ªæ ·æœ¬å¯¹åº”å››ä¸ªé—®é¢˜å’Œå¤‡é€‰é¡¹åˆå¹¶çš„å¥å­ã€‚
```python
examples = datasets["train"][:5]
features = preprocess_function(examples)
idx = 3
[tokenizer.decode(features["input_ids"][idx][i]) for i in range(4)]
#['[CLS] a drum line passes by walking down the street playing their instruments. [SEP] members of the procession are playing ping pong and celebrating one left each in quick. [SEP]',
# '[CLS] a drum line passes by walking down the street playing their instruments. [SEP] members of the procession wait slowly towards the cadets. [SEP]',
# '[CLS] a drum line passes by walking down the street playing their instruments. [SEP] members of the procession makes a square call and ends by jumping down into snowy streets where fans begin to take their positions. [SEP]',
# '[CLS] a drum line passes by walking down the street playing their instruments. [SEP] members of the procession play and go back and forth hitting the drums while the audience claps for them. [SEP]']
```
æ¥ä¸‹æ¥ä½¿ç”¨mapå‡½æ•°å¯¹æ•°æ®é›†datasetsé‡Œé¢ä¸‰ä¸ªæ ·æœ¬é›†åˆçš„æ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œå°†é¢„å¤„ç†å‡½æ•°prepare_train_featuresåº”ç”¨åˆ°ï¼ˆmap)æ‰€æœ‰æ ·æœ¬ä¸Šã€‚å‚æ•°batched=Trueå¯ä»¥æ‰¹é‡å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ã€‚è¿™æ˜¯ä¸ºäº†å……åˆ†åˆ©ç”¨å‰é¢åŠ è½½fast_tokenizerçš„ä¼˜åŠ¿ï¼Œå®ƒå°†ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘åœ°å¤„ç†æ‰¹ä¸­çš„æ–‡æœ¬ã€‚
```python
tokenized_datasets = datasets.map(preprocess_function, batched=True)
```
### å¾®è°ƒ
#### åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
åšå¤šé¡¹é€‰æ‹©ä»»åŠ¡ï¼Œé‚£ä¹ˆéœ€è¦ä¸€ä¸ªèƒ½è§£å†³è¿™ä¸ªä»»åŠ¡çš„æ¨¡å‹ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨`AutoModelForMultipleChoice` è¿™ä¸ªç±»ã€‚
```python
from transformers import AutoModelForMultipleChoice
model = AutoModelForMultipleChoice.from_pretrained(model_checkpoint)
```
#### è®¾å®šè®­ç»ƒå‚æ•°
```python
task='ner'
batch_size = 16

from transformers import  TrainingArguments

args = TrainingArguments(    
    "test-glue",    
    evaluation_strategy = "epoch",    
    learning_rate=5e-5,    
    per_device_train_batch_size=batch_size,    
    per_device_eval_batch_size=batch_size,    
    num_train_epochs=3,    
    weight_decay=0.01,
)
```
#### æ•°æ®æ”¶é›†å™¨data collator
æ¥ä¸‹æ¥éœ€è¦å‘Šè¯‰Trainerå¦‚ä½•ä»é¢„å¤„ç†çš„è¾“å…¥æ•°æ®ä¸­æ„é€ batchã€‚æˆ‘ä»¬ä½¿ç”¨æ•°æ®æ”¶é›†å™¨data collatorï¼Œå°†ç»é¢„å¤„ç†çš„è¾“å…¥åˆ†batchå†æ¬¡å¤„ç†åå–‚ç»™æ¨¡å‹ã€‚

ç”±å‰é¢preprocess_functionå‡½æ•°çš„è¾“å‡ºæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½è¿˜æ²¡æœ‰åšpaddingï¼Œæˆ‘ä»¬åœ¨data collatorä¸­æŒ‰ç…§batchå°†æ¯ä¸ªbatchçš„å¥å­paddingåˆ°æ¯ä¸ªbatchæœ€é•¿çš„é•¿åº¦ã€‚æ³¨æ„ï¼Œå› ä¸ºä¸åŒbatchä¸­æœ€é•¿çš„å¥å­ä¸ä¸€å®šéƒ½å’Œæ•´ä¸ªæ•°æ®é›†ä¸­çš„æœ€é•¿å¥å­ä¸€æ ·é•¿ï¼Œä¹Ÿå°±æ˜¯è¯´ä¸æ˜¯æ¯ä¸ªbatchéƒ½éœ€è¦é‚£ä¹ˆé•¿çš„paddingï¼Œæ‰€ä»¥è¿™é‡Œä¸ç›´æ¥paddingåˆ°æœ€å¤§é•¿åº¦ï¼Œå¯ä»¥æœ‰æ•ˆæå‡è®­ç»ƒæ•ˆç‡ã€‚

ç”±äºtransformersåº“ä¸­æ²¡æœ‰åˆé€‚çš„data collatoræ¥å¤„ç†è¿™æ ·ç‰¹å®šçš„é—®é¢˜ï¼Œæˆ‘ä»¬æ ¹æ®DataCollatorWithPaddingç¨ä½œæ”¹åŠ¨æ”¹ç¼–ä¸€ä¸ªåˆé€‚çš„ã€‚æˆ‘åœ¨ä»£ç ä¸­è¡¥å……äº†featureså’Œbatché€æ­¥è½¬åŒ–çš„æ ¼å¼å˜åŒ–è¿‡ç¨‹ï¼š
```python
from dataclasses import dataclass
from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy
from typing import Optional, Union
import torch

@dataclass
class DataCollatorForMultipleChoice:
    """
    Data collator that will dynamically pad the inputs for multiple choice received.
    """
    tokenizer: PreTrainedTokenizerBase
    padding: Union[bool, str, PaddingStrategy] = True
    max_length: Optional[int] = None
    pad_to_multiple_of: Optional[int] = None

    def __call__(self, features):
        #features:[{'attention_mask':[[],[],...],'input_ids':[[],[],...,'label':_},{'attention_mask':[[],[],...],'input_ids':[[],[],...,'label':_}]
        label_name = "label" if "label" in features[0].keys() else "labels"
        labels = [feature.pop(label_name) for feature in features] #å°†labelå•ç‹¬å¼¹å‡ºï¼Œfeatures:[{'attention_mask':[[],[],...],'input_ids':[[],[],...]},{'attention_mask':[[],[],...],'input_ids':[[],[],...]}]
        batch_size = len(features)
        num_choices = len(features[0]["input_ids"])
        
        #feature:{'attention_mask':[[],[],...],'input_ids':[[],[],...]}
        #flattened_features:[[{'attention_mask':[],'input_ids':[]},{},{},{}],[]....]
        flattened_features = [[{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features]
        #flattened_features:[{'attention_mask':[],'input_ids':[]},{},{},{},{}....]
        flattened_features = sum(flattened_features, [])
        
        # batch: {'attention_mask':[[],[],[],[],[],[],...],'input_ids':[[],[],[],[],[],[],...]}
        batch = self.tokenizer.pad(
            flattened_features,
            padding=self.padding,
            max_length=self.max_length,
            pad_to_multiple_of=self.pad_to_multiple_of,
            return_tensors="pt",
        )
        
        # Un-flatten
        # batch: {'attention_mask':[[[],[],[],[]],[[],[],[],[]],[...],...],'input_ids':[[[],[],[],[]],[[],[],[],[]],[...],...]}
        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}
        # Add back labels
        # batch: {'attention_mask':[[[],[],[],[]],[[],[],[],[]],[...],...],'input_ids':[[[],[],[],[]],[[],[],[],[]],[...],...],'label':[]}
        batch["labels"] = torch.tensor(labels, dtype=torch.int64)
        return batch
```
åœ¨ä¸€ä¸ª10ä¸ªæ ·æœ¬çš„batchä¸Šæ£€æŸ¥data collatoræ˜¯å¦æ­£å¸¸å·¥ä½œã€‚

åœ¨è¿™é‡Œæˆ‘ä»¬éœ€è¦ç¡®ä¿featuresä¸­åªæœ‰è¢«æ¨¡å‹æ¥å—çš„è¾“å…¥ç‰¹å¾ï¼ˆä½†è¿™ä¸€æ­¥åœ¨åé¢Trainerè‡ªåŠ¨ä¼šç­›é€‰ï¼‰ï¼š
```python
accepted_keys = ["input_ids", "attention_mask", "label"]
features = [{k: v for k, v in encoded_datasets["train"][i].items() if k in accepted_keys} for i in range(10)]
batch = DataCollatorForMultipleChoice(tokenizer)(features)
```
ç„¶åè®©æˆ‘ä»¬æ£€æŸ¥å•ä¸ªæ ·æœ¬æ˜¯å¦å®Œæ•´ï¼Œåˆ©ç”¨ä¹‹å‰çš„show_oneå‡½æ•°è¿›è¡Œå¯¹æ¯”ï¼Œçœ‹æ¥æ²¡é”™ï¼
```python
[tokenizer.decode(batch["input_ids"][8][i].tolist()) for i in range(4)]
#['[CLS] someone walks over to the radio. [SEP] someone hands her another phone. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',
# '[CLS] someone walks over to the radio. [SEP] someone takes the drink, then holds it. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',
# '[CLS] someone walks over to the radio. [SEP] someone looks off then looks at someone. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',
# '[CLS] someone walks over to the radio. [SEP] someone stares blearily down at the floor. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']

show_one(datasets["train"][8])
#    Context: Someone walks over to the radio.
#      A - Someone hands her another phone.
#      B - Someone takes the drink, then holds it.
#      C - Someone looks off then looks at someone.
#      D - Someone stares blearily down at the floor.
#    
#    Ground truth: option D
```
#### å®šä¹‰è¯„ä¼°æ–¹æ³•
æˆ‘ä»¬ä½¿ç”¨`accuracy`å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚

éœ€è¦å®šä¹‰ä¸€ä¸ªå‡½æ•°è®¡ç®—è¿”å›ç²¾åº¦ï¼Œå–é¢„æµ‹logitsçš„argmaxå¾—åˆ°é¢„æµ‹æ ‡ç­¾predsï¼Œå’Œground_truthè¿›è¡Œè¿›è¡Œå¯¹æ¯”ï¼Œè®¡ç®—ç²¾åº¦ï¼š
```python
import numpy as np
from datasets import load_metric
def compute_metrics(eval_predictions):
    predictions, label_ids = eval_predictions
    preds = np.argmax(predictions, axis=1)
    return {"accuracy": (preds == label_ids).astype(np.float32).mean().item()}
```
### 
```python
from transformers import  Trainer
trainer = Trainer(
    model,
    args,
    train_dataset=encoded_datasets["train"],
    eval_dataset=encoded_datasets["validation"],
    tokenizer=tokenizer,
    data_collator=DataCollatorForMultipleChoice(tokenizer),
    compute_metrics=compute_metrics,
)
trainer.train()
```
## é—®ç­”ä»»åŠ¡-æŠ½å–å¼é—®ç­”
### ä»»åŠ¡ä»‹ç»
æ³¨æ„æˆ‘ä»¬è¿™é‡Œä¸»è¦è§£å†³çš„æ˜¯æŠ½å–å¼é—®ç­”ä»»åŠ¡ï¼šç»™å®šä¸€ä¸ªé—®é¢˜å’Œä¸€æ®µæ–‡æœ¬ï¼Œä»è¿™æ®µæ–‡æœ¬ä¸­æ‰¾å‡ºèƒ½å›ç­”è¯¥é—®é¢˜çš„æ–‡æœ¬ç‰‡æ®µï¼ˆspanï¼‰ã€‚æŠ½å–å¼é—®ç­”ä»»åŠ¡æ˜¯ä»æ–‡æœ¬ä¸­æŠ½å–ç­”æ¡ˆï¼Œå¹¶ä¸æ˜¯ç›´æ¥ç”Ÿæˆç­”æ¡ˆã€‚
```
è¾“å…¥ï¼š
	é—®é¢˜ï¼šæˆ‘å®¶åœ¨å“ªé‡Œï¼Ÿ
	æ–‡æœ¬ï¼šæˆ‘çš„å®¶åœ¨ä¸œåŒ—ã€‚
è¾“å‡ºï¼šä¸œåŒ—
```
ä¸»è¦åˆ†ä¸ºä¸€ä¸‹å‡ ä¸ªéƒ¨åˆ†
- æ•°æ®åŠ è½½
- æ•°æ®é¢„å¤„ç†
- å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹
- æ¨¡å‹è¯„ä¼°

### æ•°æ®åŠ è½½
æˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®é›†æ˜¯SQUAD 2ï¼ŒStanford Question Answering Dataset (SQuAD) æ˜¯ä¸€ä¸ªé˜…è¯»ç†è§£æ•°æ®é›†ï¼Œç”±ä¼—å·¥å¯¹ä¸€ç»„ç»´åŸºç™¾ç§‘æ–‡ç« æå‡ºçš„é—®é¢˜ç»„æˆï¼Œæ¯ä¸ªé—®é¢˜çš„ç­”æ¡ˆéƒ½æ˜¯ä»ç›¸åº”çš„é˜…è¯»æ–‡ç« ä¸­èŠ‚é€‰å‡ºæ¥çš„ï¼Œæˆ–è€…è¿™ä¸ªé—®é¢˜å¯èƒ½æ˜¯æ— æ³•å›ç­”çš„ã€‚
```python
# squad_v2ç­‰äºTrueæˆ–è€…Falseåˆ†åˆ«ä»£è¡¨ä½¿ç”¨SQUAD v1 æˆ–è€… SQUAD v2ã€‚
# å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯å…¶ä»–æ•°æ®é›†ï¼Œé‚£ä¹ˆTrueä»£è¡¨çš„æ˜¯ï¼šæ¨¡å‹å¯ä»¥å›ç­”â€œä¸å¯å›ç­”â€é—®é¢˜ï¼Œä¹Ÿå°±æ˜¯éƒ¨åˆ†é—®é¢˜ä¸ç»™å‡ºç­”æ¡ˆï¼Œè€ŒFalseåˆ™ä»£è¡¨æ‰€æœ‰é—®é¢˜å¿…é¡»å›ç­”ã€‚
squad_v2 = False
# ä¸‹è½½æ•°æ®ï¼ˆç¡®ä¿æœ‰ç½‘ç»œï¼‰
datasets = load_dataset("squad_v2" if squad_v2 else "squad")
```
![](https://raw.githubusercontent.com/innovation64/Picimg/main/20221031091657.png)
### æ•°æ®é¢„å¤„ç†
#### åˆå§‹åŒ–Tokenizer
```python
from transformers import AutoTokenizer
model_checkpoint = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)
```
#### è½¬åŒ–æˆå¯¹åº”ä»»åŠ¡çš„è¾“å…¥æ¨¡å‹çš„æ ¼å¼
æœºå™¨é—®ç­”é¢„è®­ç»ƒæ¨¡å‹é€šå¸¸å°†questionå’Œcontextæ‹¼æ¥ä¹‹åä½œä¸ºè¾“å…¥ï¼Œç„¶åè®©æ¨¡å‹ä»contexté‡Œå¯»æ‰¾ç­”æ¡ˆã€‚å¯¹äºcontextä¸­æ— ç­”æ¡ˆçš„æƒ…å†µï¼Œæˆ‘ä»¬ç›´æ¥å°†æ ‡æ³¨çš„ç­”æ¡ˆèµ·å§‹ä½ç½®å’Œç»“æŸä½ç½®æ”¾ç½®åœ¨CLSçš„ä¸‹æ ‡å¤„ã€‚

æˆ‘ä»¬å°†questionä½œä¸ºtokenizerçš„å¥å­1ï¼Œcontextä½œä¸ºtokenizerçš„å¥å­2ï¼Œtokenizerä¼šå°†ä»–ä»¬æ‹¼æ¥èµ·æ¥å¹¶åŠ å…¥ç‰¹æ®Šå­—ç¬¦ä½œä¸ºæ¨¡å‹è¾“å…¥
```python
example = datasets["train"][0]
tokenized_example=tokenizer(example["question"], example["context"])
tokenized_example["input_ids"]
#[101,2000,3183,2106,1996,6261,2984,9382,3711,1999,8517,1999,10223,26371,2605,1029,102,6549,2135,1010,1996,2082,2038,1037,3234,2839,1012,10234,1996,2364,2311,1005,1055,2751,8514,2003,1037,3585,6231,1997,1996,6261,2984,1012,3202,1999,2392,1997,1996,2364,2311,1998,5307,2009,1010,2003,1037,6967,6231,1997,4828,2007,2608,2039,14995,6924,2007,1996,5722,1000,2310,3490,2618,4748,2033,18168,5267,1000,1012,2279,2000,1996,2364,2311,2003,1996,13546,1997,1996,6730,2540,1012,3202,2369,1996,13546,2003,1996,24665,23052,1010,1037,14042,2173,1997,7083,1998,9185,1012,2009,2003,1037,15059,1997,1996,24665,23052,2012,10223,26371,1010,2605,2073,1996,6261,2984,22353,2135,2596,2000,3002,16595,9648,4674,2061,12083,9711,2271,1999,8517,1012,2012,1996,2203,1997,1996,2364,3298,1006,1998,1999,1037,3622,2240,2008,8539,2083,1017,11342,1998,1996,2751,8514,1007,1010,2003,1037,3722,1010,2715,2962,6231,1997,2984,1012,102]
```
æˆ‘ä»¬ä½¿ç”¨sequence_idsæ–¹æ³•æ¥è·å–maskåŒºåˆ†questionå’Œcontextã€‚ Noneå¯¹åº”äº†special tokensï¼Œç„¶å0æˆ–è€…1åˆ†è¡¨ä»£è¡¨ç¬¬1ä¸ªæ–‡æœ¬å’Œç¬¬2ä¸ªæ–‡æœ¬ï¼Œç”±äºæˆ‘ä»¬questionç¬¬1ä¸ªä¼ å…¥ï¼Œcontextç¬¬2ä¸ªä¼ å…¥ï¼Œæ‰€ä»¥åˆ†åˆ«å¯¹åº”questionå’Œcontextã€‚
```python
sequence_ids = tokenized_example.sequence_ids()
print(sequence_ids)
#[None,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,None,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,None]
```
ç°åœ¨éœ€è¦ç‰¹åˆ«æ€è€ƒä¸€ä¸ªé—®é¢˜ï¼šå½“é‡åˆ°è¶…é•¿contextæ—¶ï¼ˆè¶…è¿‡äº†æ¨¡å‹èƒ½å¤„ç†çš„æœ€å¤§é•¿åº¦ï¼‰ä¼šä¸ä¼šå¯¹æ¨¡å‹é€ æˆå½±å“å‘¢ï¼Ÿ

ä¸€èˆ¬æ¥è¯´é¢„è®­ç»ƒæ¨¡å‹è¾“å…¥æœ‰æœ€å¤§é•¿åº¦è¦æ±‚ï¼Œç„¶åé€šå¸¸å°†è¶…é•¿è¾“å…¥è¿›è¡Œæˆªæ–­ã€‚ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬å°†é—®ç­”æ•°æ®ä¸‰å…ƒç»„<question, context, answer>ä¸­è¶…é•¿contextæˆªæ–­ï¼Œé‚£ä¹ˆå¯èƒ½ä¸¢æ‰ç­”æ¡ˆï¼ˆå› ä¸ºæ˜¯ä»contextä¸­æŠ½å–å‡ºä¸€ä¸ªå°ç‰‡æ®µä½œä¸ºç­”æ¡ˆï¼‰ã€‚
#### é‚£ä¹ˆé¢„è®­ç»ƒæœºå™¨é—®ç­”æ¨¡å‹æ˜¯å¦‚ä½•å¤„ç†è¶…é•¿æ–‡æœ¬çš„å‘¢ï¼Ÿ
æˆ‘ä»¬é¦–å…ˆæ‰¾åˆ°ä¸€ä¸ªè¶…è¿‡æ¨¡å‹æœ€å¤§é•¿åº¦çš„ä¾‹å­ï¼Œç„¶ååˆ†æå¤„ç†ä¸Šè¿°é—®é¢˜çš„æœºåˆ¶ã€‚

forå¾ªç¯éå†æ•°æ®é›†ï¼Œå¯»æ‰¾ä¸€ä¸ªè¶…é•¿æ ·æœ¬ï¼Œæˆ‘ä»¬å‰é¢é€‰æ‹©çš„æ¨¡å‹æ‰€è¦æ±‚çš„æœ€å¤§è¾“å…¥æ˜¯384ï¼ˆç»å¸¸ä½¿ç”¨çš„è¿˜æœ‰512ï¼‰ï¼š
```python
for i, example in enumerate(datasets["train"]):
    if len(tokenizer(example["question"], example["context"])["input_ids"]) > 384:
        break
example = datasets["train"][i]
```
å¦‚æœä¸æˆªæ–­çš„è¯ï¼Œé‚£ä¹ˆè¾“å…¥çš„é•¿åº¦æ˜¯396ï¼Œå¦‚æœæˆ‘ä»¬æˆªæ–­æˆæœ€å¤§é•¿åº¦384ï¼Œå°†ä¼šä¸¢å¤±è¶…é•¿éƒ¨åˆ†çš„ä¿¡æ¯
```python
len(tokenizer(example["question"], example["context"])["input_ids"])
#396
len(tokenizer(example["question"], example["context"], max_length=max_length, truncation="only_second")["input_ids"]) #truncation="only_second"è¡¨ç¤ºåªå¯¹å¥å­2è¿›è¡Œæˆªæ–­
#384
```
æˆ‘ä»¬æŠŠè¶…é•¿çš„è¾“å…¥åˆ‡ç‰‡ä¸ºå¤šä¸ªè¾ƒçŸ­çš„è¾“å…¥ï¼Œæ¯ä¸ªè¾“å…¥éƒ½è¦æ»¡è¶³æ¨¡å‹æœ€å¤§é•¿åº¦è¾“å…¥è¦æ±‚ã€‚ç”±äºç­”æ¡ˆå¯èƒ½å­˜åœ¨ä¸åˆ‡ç‰‡çš„åœ°æ–¹ï¼Œå› æ­¤å…è®¸ç›¸é‚»åˆ‡ç‰‡ä¹‹é—´æœ‰`äº¤é›†`ï¼Œtokenizerä¸­é€šè¿‡doc_strideå‚æ•°æ§åˆ¶ã€‚
é¢„è®­ç»ƒæ¨¡å‹çš„tokenizeråŒ…è£…äº†æ–¹æ³•å¸®åŠ©æˆ‘ä»¬å®Œæˆä¸Šè¿°æ­¥éª¤ï¼Œåªéœ€è¦è®¾å®šä¸€äº›å‚æ•°å³å¯ã€‚
```python
max_length = 384 # è¾“å…¥featureçš„æœ€å¤§é•¿åº¦ï¼Œquestionå’Œcontextæ‹¼æ¥ä¹‹å
doc_stride = 128 # 2ä¸ªåˆ‡ç‰‡ä¹‹é—´çš„é‡åˆtokenæ•°é‡ã€‚
```
æ³¨æ„ï¼šä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬åªå¯¹contextè¿›è¡Œåˆ‡ç‰‡ï¼Œä¸ä¼šå¯¹é—®é¢˜è¿›è¡Œåˆ‡ç‰‡ï¼Œç”±äºcontextæ˜¯æ‹¼æ¥åœ¨questionåé¢çš„ï¼Œå¯¹åº”ç€ç¬¬2ä¸ªæ–‡æœ¬ï¼Œæ‰€ä»¥ä½¿ç”¨only_secondæ§åˆ¶ã€‚tokenizerä½¿ç”¨doc_strideæ§åˆ¶åˆ‡ç‰‡ä¹‹é—´çš„é‡åˆé•¿åº¦ã€‚
```python
tokenized_example = tokenizer(
    example["question"],
    example["context"],
    max_length=max_length,
    truncation="only_second",
    return_overflowing_tokens=True,
    stride=doc_stride
)
[len(x) for x in tokenized_example["input_ids"]]
#[384, 157]
#æˆ‘ä»¬å¯ä»¥å°†é¢„å¤„ç†åçš„token IDsï¼Œinput_idsè¿˜åŸä¸ºæ–‡æœ¬æ ¼å¼ï¼Œæ–¹ä¾¿æ£€æŸ¥åˆ‡ç‰‡ç»“æœã€‚å¯ä»¥å‘ç°tokenizerè‡ªåŠ¨å¸®æˆ‘ä»¬ä¸ºç¬¬äºŒä¸ªåˆ‡ç‰‡çš„contextæ‹¼æ¥äº†questionæ–‡æœ¬ã€‚
for i, x in enumerate(tokenized_example["input_ids"][:2]):
    print("åˆ‡ç‰‡: {}".format(i))
    print(tokenizer.decode(x))
#åˆ‡ç‰‡: 0
#[CLS] how many wins does the notre dame men's basketball team have? [SEP] the men's basketball team has over 1, 600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 ncaa tournaments. former player austin carr holds the record for most points scored in a single game of the tournament with 61. although the team has never won the ncaa tournament, they were named by the helms athletic foundation as national champions twice. the team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending ucla's record 88 - game winning streak in 1974. the team has beaten an additional eight number - one teams, and those nine wins rank second, to ucla's 10, all - time in wins against the top team. the team plays in newly renovated purcell pavilion ( within the edmund p. joyce center ), which reopened for the beginning of the 2009 â€“ 2010 season. the team is coached by mike brey, who, as of the 2014 â€“ 15 season, his fifteenth at notre dame, has achieved a 332 - 165 record. in 2009 they were invited to the nit, where they advanced to the semifinals but were beaten by penn state who went on and beat baylor in the championship. the 2010 â€“ 11 team concluded its regular season ranked number seven in the country, with a record of 25 â€“ 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were [SEP]
#åˆ‡ç‰‡: 1
#[CLS] how many wins does the notre dame men's basketball team have? [SEP] championship. the 2010 â€“ 11 team concluded its regular season ranked number seven in the country, with a record of 25 â€“ 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were the most by the fighting irish team since 1908 - 09. [SEP]
```
æˆ‘ä»¬çŸ¥é“æœºå™¨é—®ç­”æ¨¡å‹å°†ä½¿ç”¨ç­”æ¡ˆçš„ä½ç½®ï¼ˆç­”æ¡ˆçš„èµ·å§‹ä½ç½®å’Œç»“æŸä½ç½®ï¼Œstartå’Œendï¼‰ä½œä¸ºè®­ç»ƒæ ‡ç­¾ï¼ˆè€Œä¸æ˜¯ç­”æ¡ˆçš„token IDSï¼‰ã€‚é‚£ä¹ˆç”±äºè¿›è¡Œäº†åˆ‡ç‰‡ï¼Œä¸€ä¸ªæ–°çš„é—®é¢˜å‡ºç°äº†ï¼šç­”æ¡ˆæ‰€åœ¨çš„ä½ç½®è¢«æ”¹å˜äº†ï¼Œå› æ­¤éœ€è¦é‡æ–°å¯»æ‰¾ç­”æ¡ˆæ‰€åœ¨ä½ç½®ï¼ˆç›¸å¯¹äºæ¯ä¸€ç‰‡contextå¼€å¤´çš„ç›¸å¯¹ä½ç½®ï¼‰ã€‚

æ‰€ä»¥åˆ‡ç‰‡éœ€è¦å’ŒåŸå§‹è¾“å…¥æœ‰ä¸€ä¸ªå¯¹åº”å…³ç³»ï¼Œæ¯ä¸ªtokenåœ¨åˆ‡ç‰‡åcontextçš„ä½ç½®å’ŒåŸå§‹è¶…é•¿contexté‡Œä½ç½®çš„å¯¹åº”å…³ç³»ã€‚
#### è·å–åˆ‡ç‰‡å‰åçš„ä½ç½®å¯¹åº”å…³ç³»
åœ¨tokenizeré‡Œå¯ä»¥ä½¿ç”¨`return_offsets_mapping=True`å‚æ•°å¾—åˆ°è¿™ä¸ªå¯¹åº”å…³ç³»çš„mapï¼š
```python
tokenized_example = tokenizer(
    example["question"],
    example["context"],
    max_length=max_length,
    truncation="only_second",
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
    stride=doc_stride
)
```
æˆ‘ä»¬æ‰“å°tokenized_exampleåˆ‡ç‰‡1ï¼ˆä¹Ÿå°±æ˜¯ç¬¬äºŒä¸ªåˆ‡ç‰‡ï¼Œå‰30ä¸ªtokensï¼‰åœ¨åŸå§‹contextç‰‡é‡Œçš„ä½ç½®ã€‚æ³¨æ„ç‰¹æ®Štokenæ˜¯ï¼ˆå¦‚[CLS]è®¾å®šä¸º(0, 0)ï¼‰ï¼Œæ˜¯å› ä¸ºè¿™ä¸ªtokenä¸å±äºqeustionæˆ–è€…answerçš„ä¸€éƒ¨åˆ†ã€‚ç¬¬2ä¸ªtokenå¯¹åº”çš„èµ·å§‹å’Œç»“æŸä½ç½®æ˜¯0å’Œ3ã€‚å¯ä»¥å‘ç°åˆ‡ç‰‡1contextéƒ¨åˆ†ç¬¬2ä¸ªtokenæ²¡æœ‰ä»(0,N)æ ‡è®°ï¼Œè€Œæ˜¯ä»è®°å½•äº†å…¶åœ¨åŸæœ¬è¶…é•¿æ–‡æœ¬ä¸­çš„ä½ç½®ã€‚
```python
# æ‰“å°åˆ‡ç‰‡å‰åä½ç½®ä¸‹æ ‡çš„å¯¹åº”å…³ç³»
print(tokenized_example["offset_mapping"][1][:30])
#[(0, 0), (0, 3), (4, 8), (9, 13), (14, 18), (19, 22), (23, 28), (29, 33), (34, 37), (37, 38), (38, 39), (40, 50), (51, 55), (56, 60), (60, 61), (0, 0), (1093, 1105), (1105, 1106), (1107, 1110), (1111, 1115), (1115, 1116), (1116, 1118), (1119, 1123), (1124, 1133), (1134, 1137), (1138, 1145), (1146, 1152), (1153, 1159), (1160, 1166), (1167, 1172)]
```
å› æ­¤æˆ‘ä»¬å¯ä»¥æ ¹æ®åˆ‡ç‰‡åçš„token idè½¬åŒ–å¯¹åº”çš„tokenï¼Œç„¶åä½¿ç”¨offset_mappingå‚æ•°æ˜ å°„å›åˆ‡ç‰‡å‰çš„tokenä½ç½®ï¼Œæ‰¾åˆ°åŸå§‹ä½ç½®çš„tokensã€‚ç”±äºquestionæ‹¼æ¥åœ¨contextå‰é¢ï¼Œæ‰€ä»¥ç›´æ¥ä»questioné‡Œæ ¹æ®ä¸‹æ ‡æ‰¾ã€‚
```python
first_token_id = tokenized_example["input_ids"][0][1] #2129
offsets = tokenized_example["offset_mapping"][0][1] #(0, 3)
print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example["question"][offsets[0]:offsets[1]])
#how How
```
å› æ­¤æˆ‘ä»¬å¾—åˆ°æ›´æ–°ç­”æ¡ˆç›¸å¯¹åˆ‡ç‰‡contextä½ç½®çš„æµç¨‹ï¼š

1. æˆ‘ä»¬é¦–å…ˆæ‰¾åˆ°contextåœ¨å¥å­1å’Œå¥å­2æ‹¼æ¥åçš„å¥å­ä¸­çš„èµ·å§‹ä½ç½®token_start_indexã€ç»ˆæ­¢ä½ç½®token_end_indexï¼›
2. ç„¶ååˆ¤æ–­ç­”æ¡ˆæ˜¯å¦åœ¨æ–‡æœ¬åŒºé—´å¤–éƒ¨ï¼š
    2.1. è‹¥åœ¨ï¼Œåˆ™æ›´æ–°ç­”æ¡ˆçš„ä½ç½®ï¼›
    2.2. è‹¥ä¸åœ¨ï¼Œåˆ™è®©ç­”æ¡ˆæ ‡æ³¨åœ¨CLS tokenä½ç½®ã€‚

æœ€ç»ˆæˆ‘ä»¬å¯ä»¥æ›´æ–°æ ‡æ³¨çš„ç­”æ¡ˆåœ¨é¢„å¤„ç†ä¹‹åçš„featuresé‡Œçš„ä½ç½®ï¼š
```python
answers = example["answers"] #ç­”æ¡ˆ{'answer_start': [30], 'text': ['over 1,600']}
start_char = answers["answer_start"][0] #ç­”æ¡ˆèµ·å§‹ä½ç½®30
end_char = start_char + len(answers["text"][0])#ç­”æ¡ˆç»ˆæ­¢ä½ç½®40

# æ‰¾åˆ°å½“å‰æ–‡æœ¬çš„Start token index.
token_start_index = 0 #å¾—åˆ°contextåœ¨å¥å­1å’Œå¥å­2æ‹¼æ¥åçš„å¥å­ä¸­çš„èµ·å§‹ä½ç½®
while sequence_ids[token_start_index] != 1: #sequence_idsåŒºåˆ†questionå’Œcontext
    token_start_index += 1 

# æ‰¾åˆ°å½“å‰æ–‡æœ¬çš„End token index.
token_end_index = len(tokenized_example["input_ids"][0]) - 1#å¾—åˆ°contextåœ¨å¥å­1å’Œå¥å­2æ‹¼æ¥åçš„å¥å­ä¸­çš„ç»ˆæ­¢ä½ç½®ï¼Œå¯èƒ½è¿˜è¦å»æ‰ä¸€äº›padding
while sequence_ids[token_end_index] != 1:
    token_end_index -= 1

# æ£€æµ‹ç­”æ¡ˆæ˜¯å¦åœ¨æ–‡æœ¬åŒºé—´çš„å¤–éƒ¨ï¼Œè¿™ç§æƒ…å†µä¸‹æ„å‘³ç€è¯¥æ ·æœ¬çš„æ•°æ®æ ‡æ³¨åœ¨CLS tokenä½ç½®ã€‚
offsets = tokenized_example["offset_mapping"][0]
if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char): #ç­”æ¡ˆåœ¨æ–‡æœ¬å†…
    # å°†token_start_indexå’Œtoken_end_indexç§»åŠ¨åˆ°answeræ‰€åœ¨ä½ç½®çš„ä¸¤ä¾§.
    # æ³¨æ„ï¼šç­”æ¡ˆåœ¨æœ€æœ«å°¾çš„è¾¹ç•Œæ¡ä»¶.
    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:
        token_start_index += 1 #ä¹‹å‰çš„token_start_indexåœ¨contextçš„ç¬¬ä¸€ä¸ªtokenä½ç½®
    start_position = token_start_index - 1
    while offsets[token_end_index][1] >= end_char:
        token_end_index -= 1#ä¹‹å‰çš„token_end_indexåœ¨contextçš„æœ€åä¸€ä¸ªtokenä½ç½®
    end_position = token_end_index + 1
    print("start_position: {}, end_position: {}".format(start_position, end_position))
else: #ç­”æ¡ˆåœ¨æ–‡æœ¬å¤–
    print("The answer is not in this feature.")
```
start_position: 23, end_position: 26

æˆ‘ä»¬å¯¹ç­”æ¡ˆçš„ä½ç½®è¿›è¡ŒéªŒè¯ï¼šä½¿ç”¨ç­”æ¡ˆæ‰€åœ¨ä½ç½®ä¸‹æ ‡ï¼Œå–åˆ°å¯¹åº”çš„token IDï¼Œç„¶åè½¬åŒ–ä¸ºæ–‡æœ¬ï¼Œç„¶åå’ŒåŸå§‹ç­”æ¡ˆè¿›è¡Œä½†å¯¹æ¯”ã€‚
```python
print(tokenizer.decode(tokenized_example["input_ids"][0][start_position: end_position+1]))
print(answers["text"][0])
```
over 1, 600 over 1,600

æ­¤å¤–ï¼Œè¿˜éœ€è¦æ³¨æ„çš„æ˜¯ï¼šæœ‰æ—¶å€™questionæ‹¼æ¥contextï¼Œè€Œæœ‰æ—¶å€™æ˜¯contextæ‹¼æ¥questionï¼Œä¸åŒçš„æ¨¡å‹æœ‰ä¸åŒçš„è¦æ±‚ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦ä½¿ç”¨padding_sideå‚æ•°æ¥æŒ‡å®šã€‚
```python
pad_on_right = tokenizer.padding_side == "right" #contextåœ¨å³è¾¹
```
ç°åœ¨ï¼ŒæŠŠæ‰€æœ‰æ­¥éª¤åˆå¹¶åˆ°ä¸€èµ·ã€‚å¦‚æœallow_impossible_answersè¿™ä¸ªå‚æ•°æ˜¯Falseçš„è¯ï¼Œæ— ç­”æ¡ˆçš„æ ·æœ¬éƒ½ä¼šè¢«æ‰”æ‰ã€‚
```python
def prepare_train_features(examples):
    # æ—¢è¦å¯¹examplesè¿›è¡Œtruncationï¼ˆæˆªæ–­ï¼‰å’Œpaddingï¼ˆè¡¥å…¨ï¼‰è¿˜è¦è¿˜è¦ä¿ç•™æ‰€æœ‰ä¿¡æ¯ï¼Œæ‰€ä»¥è¦ç”¨çš„åˆ‡ç‰‡çš„æ–¹æ³•ã€‚
    # æ¯ä¸€ä¸ªè¶…é•¿æ–‡æœ¬exampleä¼šè¢«åˆ‡ç‰‡æˆå¤šä¸ªè¾“å…¥ï¼Œç›¸é‚»ä¸¤ä¸ªè¾“å…¥ä¹‹é—´ä¼šæœ‰äº¤é›†ã€‚
    tokenized_examples = tokenizer(
        examples["question" if pad_on_right else "context"],
        examples["context" if pad_on_right else "question"],
        truncation="only_second" if pad_on_right else "only_first",
        max_length=max_length,
        stride=doc_stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    # æˆ‘ä»¬ä½¿ç”¨overflow_to_sample_mappingå‚æ•°æ¥æ˜ å°„åˆ‡ç‰‡ç‰‡IDåˆ°åŸå§‹IDã€‚
    # æ¯”å¦‚æœ‰2ä¸ªexpamplesè¢«åˆ‡æˆ4ç‰‡ï¼Œé‚£ä¹ˆå¯¹åº”æ˜¯[0, 0, 1, 1]ï¼Œå‰ä¸¤ç‰‡å¯¹åº”åŸæ¥çš„ç¬¬ä¸€ä¸ªexampleã€‚
    sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
    # offset_mappingä¹Ÿå¯¹åº”4ç‰‡
    # offset_mappingå‚æ•°å¸®åŠ©æˆ‘ä»¬æ˜ å°„åˆ°åŸå§‹è¾“å…¥ï¼Œç”±äºç­”æ¡ˆæ ‡æ³¨åœ¨åŸå§‹è¾“å…¥ä¸Šï¼Œæ‰€ä»¥æœ‰åŠ©äºæˆ‘ä»¬æ‰¾åˆ°ç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸä½ç½®ã€‚
    offset_mapping = tokenized_examples.pop("offset_mapping")

    # é‡æ–°æ ‡æ³¨æ•°æ®
    tokenized_examples["start_positions"] = []
    tokenized_examples["end_positions"] = []

    for i, offsets in enumerate(offset_mapping):
        # å¯¹æ¯ä¸€ç‰‡è¿›è¡Œå¤„ç†
        # å°†æ— ç­”æ¡ˆçš„æ ·æœ¬æ ‡æ³¨åˆ°CLSä¸Š
        input_ids = tokenized_examples["input_ids"][i]
        cls_index = input_ids.index(tokenizer.cls_token_id)

        # åŒºåˆ†questionå’Œcontext
        sequence_ids = tokenized_examples.sequence_ids(i)

        # æ‹¿åˆ°åŸå§‹çš„example ä¸‹æ ‡.
        sample_index = sample_mapping[i]
        answers = examples["answers"][sample_index]
        # å¦‚æœæ²¡æœ‰ç­”æ¡ˆï¼Œåˆ™ä½¿ç”¨CLSæ‰€åœ¨çš„ä½ç½®ä¸ºç­”æ¡ˆ.
        if len(answers["answer_start"]) == 0:
            tokenized_examples["start_positions"].append(cls_index)
            tokenized_examples["end_positions"].append(cls_index)
        else:
            # ç­”æ¡ˆçš„characterçº§åˆ«Start/endä½ç½®.
            start_char = answers["answer_start"][0]
            end_char = start_char + len(answers["text"][0])

            # æ‰¾åˆ°tokençº§åˆ«çš„index start.
            token_start_index = 0
            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):
                token_start_index += 1

            # æ‰¾åˆ°tokençº§åˆ«çš„index end.
            token_end_index = len(input_ids) - 1
            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):
                token_end_index -= 1

            # æ£€æµ‹ç­”æ¡ˆæ˜¯å¦è¶…å‡ºæ–‡æœ¬é•¿åº¦ï¼Œè¶…å‡ºçš„è¯ä¹Ÿä½¿ç”¨CLS indexä½œä¸ºæ ‡æ³¨.
            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):
                tokenized_examples["start_positions"].append(cls_index)
                tokenized_examples["end_positions"].append(cls_index)
            else:
                # å¦‚æœä¸è¶…å‡ºåˆ™æ‰¾åˆ°ç­”æ¡ˆtokençš„startå’Œendä½ç½®ã€‚.
                # Note: we could go after the last offset if the answer is the last word (edge case).
                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:
                    token_start_index += 1
                tokenized_examples["start_positions"].append(token_start_index - 1)
                while offsets[token_end_index][1] >= end_char:
                    token_end_index -= 1
                tokenized_examples["end_positions"].append(token_end_index + 1)

    return tokenized_examples
```
ä»¥ä¸Šçš„é¢„å¤„ç†å‡½æ•°å¯ä»¥å¤„ç†ä¸€ä¸ªæ ·æœ¬ï¼Œä¹Ÿå¯ä»¥å¤„ç†å¤šä¸ªæ ·æœ¬exapmlesã€‚å¦‚æœæ˜¯å¤„ç†å¤šä¸ªæ ·æœ¬ï¼Œåˆ™è¿”å›çš„æ˜¯å¤šä¸ªæ ·æœ¬è¢«é¢„å¤„ç†ä¹‹åçš„ç»“æœlistã€‚

æ¥ä¸‹æ¥ä½¿ç”¨mapå‡½æ•°å¯¹æ•°æ®é›†datasetsé‡Œé¢ä¸‰ä¸ªæ ·æœ¬é›†åˆçš„æ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œå°†é¢„å¤„ç†å‡½æ•°prepare_train_featuresåº”ç”¨åˆ°ï¼ˆmap)æ‰€æœ‰æ ·æœ¬ä¸Šã€‚å‚æ•°batched=Trueå¯ä»¥æ‰¹é‡å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ã€‚è¿™æ˜¯ä¸ºäº†å……åˆ†åˆ©ç”¨å‰é¢åŠ è½½fast_tokenizerçš„ä¼˜åŠ¿ï¼Œå®ƒå°†ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘åœ°å¤„ç†æ‰¹ä¸­çš„æ–‡æœ¬ã€‚
```python
tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets["train"].column_names)
```
### å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹
#### åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
åšæœºå™¨é—®ç­”ä»»åŠ¡ï¼Œé‚£ä¹ˆéœ€è¦ä¸€ä¸ªèƒ½è§£å†³è¿™ä¸ªä»»åŠ¡çš„æ¨¡å‹ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨AutoModelForQuestionAnswering è¿™ä¸ªç±»ã€‚
å’Œä¹‹å‰å‡ ç¯‡åšå®¢æåˆ°çš„åŠ è½½æ–¹å¼ç›¸åŒä¸å†èµ˜è¿°ã€‚
```python
from transformers import AutoModelForQuestionAnswering
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)
```
#### è®¾å®šè®­ç»ƒå‚æ•°
ä¸ºäº†èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ªTrainerè®­ç»ƒå·¥å…·ï¼Œæˆ‘ä»¬è¿˜éœ€è¦è®­ç»ƒçš„è®¾å®š/å‚æ•° TrainingArgumentsã€‚è¿™ä¸ªè®­ç»ƒè®¾å®šåŒ…å«äº†èƒ½å¤Ÿå®šä¹‰è®­ç»ƒè¿‡ç¨‹çš„æ‰€æœ‰å±æ€§ã€‚
```python
from transformers import TrainingArguments
args = TrainingArguments(
    f"test-squad",
    evaluation_strategy = "epoch",
    learning_rate=2e-5, #å­¦ä¹ ç‡
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=3, # è®­ç»ƒçš„æ¬¡æ•°
    weight_decay=0.01,
)
```
#### æ•°æ®æ”¶é›†å™¨data collator
æ¥ä¸‹æ¥éœ€è¦å‘Šè¯‰Trainerå¦‚ä½•ä»é¢„å¤„ç†çš„è¾“å…¥æ•°æ®ä¸­æ„é€ batchã€‚æˆ‘ä»¬ä½¿ç”¨æ•°æ®æ”¶é›†å™¨data collatorï¼Œå°†ç»é¢„å¤„ç†çš„è¾“å…¥åˆ†batchå†æ¬¡å¤„ç†åå–‚ç»™æ¨¡å‹ã€‚

æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªdefault_data_collatorå°†é¢„å¤„ç†å¥½çš„æ•°æ®å–‚ç»™æ¨¡å‹ã€‚
```python
from transformers import default_data_collator
data_collator = default_data_collator
```
#### å®šä¹‰è¯„ä¼°æ–¹æ³•
æ³¨æ„ï¼Œæœ¬æ¬¡è®­ç»ƒçš„æ—¶å€™ï¼Œæˆ‘ä»¬å°†åªä¼šè®¡ç®—lossï¼Œæš‚æ—¶ä¸å®šä¹‰è¯„ä¼°æ–¹æ³•ã€‚

### å¼€å§‹è®­ç»ƒ
å°†æ•°æ®/æ¨¡å‹/å‚æ•°ä¼ å…¥Trainerå³å¯ï¼š
```python
from transformers import Trainer
trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
trainer.train()
#åŠæ—¶ä¿å­˜æ¨¡å‹
trainer.save_model("test-squad-trained")
```
æ¨¡å‹çš„è¾“å‡ºæ˜¯answeræ‰€åœ¨start/endä½ç½®çš„logitsã€‚
ç”¨ç¬¬ä¸€ä¸ªbatchæ¥ä¸¾ä¸€ä¸ªä¾‹å­ï¼š
```python
import torch

for batch in trainer.get_eval_dataloader(): #äº§ç”Ÿbatchçš„è¿­ä»£å™¨
    break
batch = {k: v.to(trainer.args.device) for k, v in batch.items()}
with torch.no_grad():
    output = trainer.model(**batch)
output.keys()
#odict_keys(['loss', 'start_logits', 'end_logits'])
```
è¿˜è®°å¾—æˆ‘ä»¬åœ¨åˆ†æBERT-based Modelæºç æ—¶ï¼Œä¹Ÿå¯ä»¥çœ‹å‡ºBertForQuestionAnsweringçš„è¾“å‡ºåŒ…æ‹¬ï¼š
```python
return QuestionAnsweringModelOutput(
            loss=total_loss,
            start_logits=start_logits,
            end_logits=end_logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )
```
æˆ‘ä»¬åœ¨è¾“å‡ºé¢„æµ‹ç»“æœçš„æ—¶å€™ä¸éœ€è¦çœ‹lossï¼Œæ¯ä¸ªfeatureï¼ˆåˆ‡ç‰‡ï¼‰é‡Œçš„æ¯ä¸ªtokenéƒ½ä¼šæœ‰ä¸¤ä¸ªlogitå€¼ï¼ˆåˆ†åˆ«ç»„æˆstart_logitsï¼Œend_logitsï¼‰ï¼Œç›´æ¥æ ¹æ®logitsæ‰¾åˆ°ç­”æ¡ˆçš„ä½ç½®å³å¯ã€‚
### å¦‚ä½•æ ¹æ®logitsæ‰¾åˆ°ç­”æ¡ˆçš„ä½ç½®ï¼Ÿ
- æ–¹æ³•1
- é¢„æµ‹answeræœ€ç®€å•çš„æ–¹æ³•å°±æ˜¯é€‰æ‹©start_logitsé‡Œæœ€å¤§çš„ä¸‹æ ‡ä½œä¸ºanswerèµ·å§‹ä½ç½®ï¼Œend_logitsé‡Œæœ€å¤§ä¸‹æ ‡ä½œä¸ºanswerçš„ç»“æŸä½ç½®ã€‚
```python
output.start_logits.shape, output.end_logits.shape
#(torch.Size([16, 384]), torch.Size([16, 384]))

output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)
# (tensor([ 46,  57,  78,  43, 118,  15,  72,  35,  15,  34,  73,  41,  80,  91, 156,  35], device='cuda:0'),tensor([ 47,  58,  81,  55, 118, 110,  75,  37, 110,  36,  76,  53,  83,  94, 158,  35], device='cuda:0'))
```
- æ–¹æ³•2
- è¯¥ç­–ç•¥å¤§éƒ¨åˆ†æƒ…å†µä¸‹éƒ½æ˜¯ä¸é”™çš„ã€‚ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬çš„è¾“å…¥å‘Šè¯‰æˆ‘ä»¬æ‰¾ä¸åˆ°ç­”æ¡ˆï¼šæ¯”å¦‚startçš„ä½ç½®æ¯”endçš„ä½ç½®ä¸‹æ ‡å¤§ï¼Œæˆ–è€…startå’Œendçš„ä½ç½®æŒ‡å‘äº†questionã€‚è¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿ
    è¿™ä¸ªæ—¶å€™ï¼Œç®€å•çš„æ–¹æ³•æ˜¯ç»§ç»­éœ€è¦é€‰æ‹©ç¬¬2å¥½çš„é¢„æµ‹ä½œä¸ºç­”æ¡ˆï¼Œå®åœ¨ä¸è¡Œçœ‹ç¬¬3å¥½çš„é¢„æµ‹ï¼Œä»¥æ­¤ç±»æ¨ã€‚

    ä½†æ˜¯æ–¹æ³•2ä¸å¤ªå®¹æ˜“æ‰¾åˆ°å¯è¡Œçš„ç­”æ¡ˆï¼Œè¿˜æœ‰æ²¡æœ‰æ›´åˆç†ä¸€äº›çš„æ–¹æ³•å‘¢ï¼Ÿ
- æ–¹æ³•3
- åˆ†ä¸ºå››ä¸ªæ­¥éª¤ï¼š

æˆ‘ä»¬å…ˆæ‰¾åˆ°æœ€å¥½çš„n_best_sizeä¸ªï¼ˆè‡ªå®šä¹‰ï¼‰startå’Œendå¯¹åº”çš„å¯èƒ½çš„å¤‡é€‰èµ·å§‹ç‚¹å’Œç»ˆæ­¢ç‚¹ï¼›
ä»ä¸­å…ˆæ„å»ºåˆç†çš„å¤‡é€‰ç­”æ¡ˆï¼Œä¸åˆç†çš„æƒ…å†µåŒ…æ‹¬ä»¥ä¸‹å‡ ç§ï¼š
start>endçš„å¤‡é€‰èµ·å§‹ç‚¹å’Œç»ˆæ­¢ç‚¹ï¼›
startæˆ–endè¶…è¿‡æœ€å¤§é•¿åº¦ï¼›
startå’Œendä½ç½®å¯¹åº”çš„æ–‡æœ¬åœ¨questioné‡Œé¢è€Œä¸åœ¨contexté‡Œé¢ï¼ˆè¿™é‡ŒåŸ‹äº†ä¸€ä¸ªé›·ï¼Œï¼‰ï¼›
ç„¶åå°†åˆç†å¤‡é€‰ç­”æ¡ˆçš„startå’Œendçš„logitsç›¸åŠ å¾—åˆ°æ–°çš„æ‰“åˆ†ï¼›
æœ€åæˆ‘ä»¬æ ¹æ®scoreå¯¹valid_answersè¿›è¡Œæ’åºï¼Œæ‰¾åˆ°æœ€å¥½çš„é‚£ä¸€ä¸ªåšä¸ºç­”æ¡ˆã€‚
ä¸ºäº†æ‰¾åˆ°ç¬¬3ç§ä¸åˆç†çš„æƒ…å†µï¼Œæˆ‘ä»¬åœ¨validationçš„featuresé‡Œæ·»åŠ ä»¥ä¸‹ä¸¤ä¸ªä¿¡æ¯ï¼š

äº§ç”Ÿfeatureçš„example ID-overflow_to_sample_mappingï¼šç”±äºæ¯ä¸ªexampleå¯èƒ½ä¼šäº§ç”Ÿå¤šä¸ªfeatureï¼Œæ‰€ä»¥æ¯ä¸ªfeature/åˆ‡ç‰‡çš„featureéœ€è¦çŸ¥é“ä»–ä»¬å¯¹åº”çš„exampleæ˜¯å“ªä¸€ä¸ªã€‚
offset mapping-offset_mappingï¼š å°†æ¯ä¸ªåˆ‡ç‰‡tokensçš„ä½ç½®æ˜ å°„å›åŸå§‹æ–‡æœ¬åŸºäºcharacterçš„ä¸‹æ ‡ä½ç½®ï¼ŒæŠŠquestionéƒ¨åˆ†çš„offset_mappingç”¨Noneæ©ç ï¼Œcontextéƒ¨åˆ†ä¿ç•™ä¸å˜ã€‚
æˆ‘ä»¬ç°åœ¨åˆ©ç”¨ä¸€ä¸ªprepare_validation_featureså‡½æ•°å¤„ç†validationéªŒè¯é›†ï¼Œæ·»åŠ ä¸Šé¢ä¸¤ä¸ªä¿¡æ¯ï¼Œè¯¥å‡½æ•°å’Œå¤„ç†è®­ç»ƒçš„æ—¶å€™çš„prepare_train_featuresç¨æœ‰ä¸åŒã€‚ç„¶ååˆ©ç”¨å¤„ç†åçš„éªŒè¯é›†è¿›è¡Œè¯„ä¼°
```python
def prepare_validation_features(examples):
    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results
    # in one example possible giving several features when a context is long, each of those features having a
    # context that overlaps a bit the context of the previous feature.
    tokenized_examples = tokenizer(
        examples["question" if pad_on_right else "context"],
        examples["context" if pad_on_right else "question"],
        truncation="only_second" if pad_on_right else "only_first",
        max_length=max_length,
        stride=doc_stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    # Since one example might give us several features if it has a long context, we need a map from a feature to
    # its corresponding example. This key gives us just that.
    # æˆ‘ä»¬ä½¿ç”¨overflow_to_sample_mappingå‚æ•°æ¥æ˜ å°„åˆ‡ç‰‡ç‰‡IDåˆ°åŸå§‹IDã€‚
    # æ¯”å¦‚æœ‰2ä¸ªexpamplesè¢«åˆ‡æˆ4ç‰‡ï¼Œé‚£ä¹ˆå¯¹åº”æ˜¯[0, 0, 1, 1]ï¼Œå‰ä¸¤ç‰‡å¯¹åº”åŸæ¥çš„ç¬¬ä¸€ä¸ªexampleã€‚
    sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")

    # We keep the example_id that gave us this feature and we will store the offset mappings.
    tokenized_examples["example_id"] = []

    for i in range(len(tokenized_examples["input_ids"])):
        # Grab the sequence corresponding to that example (to know what is the context and what is the question).
        sequence_ids = tokenized_examples.sequence_ids(i)
        context_index = 1 if pad_on_right else 0

        # One example can give several spans, this is the index of the example containing this span of text.
        # æ‹¿åˆ°åŸå§‹çš„example ä¸‹æ ‡.
        sample_index = sample_mapping[i]
        tokenized_examples["example_id"].append(examples["id"][sample_index])

        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token
        # position is part of the context or not.
        # æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨contextä¸­ï¼Œå¦‚æœä¸åœ¨offset_mappingä¸ºNone
        # å…¶å®å°±æ˜¯æŠŠquestionéƒ¨åˆ†çš„offset_mappingç”¨Noneæ©ç ï¼Œcontextéƒ¨åˆ†ä¿ç•™ä¸å˜
        tokenized_examples["offset_mapping"][i] = [
            (o if sequence_ids[k] == context_index else None)
            for k, o in enumerate(tokenized_examples["offset_mapping"][i])
        ]

    return tokenized_examples
```
å’Œä¹‹å‰ä¸€æ ·å°†`prepare_validation_features`å‡½æ•°åº”ç”¨åˆ°æ¯ä¸ªéªŒè¯é›†åˆçš„æ ·æœ¬ä¸Šã€‚
```python
validation_features = datasets["validation"].map(
    prepare_validation_features,
    batched=True,
    remove_columns=datasets["validation"].column_names
)
```
ä½¿ç”¨`Trainer.predict`æ–¹æ³•è·å¾—æ‰€æœ‰é¢„æµ‹ç»“æœï¼š
```python
raw_predictions = trainer.predict(validation_features)
#è¿™ä¸ª Trainer éšè—äº† ä¸€äº›æ¨¡å‹è®­ç»ƒæ—¶å€™æ²¡æœ‰ä½¿ç”¨çš„å±æ€§(è¿™é‡Œæ˜¯ example_idå’Œoffset_mappingï¼Œåå¤„ç†çš„æ—¶å€™ä¼šç”¨åˆ°)ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦æŠŠè¿™äº›è®¾ç½®å›æ¥:
validation_features.set_format(type=validation_features.format["type"], columns=list(validation_features.features.keys()))
#ç»è¿‡å‰é¢çš„prepare_validation_featureså‡½æ•°å¤„ç†ï¼Œå½“ä¸€ä¸ªtokenä½ç½®å¯¹åº”questionéƒ¨åˆ†offset mappingsä¸ºNoneï¼Œæ‰€ä»¥æˆ‘ä»¬æ ¹æ®offset mappingå¯ä»¥åˆ¤æ–­tokenæ˜¯å¦åœ¨contexté‡Œé¢ã€‚

#æ›´è¿‘ä¸€æ­¥åœ°ï¼Œæˆ‘ä»¬ç”¨max_answer_lengthæ§åˆ¶å»æ‰ç‰¹åˆ«é•¿çš„ç­”æ¡ˆã€‚
n_best_size = 20
max_answer_length = 30

start_logits = output.start_logits[0].cpu().numpy()
end_logits = output.end_logits[0].cpu().numpy()
offset_mapping = validation_features[0]["offset_mapping"]
# The first feature comes from the first example. For the more general case, we will need to be match the example_id to
# an example index
context = datasets["validation"][0]["context"]
# æ”¶é›†æœ€ä½³çš„startå’Œend logitsçš„ä½ç½®
# Gather the indices the best start/end logits:
start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()
end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()
valid_answers = []
for start_index in start_indexes:
    for end_index in end_indexes:
        
        # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond
        # to part of the input_ids that are not in the context.
        if (#ç­”æ¡ˆä¸åˆç†
            start_index >= len(offset_mapping)
            or end_index >= len(offset_mapping)
            or offset_mapping[start_index] is None
            or offset_mapping[end_index] is None
        ):
            continue
        # Don't consider answers with a length that is either < 0 or > max_answer_length.
        if end_index < start_index or end_index - start_index + 1 > max_answer_length:#ç­”æ¡ˆä¸åˆç†
            continue
        if start_index <= end_index: # We need to refine that test to check the answer is inside the context # å¦‚æœstartå°äºendï¼Œé‚£ä¹ˆæ˜¯åˆç†çš„å¯èƒ½ç­”æ¡ˆ
            start_char = offset_mapping[start_index][0]
            end_char = offset_mapping[end_index][1]
            valid_answers.append(
                {
                    "score": start_logits[start_index] + end_logits[end_index],
                    "text": context[start_char: end_char]# åç»­éœ€è¦æ ¹æ®tokençš„ä¸‹æ ‡å°†ç­”æ¡ˆæ‰¾å‡ºæ¥
                }
            )
#æœ€åæ ¹æ®`score`å¯¹`valid_answers`è¿›è¡Œæ’åºï¼Œæ‰¾åˆ°æœ€å¥½çš„é‚£ä¸€ä¸ª
valid_answers = sorted(valid_answers, key=lambda x: x["score"], reverse=True)[:n_best_size]
valid_answers
```
è¿™é‡Œè¿˜æœ‰ä¸€ä¸ªé—®é¢˜éœ€è¦æ€è€ƒï¼š

å½“ä¸€ä¸ªexampleè¢«åˆ†æˆå¤šä¸ªåˆ‡ç‰‡è¾“å…¥æ¨¡å‹ï¼Œæ¨¡å‹ä¼šæŠŠè¿™äº›åˆ‡ç‰‡å½“ä½œå¤šä¸ªå•ç‹¬çš„â€œæ ·æœ¬â€è¿›è¡Œè®­ç»ƒï¼Œé‚£æˆ‘ä»¬åœ¨è®¡ç®—æ­£ç¡®ç‡å’Œå¬å›ç‡çš„æ—¶å€™ï¼Œä¸èƒ½ä»¥è¿™å¤šä¸ªåˆ‡ç‰‡ä¸ºå•ä½ç›´æ¥è®¡ç®—ï¼Œè€Œæ˜¯åº”è¯¥å°†å…¶å¯¹åº”çš„ä¸€ä¸ªexampleä¸ºå•ä½è¿›è¡Œè®¡ç®—ã€‚

å¯¹äºä¸Šé¢åœ°ä¾‹å­æ¥è¯´ï¼Œç”±äºç­”æ¡ˆæ­£å¥½åœ¨ç¬¬1ä¸ªfeatureï¼Œè€Œç¬¬1ä¸ªfeatureä¸€å®šæ˜¯æ¥è‡ªäºç¬¬1ä¸ªexampleï¼Œæ‰€ä»¥ç›¸å¯¹å®¹æ˜“ã€‚å¯¹äºä¸€ä¸ªè¶…é•¿exampleäº§ç”Ÿçš„å…¶ä»–fearuresæ¥è¯´ï¼Œéœ€è¦ä¸€ä¸ªfeatureså’Œexamplesè¿›è¡Œæ˜ å°„çš„mapã€‚å› æ­¤ç”±äºä¸€ä¸ªexampleå¯èƒ½è¢«åˆ‡ç‰‡æˆå¤šä¸ªfeaturesï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å°†æ‰€æœ‰featuresé‡Œçš„ç­”æ¡ˆå…¨éƒ¨æ”¶é›†èµ·æ¥ã€‚

ä»¥ä¸‹çš„ä»£ç å°±å°†exmapleçš„ä¸‹æ ‡å’Œfeaturesçš„ä¸‹æ ‡è¿›è¡Œmapæ˜ å°„ã€‚
```python
import collections

examples = datasets["validation"]
features = validation_features

example_id_to_index = {k: i for i, k in enumerate(examples["id"])}
features_per_example = collections.defaultdict(list)
for i, feature in enumerate(features):
    features_per_example[example_id_to_index[feature["example_id"]]].append(i)
```
å¯¹äºåå¤„ç†è¿‡ç¨‹åŸºæœ¬ä¸Šå·²ç»å…¨éƒ¨å®Œæˆäº†ã€‚

ä½†æ˜¯è¿™é‡Œè¿˜è¿˜è¿˜æœ‰ä¸€ä¸ªé—®é¢˜ï¼šå¦‚ä½•è§£å†³æ— ç­”æ¡ˆçš„æƒ…å†µï¼ˆsquad_v2=Trueï¼‰ã€‚

ä»¥ä¸Šçš„ä»£ç éƒ½åªè€ƒè™‘äº†contexté‡Œé¢çš„asnwersï¼Œæˆ‘ä»¬åŒæ ·éœ€è¦å°†æ— ç­”æ¡ˆçš„é¢„æµ‹å¾—åˆ†è¿›è¡Œæœé›†ï¼ˆæ— ç­”æ¡ˆçš„é¢„æµ‹å¯¹åº”äº†CLS tokençš„startå’Œend logitsï¼‰ã€‚å¦‚æœä¸€ä¸ªexampleæ ·æœ¬æœ‰å¤šä¸ªfeaturesï¼Œé‚£ä¹ˆæˆ‘ä»¬è¿˜éœ€è¦åœ¨å¤šä¸ªfeaturesé‡Œé¢„æµ‹æ˜¯ä¸æ˜¯éƒ½æ— ç­”æ¡ˆã€‚æ‰€ä»¥æ— ç­”æ¡ˆçš„æœ€ç»ˆå¾—åˆ†æ˜¯æ‰€æœ‰featuresçš„æ— ç­”æ¡ˆå¾—åˆ†æœ€å°çš„é‚£ä¸ªã€‚ï¼ˆä¸ºä»€ä¹ˆæ˜¯æœ€å°çš„é‚£ä¸ªå‘¢ï¼Ÿå› ä¸ºç­”æ¡ˆå¦‚æœåªåœ¨ä¸€ä¸ªåˆ‡ç‰‡é‡Œï¼Œå…¶ä»–åˆ‡ç‰‡è‚¯å®šæ˜¯æ²¡æœ‰ç­”æ¡ˆçš„ï¼Œå¦‚æœè¦ç¡®ä¿æ•´ä¸ªexampleæ˜¯æ²¡æœ‰ç­”æ¡ˆçš„è¯ï¼Œç›¸å½“äºæœ€æœ‰å¯èƒ½æœ‰ç­”æ¡ˆçš„åˆ‡ç‰‡é‡Œé¢ä¹Ÿæ²¡æœ‰ç­”æ¡ˆï¼‰åªè¦æ— ç­”æ¡ˆçš„æœ€ç»ˆå¾—åˆ†é«˜äºå…¶ä»–æ‰€æœ‰ç­”æ¡ˆçš„å¾—åˆ†ï¼Œé‚£ä¹ˆè¯¥é—®é¢˜å°±æ˜¯æ— ç­”æ¡ˆã€‚

æœ€ç»ˆçš„åå¤„ç†å‡½æ•°ï¼š
```python
from tqdm.auto import tqdm

def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):
    all_start_logits, all_end_logits = raw_predictions
    # Build a map example to its corresponding features.
    example_id_to_index = {k: i for i, k in enumerate(examples["id"])}
    features_per_example = collections.defaultdict(list)
    for i, feature in enumerate(features):
        features_per_example[example_id_to_index[feature["example_id"]]].append(i)

    # The dictionaries we have to fill.
    predictions = collections.OrderedDict()

    # Logging.
    print(f"Post-processing {len(examples)} example predictions split into {len(features)} features.")

    # Let's loop over all the examples!
    for example_index, example in enumerate(tqdm(examples)):
        # Those are the indices of the features associated to the current example.
        feature_indices = features_per_example[example_index]

        min_null_score = None # Only used if squad_v2 is True.
        valid_answers = []
        
        context = example["context"]
        # Looping through all the features associated to the current example.
        for feature_index in feature_indices:
            # We grab the predictions of the model for this feature.
            start_logits = all_start_logits[feature_index]
            end_logits = all_end_logits[feature_index]
            # This is what will allow us to map some the positions in our logits to span of texts in the original
            # context.
            offset_mapping = features[feature_index]["offset_mapping"]

            # Update minimum null prediction.
            cls_index = features[feature_index]["input_ids"].index(tokenizer.cls_token_id)
            feature_null_score = start_logits[cls_index] + end_logits[cls_index]
            if min_null_score is None or min_null_score < feature_null_score:
                min_null_score = feature_null_score

            # Go through all possibilities for the `n_best_size` greater start and end logits.
            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()
            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()
            for start_index in start_indexes:
                for end_index in end_indexes:
                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond
                    # to part of the input_ids that are not in the context.
                    if (
                        start_index >= len(offset_mapping)
                        or end_index >= len(offset_mapping)
                        or offset_mapping[start_index] is None
                        or offset_mapping[end_index] is None
                    ):
                        continue
                    # Don't consider answers with a length that is either < 0 or > max_answer_length.
                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:
                        continue

                    start_char = offset_mapping[start_index][0]
                    end_char = offset_mapping[end_index][1]
                    valid_answers.append(
                        {
                            "score": start_logits[start_index] + end_logits[end_index],
                            "text": context[start_char: end_char]
                        }
                    )
        
        if len(valid_answers) > 0:
            best_answer = sorted(valid_answers, key=lambda x: x["score"], reverse=True)[0]
        else:
            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid
            # failure.
            best_answer = {"text": "", "score": 0.0}
        
        # Let's pick our final answer: the best one or the null answer (only for squad_v2)
        if not squad_v2:
            predictions[example["id"]] = best_answer["text"]
        else:
            answer = best_answer["text"] if best_answer["score"] > min_null_score else ""
            predictions[example["id"]] = answer

    return predictions
```
å°†åå¤„ç†å‡½æ•°åº”ç”¨åˆ°åŸå§‹é¢„æµ‹è¾“å‡ºä¸Šï¼š
```python
final_predictions = postprocess_qa_predictions(datasets["validation"], validation_features, raw_predictions.predictions)
```
åŠ è½½è¯„æµ‹æŒ‡æ ‡
```python
from datasets import load_metric
metric = load_metric("squad_v2" if squad_v2 else "squad")
```
åŸºäºé¢„æµ‹å’Œæ ‡æ³¨å¯¹è¯„æµ‹æŒ‡æ ‡è¿›è¡Œè®¡ç®—ã€‚ä¸ºäº†åˆç†çš„æ¯”è¾ƒï¼Œæˆ‘ä»¬éœ€è¦å°†é¢„æµ‹å’Œæ ‡æ³¨çš„æ ¼å¼ã€‚å¯¹äºsquad2æ¥è¯´ï¼Œè¯„æµ‹æŒ‡æ ‡è¿˜éœ€è¦no_answer_probabilityå‚æ•°ï¼ˆç”±äºå·²ç»æ— ç­”æ¡ˆç›´æ¥è®¾ç½®æˆäº†ç©ºå­—ç¬¦ä¸²ï¼Œæ‰€ä»¥è¿™é‡Œç›´æ¥å°†è¿™ä¸ªå‚æ•°è®¾ç½®ä¸º0.0ï¼‰
```python
if squad_v2:
    formatted_predictions = [{"id": k, "prediction_text": v, "no_answer_probability": 0.0} for k, v in predictions.items()]
else:
    formatted_predictions = [{"id": k, "prediction_text": v} for k, v in final_predictions.items()]
references = [{"id": ex["id"], "answers": ex["answers"]} for ex in datasets["validation"]]
metric.compute(predictions=formatted_predictions, references=references)
```
## ç”Ÿæˆä»»åŠ¡-æœºå™¨ç¿»è¯‘
### ä»»åŠ¡ä»‹ç»
ç¿»è¯‘ä»»åŠ¡ï¼ŒæŠŠä¸€ç§è¯­è¨€ä¿¡æ¯è½¬å˜æˆå¦ä¸€ç§è¯­è¨€ä¿¡æ¯ã€‚æ˜¯å…¸å‹çš„seq2seqä»»åŠ¡ï¼Œè¾“å…¥ä¸ºä¸€ä¸ªåºåˆ—ï¼Œè¾“å‡ºä¸ºä¸å›ºå®šé•¿åº¦ï¼ˆç”±æœºå™¨è‡ªè¡Œå­¦ä¹ ç”Ÿæˆçš„åºåˆ—åº”è¯¥å¤šé•¿ï¼‰çš„åºåˆ—ã€‚

æ¯”å¦‚è¾“å…¥ä¸€å¥ä¸­æ–‡ï¼Œç¿»è¯‘ä¸ºè‹±æ–‡ï¼š
```
è¾“å…¥ï¼šæˆ‘çˆ±ä¸­å›½ã€‚
è¾“å‡ºï¼šI love China.
```
### æ•°æ®åŠ è½½
#### æ•°æ®é›†ä»‹ç»
æˆ‘ä»¬ä½¿ç”¨WMT datasetæ•°æ®é›†ã€‚è¿™æ˜¯ç¿»è¯‘ä»»åŠ¡æœ€å¸¸ç”¨çš„æ•°æ®é›†ä¹‹ä¸€ã€‚å…¶ä¸­åŒ…æ‹¬English/RomanianåŒè¯­ç¿»è¯‘ã€‚
#### åŠ è½½æ•°æ®
è¯¥æ•°æ®çš„åŠ è½½æ–¹å¼åœ¨transformersåº“ä¸­è¿›è¡Œäº†å°è£…ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹è¯­å¥è¿›è¡Œæ•°æ®åŠ è½½ï¼š
```python
pip install datasets transformers sacrebleu sentencepiece
#transformers==4.9.2
#datasets==1.11.0
#sacrebleu==1.5.1
#sentencepiece==0.1.96
from datasets import load_dataset
raw_datasets = load_dataset("wmt16", "ro-en")
raw_datasets["train"][0]
# æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸€å¥è‹±è¯­enå¯¹åº”ä¸€å¥ç½—é©¬å°¼äºšè¯­è¨€ro
# {'translation': {'en': 'Membership of Parliament: see Minutes','ro': 'ComponenÅ£a Parlamentului: a se vedea procesul-verbal'}}
```
![](https://raw.githubusercontent.com/innovation64/Picimg/main/20221031101718.png)
### æ•°æ®é¢„å¤„ç†
#### åˆå§‹åŒ–Tokenizer
```python
from transformers import AutoTokenizer
model_checkpoint = "Helsinki-NLP/opus-mt-en-ro" 
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)
if "mbart" in model_checkpoint:
    tokenizer.src_lang = "en-XX"
    tokenizer.tgt_lang = "ro-RO"
```
#### è½¬åŒ–æˆå¯¹åº”ä»»åŠ¡è¾“å…¥æ¨¡å‹çš„æ ¼å¼
```python
with tokenizer.as_target_tokenizer():
    print(tokenizer("Hello, this one sentence!"))
    model_input = tokenizer("Hello, this one sentence!")
    tokens = tokenizer.convert_ids_to_tokens(model_input['input_ids'])
    # æ‰“å°çœ‹ä¸€ä¸‹special toke
    print('tokens: {}'.format(tokens))
#{'input_ids': [10334, 1204, 3, 15, 8915, 27, 452, 59, 29579, 581, 23, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
#tokens: ['â–Hel', 'lo', ',', 'â–', 'this', 'â–o', 'ne', 'â–se', 'nten', 'ce', '!', '</s>']
```
å¦‚æœä½¿ç”¨çš„æ˜¯T5é¢„è®­ç»ƒæ¨¡å‹çš„checkpointsï¼Œéœ€è¦å¯¹ç‰¹æ®Šçš„å‰ç¼€è¿›è¡Œæ£€æŸ¥ã€‚T5ä½¿ç”¨ç‰¹æ®Šçš„å‰ç¼€æ¥å‘Šè¯‰æ¨¡å‹å…·ä½“è¦åšçš„ä»»åŠ¡ï¼ˆ"translate English to Romanian: "ï¼‰ï¼Œå…·ä½“å‰ç¼€ä¾‹å­å¦‚ä¸‹ï¼š
```python
if model_checkpoint in ["t5-small", "t5-base", "t5-larg", "t5-3b", "t5-11b"]:
    prefix = "translate English to Romanian: "
else:
    prefix = ""
```
ç°åœ¨æˆ‘ä»¬å¯ä»¥æŠŠä¸Šé¢çš„å†…å®¹æ”¾åœ¨ä¸€èµ·ç»„æˆé¢„å¤„ç†å‡½æ•°preprocess_functionã€‚å¯¹æ ·æœ¬è¿›è¡Œé¢„å¤„ç†çš„æ—¶å€™ï¼Œä½¿ç”¨truncation=Trueå‚æ•°æ¥ç¡®ä¿è¶…é•¿æ–‡æœ¬è¢«æˆªæ–­ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¯¹ä¸æ¯”è¾ƒçŸ­çš„å¥å­ä¼šè‡ªåŠ¨paddingã€‚
```python
max_input_length = 128
max_target_length = 128
source_lang = "en"
target_lang = "ro"

def preprocess_function(examples):
    inputs = [prefix + ex[source_lang] for ex in examples["translation"]]
    targets = [ex[target_lang] for ex in examples["translation"]]
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)

    # Setup the tokenizer for targets
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=max_target_length, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs
```
ä»¥ä¸Šçš„é¢„å¤„ç†å‡½æ•°å¯ä»¥å¤„ç†ä¸€ä¸ªæ ·æœ¬ï¼Œä¹Ÿå¯ä»¥å¤„ç†å¤šä¸ªæ ·æœ¬exapmlesã€‚å¦‚æœæ˜¯å¤„ç†å¤šä¸ªæ ·æœ¬ï¼Œåˆ™è¿”å›çš„æ˜¯å¤šä¸ªæ ·æœ¬è¢«é¢„å¤„ç†ä¹‹åçš„ç»“æœlistã€‚

æ¥ä¸‹æ¥ä½¿ç”¨mapå‡½æ•°å¯¹æ•°æ®é›†datasetsé‡Œé¢ä¸‰ä¸ªæ ·æœ¬é›†åˆçš„æ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œå°†é¢„å¤„ç†å‡½æ•°preprocess_functionåº”ç”¨åˆ°ï¼ˆmap)æ‰€æœ‰æ ·æœ¬ä¸Šã€‚å‚æ•°batched=Trueå¯ä»¥æ‰¹é‡å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ã€‚è¿™æ˜¯ä¸ºäº†å……åˆ†åˆ©ç”¨å‰é¢åŠ è½½fast_tokenizerçš„ä¼˜åŠ¿ï¼Œå®ƒå°†ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘åœ°å¤„ç†æ‰¹ä¸­çš„æ–‡æœ¬ã€‚
```python
tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
```
### åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
åšseq2seqä»»åŠ¡ï¼Œé‚£ä¹ˆéœ€è¦ä¸€ä¸ªèƒ½è§£å†³è¿™ä¸ªä»»åŠ¡çš„æ¨¡å‹ç±»ã€‚æˆ‘ä»¬ä½¿ç”¨`AutoModelForSeq2SeqLM` è¿™ä¸ªç±»ã€‚
```python
from transformers import AutoModelForSeq2SeqLM, 
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
```
#### è®¾å®šè®­ç»ƒå‚æ•°
```
from transformers import Seq2SeqTrainingArguments
batch_size = 16
args = Seq2SeqTrainingArguments(
    "test-translation",
    evaluation_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    save_total_limit=3, #è‡³å¤šä¿å­˜æ¨¡å‹ä¸ªæ•°
    num_train_epochs=1,
    predict_with_generate=True,
    fp16=False,
)
```
#### æ•°æ®æ”¶é›†å™¨data collator
æ¥ä¸‹æ¥éœ€è¦å‘Šè¯‰Trainerå¦‚ä½•ä»é¢„å¤„ç†çš„è¾“å…¥æ•°æ®ä¸­æ„é€ batchã€‚æˆ‘ä»¬ä½¿ç”¨æ•°æ®æ”¶é›†å™¨DataCollatorForSeq2Seqï¼Œå°†ç»é¢„å¤„ç†çš„è¾“å…¥åˆ†batchå†æ¬¡å¤„ç†åå–‚ç»™æ¨¡å‹ã€‚
```python
from transformers import DataCollatorForSeq2Seq
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
```
#### å®šä¹‰è¯„ä¼°æ–¹æ³•
æˆ‘ä»¬ä½¿ç”¨'bleu'æŒ‡æ ‡ï¼Œåˆ©ç”¨metric.computeè®¡ç®—è¯¥æŒ‡æ ‡å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚

metric.computeå¯¹æ¯”predictionså’Œlabelsï¼Œä»è€Œè®¡ç®—å¾—åˆ†ã€‚predictionså’Œlabelséƒ½éœ€è¦æ˜¯ä¸€ä¸ªlistã€‚å…·ä½“æ ¼å¼è§ä¸‹é¢çš„ä¾‹å­:
```python
fake_preds = ["hello there", "general kenobi"]
fake_labels = [["hello there"], ["general kenobi"]]
metric.compute(predictions=fake_preds, references=fake_labels)
#{'bp': 1.0,
# 'counts': [4, 2, 0, 0],
# 'precisions': [100.0, 100.0, 0.0, 0.0],
# 'ref_len': 4,
# 'score': 0.0,
# 'sys_len': 4,
# 'totals': [4, 2, 0, 0]}
# å°†æ¨¡å‹é¢„æµ‹é€å…¥è¯„ä¼°ä¹‹å‰ï¼Œè¿˜éœ€è¦å†™postprocess_textå‡½æ•°åšä¸€äº›æ•°æ®åå¤„ç†ï¼š
import numpy as np
from datasets import load_metric
metric = load_metric("sacrebleu")

def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [[label.strip()] for label in labels]

    return preds, labels

def compute_metrics(eval_preds):
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Some simple post-processing
    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    result = {"bleu": result["score"]}

    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
    result["gen_len"] = np.mean(prediction_lens)
    result = {k: round(v, 4) for k, v in result.items()}
    return result
```
### å¼€å§‹è®­ç»ƒ
```python
from transformers import Seq2SeqTrainer
trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
trainer.train()
```
## ç”Ÿæˆä»»åŠ¡-ç”Ÿæˆæ‘˜è¦
### ä»»åŠ¡ä»‹ç»
æ‘˜è¦ç”Ÿæˆï¼Œç”¨ä¸€äº›ç²¾ç‚¼çš„è¯ï¼ˆæ‘˜è¦ï¼‰æ¥æ¦‚æ‹¬æ•´ç‰‡æ–‡ç« çš„å¤§æ„ï¼Œç”¨æˆ·é€šè¿‡è¯»æ–‡æ‘˜å°±å¯ä»¥äº†è§£åˆ°åŸæ–‡è¦è¡¨è¾¾ã€‚
### æ•°æ®åŠ è½½
#### æ•°æ®é›†ä»‹ç»
æˆ‘ä»¬ä½¿ç”¨XSum datasetæ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«äº†å¤šç¯‡BBCçš„æ–‡ç« å’Œä¸€å¥å¯¹åº”çš„æ‘˜è¦ã€‚
#### åŠ è½½æ•°æ®
```python
pip install datasets transformers rouge-score nltk
#transformers==4.9.2
#datasets==1.11.0
#rouge-score==0.0.4
#nltk==3.6.2
from datasets import load_dataset
raw_datasets = load_dataset("xsum")
raw_datasets["train"][0]
# {'document': 'Recent reports have linked some France-based players with returns to Wales.\n"I\'ve always felt - and this is with my rugby hat on now; this is not region or WRU - I\'d rather spend that money on keeping players in Wales," said Davies.\nThe WRU provides Â£2m to the fund and Â£1.3m comes from the regions.\nFormer Wales and British and Irish Lions fly-half Davies became WRU chairman on Tuesday 21 October, succeeding deposed David Pickering following governing body elections.\nHe is now serving a notice period to leave his role as Newport Gwent Dragons chief executive after being voted on to the WRU board in September.\nDavies was among the leading figures among Dragons, Ospreys, Scarlets and Cardiff Blues officials who were embroiled in a protracted dispute with the WRU that ended in a Â£60m deal in August this year.\nIn the wake of that deal being done, Davies said the Â£3.3m should be spent on ensuring current Wales-based stars remain there.\nIn recent weeks, Racing Metro flanker Dan Lydiate was linked with returning to Wales.\nLikewise the Paris club\'s scrum-half Mike Phillips and centre Jamie Roberts were also touted for possible returns.\nWales coach Warren Gatland has said: "We haven\'t instigated contact with the players.\n"But we are aware that one or two of them are keen to return to Wales sooner rather than later."\nSpeaking to Scrum V on BBC Radio Wales, Davies re-iterated his stance, saying keeping players such as Scarlets full-back Liam Williams and Ospreys flanker Justin Tipuric in Wales should take precedence.\n"It\'s obviously a limited amount of money [available]. The union are contributing 60% of that contract and the regions are putting Â£1.3m in.\n"So it\'s a total pot of just over Â£3m and if you look at the sorts of salaries that the... guys... have been tempted to go overseas for [are] significant amounts of money.\n"So if we were to bring the players back, we\'d probably get five or six players.\n"And I\'ve always felt - and this is with my rugby hat on now; this is not region or WRU - I\'d rather spend that money on keeping players in Wales.\n"There are players coming out of contract, perhaps in the next year or soâ€¦ you\'re looking at your Liam Williams\' of the world; Justin Tipuric for example - we need to keep these guys in Wales.\n"We actually want them there. They are the ones who are going to impress the young kids, for example.\n"They are the sort of heroes that our young kids want to emulate.\n"So I would start off [by saying] with the limited pot of money, we have to retain players in Wales.\n"Now, if that can be done and there\'s some spare monies available at the end, yes, let\'s look to bring players back.\n"But it\'s a cruel world, isn\'t it?\n"It\'s fine to take the buck and go, but great if you can get them back as well, provided there\'s enough money."\nBritish and Irish Lions centre Roberts has insisted he will see out his Racing Metro contract.\nHe and Phillips also earlier dismissed the idea of leaving Paris.\nRoberts also admitted being hurt by comments in French Newspaper L\'Equipe attributed to Racing Coach Laurent Labit questioning their effectiveness.\nCentre Roberts and flanker Lydiate joined Racing ahead of the 2013-14 season while scrum-half Phillips moved there in December 2013 after being dismissed for disciplinary reasons by former club Bayonne.',
# 'id': '29750031',
# 'summary': 'New Welsh Rugby Union chairman Gareth Davies believes a joint Â£3.3m WRU-regions fund should be used to retain home-based talent such as Liam Williams, not bring back exiled stars.'}
```
![](https://raw.githubusercontent.com/innovation64/Picimg/main/20221031102824.png)
### æ•°æ®é¢„å¤„ç†
```python
from transformers import AutoTokenizer
model_checkpoint = "t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)
with tokenizer.as_target_tokenizer():
    print(tokenizer(["Hello, this one sentence!", "This is another sentence."]))
#{'input_ids': [[8774, 6, 48, 80, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}
if model_checkpoint in ["t5-small", "t5-base", "t5-larg", "t5-3b", "t5-11b"]:
    prefix = "summarize: "
else:
    prefix = ""

max_input_length = 1024
max_target_length = 128

def preprocess_function(examples):
    inputs = [prefix + doc for doc in examples["document"]]
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)

    # Setup the tokenizer for targets
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples["summary"], max_length=max_target_length, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
```
### å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹
```python
from transformers import AutoModelForSeq2SeqLM, 
model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
from transformers import Seq2SeqTrainingArguments
batch_size = 16
args = Seq2SeqTrainingArguments(
    "test-summarization",
    evaluation_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    save_total_limit=3,#è‡³å¤šä¿å­˜3ä¸ªæ¨¡å‹
    num_train_epochs=1,
    predict_with_generate=True,
    fp16=True,
)

from transformers import DataCollatorForSeq2Seq
data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)


fake_preds = ["hello there", "general kenobi"]
fake_labels = ["hello there", "general kenobi"]
metric.compute(predictions=fake_preds, references=fake_labels)
#{'rouge1': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),
# 'rouge2': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),
# 'rougeL': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),
# 'rougeLsum': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))}

import nltk
import numpy as np

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    
    # Rouge expects a newline after each sentence
    decoded_preds = ["\n".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds] #æŒ‰å¥å­åˆ†å‰²åæ¢è¡Œç¬¦æ‹¼æ¥
    decoded_labels = ["\n".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]
    
    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)
    # Extract a few results
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    
    # Add mean generated length
    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]
    result["gen_len"] = np.mean(prediction_lens)
    
    return {k: round(v, 4) for k, v in result.items()}

from transformers import Seq2SeqTrainer
trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()
```
## ç”Ÿæˆä»»åŠ¡-è¯­è¨€æ¨¡å‹
### ä»»åŠ¡ä»‹ç»
ä»‹ç»ä¸¤ç±»è¯­è¨€æ¨¡å‹å»ºæ¨¡ä»»åŠ¡
- **å› æœè¯­è¨€æ¨¡å‹ï¼ˆCausal language modelingï¼ŒCLMï¼‰**ï¼šæ¨¡å‹éœ€è¦é¢„æµ‹å¥å­ä¸­çš„ä¸‹ä¸€ä½ç½®å¤„çš„å­—ç¬¦ï¼ˆç±»ä¼¼BERTç±»æ¨¡å‹çš„decoderå’ŒGPTï¼Œä»å·¦å¾€å³è¾“å…¥å­—ç¬¦ï¼‰ã€‚æ¨¡å‹ä¼šä½¿ç”¨çŸ©é˜µå¯¹è§’çº¿attention maskæœºåˆ¶é˜²æ­¢æ¨¡å‹æå‰çœ‹åˆ°ç­”æ¡ˆã€‚ä¾‹å¦‚ï¼Œå½“æ¨¡å‹è¯•å›¾é¢„æµ‹å¥å­ä¸­çš„i+1ä½ç½®å¤„çš„å­—ç¬¦æ—¶ï¼Œè¿™ä¸ªæ©ç å°†é˜»æ­¢å®ƒè®¿é—®ä½ç½®ä¹‹åçš„å­—ç¬¦ã€‚
![](https://ifwind.github.io/2021/08/31/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%887%EF%BC%89%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/image-20210901190347576.png)

- **æ©ç è¯­è¨€å»ºæ¨¡ï¼ˆMasked language modelingï¼ŒMLMï¼‰**ï¼šæ¨¡å‹éœ€è¦æ¢å¤è¾“å…¥ä¸­è¢«"MASK"æ‰çš„ä¸€äº›å­—ç¬¦ï¼ˆBERTç±»æ¨¡å‹çš„é¢„è®­ç»ƒä»»åŠ¡ï¼Œåªç”¨transformerçš„encoderéƒ¨åˆ†ï¼‰ã€‚æ¨¡å‹å¯ä»¥çœ‹åˆ°æ•´ä¸ªå¥å­ï¼Œå› æ­¤æ¨¡å‹å¯ä»¥æ ¹æ®â€œ[MASK]â€æ ‡è®°ä¹‹å‰å’Œä¹‹åçš„å­—ç¬¦æ¥é¢„æµ‹è¯¥ä½ç½®è¢«â€œ[MASK]â€ä¹‹å‰çš„å­—ç¬¦ã€‚
![](https://ifwind.github.io/2021/08/31/BERT%E5%AE%9E%E6%88%98%E2%80%94%E2%80%94%EF%BC%887%EF%BC%89%E7%94%9F%E6%88%90%E4%BB%BB%E5%8A%A1-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/masked_language_modeling.png)
### æ•°æ®åŠ è½½
#### æ•°æ®é›†ä»‹ç»
æˆ‘ä»¬ä½¿ç”¨Wikitext 2æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…æ‹¬äº†ä»Wikipediaä¸Šç»è¿‡éªŒè¯çš„Goodå’ŒFeaturedæ–‡ç« é›†ä¸­æå–çš„è¶…è¿‡1äº¿ä¸ªtokençš„é›†åˆã€‚
#### åŠ è½½æ•°æ®
```python
pip install datasets transformers sacrebleu sentencepiece
#transformers==4.9.2
#datasets==1.11.0
from datasets import load_dataset
datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')
```
![](https://raw.githubusercontent.com/innovation64/Picimg/main/20221031103745.png)

### CLM
#### æ•°æ®é¢„å¤„ç†
```python
from transformers import AutoTokenizer
model_checkpoint = "distilgpt2"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)
```
å¯¹äºå› æœè¯­è¨€æ¨¡å‹(CLM)ï¼Œæˆ‘ä»¬é¦–å…ˆè·å–åˆ°æ•°æ®é›†ä¸­çš„æ‰€æœ‰æ–‡æœ¬å¹¶åˆ†è¯ï¼Œä¹‹åå°†å®ƒä»¬è¿æ¥èµ·æ¥ã€‚æœ€åï¼Œåœ¨ç‰¹å®šåºåˆ—é•¿åº¦çš„ä¾‹å­ä¸­æ‹†åˆ†å®ƒä»¬ï¼Œå°†å„ä¸ªæ‹†åˆ†éƒ¨åˆ†ä½œä¸ºæ¨¡å‹è¾“å…¥ã€‚

é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¨¡å‹å°†æ¥æ”¶å¦‚ä¸‹çš„è¿ç»­æ–‡æœ¬å—ï¼Œ[BOS_TOKEN]ç”¨äºåˆ†å‰²æ‹¼æ¥äº†æ¥è‡ªä¸åŒå†…å®¹çš„æ–‡æœ¬ã€‚
```
è¾“å…¥ç±»å‹1ï¼šæ–‡æœ¬1
è¾“å…¥ç±»å‹2ï¼šæ–‡æœ¬1ç»“å°¾ [BOS_TOKEN] æ–‡æœ¬2å¼€å¤´
```
```python
def tokenize_function(examples):
    return tokenizer(examples["text"])
```
ä½¿ç”¨mapå‡½æ•°å¯¹æ•°æ®é›†datasetsé‡Œé¢ä¸‰ä¸ªæ ·æœ¬é›†åˆçš„æ‰€æœ‰æ ·æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œå°†å‡½æ•°tokenize_functionåº”ç”¨åˆ°ï¼ˆmap)æ‰€æœ‰æ ·æœ¬ä¸Šã€‚ä½¿ç”¨batch=Trueå’Œ4ä¸ªè¿›ç¨‹æ¥åŠ é€Ÿé¢„å¤„ç†ã€‚è¿™æ˜¯ä¸ºäº†å……åˆ†åˆ©ç”¨å‰é¢åŠ è½½fast_tokenizerçš„ä¼˜åŠ¿ï¼Œå®ƒå°†ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘åœ°å¤„ç†æ‰¹ä¸­çš„æ–‡æœ¬ã€‚ä¹‹åæˆ‘ä»¬å¹¶ä¸éœ€è¦textåˆ—ï¼Œæ‰€ä»¥å°†å…¶èˆå¼ƒï¼ˆremove_columnsï¼‰ã€‚
```
tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=["text"])
```
æˆ‘ä»¬éœ€è¦å°†æ‰€æœ‰æ–‡æœ¬è¿æ¥åœ¨ä¸€èµ·ï¼Œç„¶åå°†ç»“æœåˆ†å‰²æˆç‰¹å®šblock_sizeçš„å°å—ã€‚block_sizeè®¾ç½®ä¸ºé¢„è®­ç»ƒæ¨¡å‹æ—¶æ‰€ä½¿ç”¨çš„æœ€å¤§é•¿åº¦ã€‚

ç¼–å†™é¢„å¤„ç†å‡½æ•°æ¥å¯¹æ–‡æœ¬è¿›è¡Œç»„åˆå’Œæ‹†åˆ†ï¼š
```python
# block_size = tokenizer.model_max_length
block_size = 128
def group_texts(examples):
    # æ‹¼æ¥æ‰€æœ‰æ–‡æœ¬
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # æˆ‘ä»¬å°†ä½™æ•°å¯¹åº”çš„éƒ¨åˆ†å»æ‰ã€‚ä½†å¦‚æœæ¨¡å‹æ”¯æŒçš„è¯ï¼Œå¯ä»¥æ·»åŠ paddingï¼Œæ‚¨å¯ä»¥æ ¹æ®éœ€è¦å®šåˆ¶æ­¤éƒ¨ä»¶ã€‚
    total_length = (total_length // block_size) * block_size
    # é€šè¿‡max_lenè¿›è¡Œåˆ†å‰²ã€‚
    result = {
        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
        for k, t in concatenated_examples.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result
```
æ³¨æ„ï¼šå› ä¸ºæˆ‘ä»¬åšçš„æ˜¯å› æœè¯­è¨€æ¨¡å‹ï¼Œå…¶é¢„æµ‹labelå°±æ˜¯å…¶è¾“å…¥çš„input_idï¼Œæ‰€ä»¥æˆ‘ä»¬å¤åˆ¶äº†æ ‡ç­¾çš„è¾“å…¥ã€‚

æˆ‘ä»¬å°†å†æ¬¡ä½¿ç”¨mapæ–¹æ³•ï¼Œbatched=Trueè¡¨ç¤ºå…è®¸é€šè¿‡è¿”å›ä¸åŒæ•°é‡çš„æ ·æœ¬æ¥æ”¹å˜æ•°æ®é›†ä¸­çš„æ ·æœ¬æ•°é‡ï¼Œè¿™æ ·å¯ä»¥ä»ä¸€æ‰¹ç¤ºä¾‹ä¸­åˆ›å»ºæ–°çš„ç¤ºä¾‹ã€‚

æ³¨æ„ï¼Œåœ¨é»˜è®¤æƒ…å†µä¸‹ï¼Œmapæ–¹æ³•å°†å‘é€ä¸€æ‰¹1,000ä¸ªç¤ºä¾‹ï¼Œç”±é¢„å¤„ç†å‡½æ•°å¤„ç†ã€‚å¯ä»¥é€šè¿‡ä¼ é€’ä¸åŒbatch_sizeæ¥è°ƒæ•´ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨num_procæ¥åŠ é€Ÿé¢„å¤„ç†ã€‚
```python
lm_datasets = tokenized_datasets.map(
    group_texts,
    batched=True,
    batch_size=1000,
    num_proc=4,
)
```
æ£€æŸ¥ä¸€ä¸‹æ•°æ®é›†æ˜¯å¦å‘ç”Ÿäº†å˜åŒ–ï¼š
```python
tokenizer.decode(lm_datasets["train"][1]["input_ids"])
```
### å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹
```python
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(model_checkpoint)
```
#### è®¾å®šè®­ç»ƒå‚æ•°
ä¸ºäº†èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ªTrainerè®­ç»ƒå·¥å…·ï¼Œæˆ‘ä»¬è¿˜éœ€è¦è®­ç»ƒçš„è®¾å®š/å‚æ•° TrainingArgumentsã€‚è¿™ä¸ªè®­ç»ƒè®¾å®šåŒ…å«äº†èƒ½å¤Ÿå®šä¹‰è®­ç»ƒè¿‡ç¨‹çš„æ‰€æœ‰å±æ€§ã€‚
```python
from transformers import TrainingArguments
batch_size = 16
training_args = TrainingArguments(
    "test-clm",
    evaluation_strategy = "epoch",
    learning_rate=2e-5,
    weight_decay=0.01,
)
```
å®Œæˆè¯¥ä»»åŠ¡ä¸éœ€è¦ç‰¹åˆ«å®šä¹‰è¯„ä¼°æŒ‡æ ‡å¤„ç†å‡½æ•°ï¼Œæ¨¡å‹å°†ç›´æ¥è®¡ç®—å›°æƒ‘åº¦perplexityä½œä¸ºè¯„ä¼°æŒ‡æ ‡ã€‚
#### å¼€å§‹è®­ç»ƒ
```python
from transformers import Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=lm_datasets["train"][:1000],
    eval_dataset=lm_datasets["validation"][:1000],
)
trainer.train()
```
#### è¯„ä¼°æ¨¡å‹
```
import math
eval_results = trainer.evaluate()
print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
```
### MLM 
```python
from transformers import AutoTokenizer
model_checkpoint = "distilroberta-base"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)
```
å¯¹äºæ©ç è¯­è¨€æ¨¡å‹(MLM)ï¼Œæˆ‘ä»¬é¦–å…ˆè·å–åˆ°æ•°æ®é›†ä¸­çš„æ‰€æœ‰æ–‡æœ¬å¹¶åˆ†è¯ï¼Œä¹‹åå°†å®ƒä»¬è¿æ¥èµ·æ¥ï¼Œæ¥ç€åœ¨ç‰¹å®šåºåˆ—é•¿åº¦çš„ä¾‹å­ä¸­æ‹†åˆ†å®ƒä»¬ã€‚ä¸å› æœè¯­è¨€æ¨¡å‹ä¸åŒçš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨æ‹†åˆ†åè¿˜éœ€è¦éšæœº"MASK"ä¸€äº›å­—ç¬¦(ä½¿ç”¨"[MASK]"è¿›è¡Œæ›¿æ¢)ä»¥åŠè°ƒæ•´æ ‡ç­¾ä¸ºåªåŒ…å«åœ¨"[MASK]"ä½ç½®å¤„çš„æ ‡ç­¾(å› ä¸ºæˆ‘ä»¬ä¸éœ€è¦é¢„æµ‹æ²¡æœ‰è¢«"MASK"çš„å­—ç¬¦)ï¼Œæœ€åå°†å„ä¸ªç»æ©ç çš„æ‹†åˆ†éƒ¨åˆ†ä½œä¸ºæ¨¡å‹è¾“å…¥ã€‚
```python
def tokenize_function(examples):
    return tokenizer(examples["text"])
tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=["text"])

# block_size = tokenizer.model_max_length
block_size = 128
def group_texts(examples):
    # æ‹¼æ¥æ‰€æœ‰æ–‡æœ¬
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # æˆ‘ä»¬å°†ä½™æ•°å¯¹åº”çš„éƒ¨åˆ†å»æ‰ã€‚ä½†å¦‚æœæ¨¡å‹æ”¯æŒçš„è¯ï¼Œå¯ä»¥æ·»åŠ paddingï¼Œæ‚¨å¯ä»¥æ ¹æ®éœ€è¦å®šåˆ¶æ­¤éƒ¨ä»¶ã€‚
    total_length = (total_length // block_size) * block_size
    # é€šè¿‡max_lenè¿›è¡Œåˆ†å‰²ã€‚
    result = {
        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
        for k, t in concatenated_examples.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result
```
æ³¨æ„ï¼šæˆ‘ä»¬è¿™é‡Œä»ç„¶å¤åˆ¶äº†æ ‡ç­¾çš„è¾“å…¥ä½œä¸ºlabelï¼Œå› ä¸ºæ©ç è¯­è¨€æ¨¡å‹çš„æœ¬è´¨è¿˜æ˜¯é¢„æµ‹åŸæ–‡ï¼Œè€Œæ©ç åœ¨data collatorä¸­é€šè¿‡æ·»åŠ ç‰¹åˆ«çš„å‚æ•°è¿›è¡Œå¤„ç†ï¼Œä¸‹æ–‡ä¼šç€é‡è¯´æ˜ã€‚

æˆ‘ä»¬å°†å†æ¬¡ä½¿ç”¨mapæ–¹æ³•ï¼Œbatched=Trueè¡¨ç¤ºå…è®¸é€šè¿‡è¿”å›ä¸åŒæ•°é‡çš„æ ·æœ¬æ¥æ”¹å˜æ•°æ®é›†ä¸­çš„æ ·æœ¬æ•°é‡ï¼Œè¿™æ ·å¯ä»¥ä»ä¸€æ‰¹ç¤ºä¾‹ä¸­åˆ›å»ºæ–°çš„ç¤ºä¾‹ã€‚

æ³¨æ„ï¼Œåœ¨é»˜è®¤æƒ…å†µä¸‹ï¼Œmapæ–¹æ³•å°†å‘é€ä¸€æ‰¹1,000ä¸ªç¤ºä¾‹ï¼Œç”±é¢„å¤„ç†å‡½æ•°å¤„ç†ã€‚å¯ä»¥é€šè¿‡ä¼ é€’ä¸åŒbatch_sizeæ¥è°ƒæ•´ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨num_procæ¥åŠ é€Ÿé¢„å¤„ç†
```python
lm_datasets = tokenized_datasets.map(
    group_texts,
    batched=True,
    batch_size=1000,
    num_proc=4,
)
```
#### å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹
ä¸ºäº†èƒ½å¤Ÿå¾—åˆ°ä¸€ä¸ªTrainerè®­ç»ƒå·¥å…·ï¼Œæˆ‘ä»¬è¿˜éœ€è¦è®­ç»ƒçš„è®¾å®š/å‚æ•° TrainingArgumentsã€‚è¿™ä¸ªè®­ç»ƒè®¾å®šåŒ…å«äº†èƒ½å¤Ÿå®šä¹‰è®­ç»ƒè¿‡ç¨‹çš„æ‰€æœ‰å±æ€§ã€‚
```python
from transformers import TrainingArguments
batch_size = 16
training_args = TrainingArguments(
    "test-clm",
    evaluation_strategy = "epoch",
    learning_rate=2e-5,
    weight_decay=0.01,
)
```
#### æ•°æ®æ”¶é›†å™¨data collator
data_collatoræ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œè´Ÿè´£è·å–æ ·æœ¬å¹¶å°†å®ƒä»¬æ‰¹å¤„ç†æˆå¼ é‡ã€‚data_collatorè´Ÿè´£è·å–æ ·æœ¬å¹¶å°†å®ƒä»¬æ‰¹å¤„ç†æˆå¼ é‡ã€‚æ©ç è¯­è¨€æ¨¡å‹ä»»åŠ¡éœ€è¦ä½¿ç”¨ä¸€ä¸ªç‰¹æ®Šçš„data_collatorï¼Œç”¨äºéšæœº"MASK"å¥å­ä¸­çš„tokenã€‚

æ³¨æ„ï¼šæˆ‘ä»¬å¯ä»¥å°†MASKä½œä¸ºé¢„å¤„ç†æ­¥éª¤(tokenizer)è¿›è¡Œå¤„ç†ï¼Œä½†tokenizeråœ¨æ¯ä¸ªé˜¶æ®µå­—ç¬¦æ€»æ˜¯ä»¥ç›¸åŒçš„æ–¹å¼è¢«æ©ç›–ã€‚è€Œé€šè¿‡åœ¨data_collatorä¸­æ‰§è¡Œè¿™ä¸€æ­¥ï¼Œå¯ä»¥ç¡®ä¿æ¯æ¬¡ç”Ÿæˆæ•°æ®æ—¶éƒ½ä»¥æ–°çš„æ–¹å¼å®Œæˆæ©ç ï¼ˆéšæœºï¼‰ã€‚

ä¸ºäº†å®ç°éšæœºmaskï¼ŒTransformersä¸ºæ©ç è¯­è¨€æ¨¡å‹æä¾›äº†ä¸€ä¸ªç‰¹æ®Šçš„DataCollatorForLanguageModelingã€‚å¯ä»¥é€šè¿‡mlm_probabilityè°ƒæ•´æ©ç çš„æ¦‚ç‡ï¼š
```python
from transformers import DataCollatorForLanguageModeling
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)
```
#### å¼€å§‹è®­ç»ƒ
```python
from transformers import Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=lm_datasets["train"][:1000],
    eval_dataset=lm_datasets["validation"][:100],
    data_collator=data_collator,
)
trainer.train()
import math
eval_results = trainer.evaluate()
print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
```