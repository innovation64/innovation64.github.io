---
tags: LLM
---
# llama from scratch
## Transformers vs llama
![](https://raw.githubusercontent.com/innovation64/Picimg/main/20240109000528.png)

## normalization choice

Layer normalization or Root mean squear normalization

## Attention mechanism
![](https://raw.githubusercontent.com/innovation64/Picimg/main/20240109233802.png)

## Rotary Position Embedding
![](https://raw.githubusercontent.com/innovation64/Picimg/main/20240109234322.png)

![](https://raw.githubusercontent.com/innovation64/Picimg/main/20240109234541.png)

![](https://raw.githubusercontent.com/innovation64/Picimg/main/20240109234717.png)

![](https://raw.githubusercontent.com/innovation64/Picimg/main/20240109235008.png)
## self attention

![](https://raw.githubusercontent.com/innovation64/Picimg/main/20240110001644.png)

# KV cache
![](https://raw.githubusercontent.com/innovation64/Picimg/main/20240110002355.png)

![](https://raw.githubusercontent.com/innovation64/Picimg/main/20240110002759.png)

## GPU problem
![](https://raw.githubusercontent.com/innovation64/Picimg/main/20240110003429.png)

## Multi query attention
![](https://raw.githubusercontent.com/innovation64/Picimg/main/20240110003759.png)

![](https://raw.githubusercontent.com/innovation64/Picimg/main/20240110003917.png)

![](https://raw.githubusercontent.com/innovation64/Picimg/main/20240110004041.png)

