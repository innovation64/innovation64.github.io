---
tags: papers
---
# 经典论文回顾：RNN &LSTM
>2015 NYU,GOOGLE
- RECURRENT NEURAL NETWORK REGULARIZATION

这篇文章教怎么正确用 dropout 到 LSTMs上
并且持续性的降低多任务的过拟合现象
- 语言模型
- 语音识别
- 图片生成
- 机器翻译


对于FFN， dropout是最强力的正则化方法

过去的 RNN 通常太小，因为太大过拟合

代码仓库: https://github.com/wojzaremba/lstm.


![](https://raw.githubusercontent.com/innovation64/Picimg/main/20240610171442.png)

![](https://raw.githubusercontent.com/innovation64/Picimg/main/20240610171506.png)

![](https://raw.githubusercontent.com/innovation64/Picimg/main/20240610171555.png)


- Relational recurrent neural networks

>2018


