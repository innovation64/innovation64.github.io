---
tags: GNN
---
# Graph Neural Networks

## Outline of Today's Lecture
- Basics of deep learning
- Deep learning for graphs
- Graph Convoultional Networks and GraphSAGE

## Machine Learning as Optimization
- Supervised learning 

## Loss Function Example

## Minibatch SGD
- Concepts:
    - Batch size : the number of data points in a minibatch
    - Iteration: 1 step of SGD on a minibatch
    - Epoch: one full pass over the dataset (iterations is equal to ratio of dataset size and batch size)
- SGD is unbiased estimator of full gradient

## Milti-layer  perceptron(MLP)

# Deep learning of Graph

## Local network neighborhoods
## Stacking multiple layers

- A Native Approach
- Idea : Convolutional Networks

## Real-World Graphs

## Neighborhood Aggregation

- The Math : Deep Encoder
    - Basic approach : Average neighbor messages and apply a neural network

## Summary
-  Recap :Generate node embeddings by aggregating neighborhood information