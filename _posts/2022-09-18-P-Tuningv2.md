---
tags: Paper
---
# P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks

提示调优仅使用冻结的语言模型调整连续提示，大大减少了训练时每个任务的存储和内存使用量。 然而，在 NLU 的背景下，先前的工作表明，对于正常大小的预训练模型，快速调优效果不佳。 我们还发现，现有的快速调整方法无法处理硬序列标记任务，表明缺乏普遍性。 我们提出了一个新的经验发现，即适当优化的即时调整可以在广泛的模型尺度和 NLU 任务中普遍有效。 它与微调的性能相匹配，但只有 0.1%-3% 的调整参数。 我们的方法 P-Tuning v2 是针对 NLU 优化和调整的 Deep Prompt Tuning（Li 和 Liang，2021；Qin 和 Eisner，2021）的实现。 鉴于 P-Tuning v2 的普遍性和简单性，我们相信它可以作为微调的替代方案和未来研究的强大基线。

## Introduction
- prompting
- prompt tuning

properly optimized prompt
tuning can be comparable to fine-tuning universally
across various model scales and NLU tasks.

Deep Prompt Tuning

## Preliminaries
- NLU Tasks
    - simple classification tasks
    - hard sequence labeling tasks

## P-Tuning v2
### Lack of Universality
- across scales
- across tasks

### Deep Prompt Tuning
continuous prompts are only inserted into the input embedding sequence. This leads to two challenges :
- first

the number of tunable
parameters is limited due to the constraints of sequence
length
- Second

the input embeddings have
relatively indirect impact on model predictions.
### Optimization and Implementation
- Reparameterization( such as MLP)
- Prompt Length

    simple classification tasks prefer shorter prompts (less than20)  

    hard sequence labeling tasks prefer longer ones (around 100)

#### Multi-task Learning

#### Classification Head

## Experiments

### P-tuning v2: Across Scales

P-tuning v2 even significantly
outperforms fine-tuning on RTE.

P-tuning v2 is always comparable
to fine-tuning at all scales but with only
0.1% task-specific parameters needed comparing
to fine-tuning.

我们提出了 P-tuning v2，一种快速调整方法。
尽管其技术新颖性相对有限，但它促成了一个新发现，即快速调整可以与跨尺度（从 330M 到 10B 参数）和任务的普遍微调相媲美。
凭借高精度和参数效率，PTuning v2 可以成为微调的潜在替代方案和未来工作的强大基线。