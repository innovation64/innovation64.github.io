---
tags: GNN
---
# A Single Layer of GNN
## A GNN Layer
GNN Layer = Message + Aggregation
- Message Computation
    -Message function: m~u~^(l)^ = MSG^(l)^(h~u~^(l-1)^)
        - intuition: Each node will create a message , which will be sent to other nodes later
- Aggregation
  - intuition :Each node will aggregate the messages from node v's neighbors
  > h~v~^(l)^ = AGG^(l)^({m~u~^(l)^,u  $\in$ N(v)}) 

## Message Aggregation : Issue 
Information from node v itself could get lost

## GraphSAGE Neighbor Aggregation
- Mean : Take a weighted average of neighbors
- Pool : Transform neighbor vectors and apply symmetric vector function Mean(·) or Max(·) 
- LSTM : Apply LSTM to reshuffled of neighbors
### GraphSAGE : l2 Normalization

## GAT
- Graph Attention Networks
> h~v~^(l)^ = $\sigma$($\sum_{(u /in  N(v))}^{} {\alpha~uv W^(l)h~u^(l-1)}$)

- In GCN/GraphSAGE
## Attention Mechanism
- Normalize 
- Weighted sum
- Multi-head attention

## Dropout 
Goal: regularize a neural net to prevent overfitting

Dropout for GNNS
In GNN , Dropout is applied to the linear layer in the message function

![](https://s2.loli.net/2022/01/11/I5PXjfMw3ZLuCyE.png)

# Stacking Layers of a GNN

## Stacking GNN Layers

## Receptive Field of a GNN
- Receptive field: the set of nodes that determine the embedding of a node of interest

- Receptive Field & Over-smoothing
we can explain over-smoothing via the notion of receptive field

## Design GNN Layer Connectivity

## Expressive Power for shallow GNNs

## Idea of Skip Connections
[markdown符号大全](https://www.zybuluo.com/codeep/note/163962#10%E5%A6%82%E4%BD%95%E8%BE%93%E5%85%A5%E7%B4%AF%E5%8A%A0%E7%B4%AF%E4%B9%98%E8%BF%90%E7%AE%97)