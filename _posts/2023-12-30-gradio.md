---
tags: LLM
---
# gradio
## NLP Tasks interface
åŠ è½½HFAPIå’Œç›¸å…³åº“
```python
import os
import io
from IPython.display import Image, display, HTML
from PIL import Image
import base64 
from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv()) # read local .env file
hf_api_key = os.environ['HF_API_KEY']
```

```python
# Helper function
import requests, json

#Summarization endpoint
def get_completion(inputs, parameters=None,ENDPOINT_URL=os.environ['HF_API_SUMMARY_BASE']): 
    headers = {
      "Authorization": f"Bearer {hf_api_key}",
      "Content-Type": "application/json"
    }
    data = { "inputs": inputs }
    if parameters is not None:
        data.update({"parameters": parameters})
    response = requests.request("POST",
                                ENDPOINT_URL, headers=headers,
                                data=json.dumps(data)
                               )
    return json.loads(response.content.decode("utf-8"))
```

### æ€ä¹ˆæœ¬åœ°è¿è¡Œ
è·ŸAPIå·®ä¸å¤šï¼Œå…·ä½“çœ‹[pipelineæµç¨‹](https://huggingface.co/docs/transformers/main_classes/pipelines)

```python
from transformers import pipeline

get_completion = pipeline("summarization", model="shleifer/distilbart-cnn-12-6")

def summarize(input):
    output = get_completion(input)
    return output[0]['summary_text']
```

### æ„å»ºä¸€ä¸ªæ–‡æœ¬æ€»ç»“APP
```python
text = ('''The tower is 324 metres (1,063 ft) tall, about the same height
        as an 81-storey building, and the tallest structure in Paris. 
        Its base is square, measuring 125 metres (410 ft) on each side. 
        During its construction, the Eiffel Tower surpassed the Washington 
        Monument to become the tallest man-made structure in the world,
        a title it held for 41 years until the Chrysler Building
        in New York City was finished in 1930. It was the first structure 
        to reach a height of 300 metres. Due to the addition of a broadcasting 
        aerial at the top of the tower in 1957, it is now taller than the 
        Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the 
        Eiffel Tower is the second tallest free-standing structure in France 
        after the Millau Viaduct.''')

get_completion(text)
```

#### ä½¿ç”¨ Gradio gr.Interface å¯åŠ¨
```python

import gradio as gr
def summarize(input):
    output = get_completion(input)
    return output[0]['summary_text']
    
gr.close_all()
demo = gr.Interface(fn=summarize, inputs="text", outputs="text")
demo.launch(share=True, server_port=int(os.environ['PORT1']))
```
demo.launch(share=True) å¯ä»¥åˆ†äº«å…¬å…±è¿æ¥

```python
import gradio as gr

def summarize(input):
    output = get_completion(input)
    return output[0]['summary_text']

gr.close_all()
demo = gr.Interface(fn=summarize, 
                    inputs=[gr.Textbox(label="Text to summarize", lines=6)],
                    outputs=[gr.Textbox(label="Result", lines=3)],
                    title="Text summarization with distilbart-cnn",
                    description="Summarize any text using the `shleifer/distilbart-cnn-12-6` model under the hood!"
                   )
demo.launch(share=True, server_port=int(os.environ['PORT2']))
```

#### æ„æ¶ä¸€ä¸ªåå­—å®ä½“è¯†åˆ«APP
æˆ‘ä»¬ç”¨dslim/bert-base-NERçš„æ¨ç†ç«¯ç‚¹ï¼Œä¸€ä¸ª108Må‚æ•°çš„é’ˆå¯¹NERå¾®è°ƒçš„BARTæ¨¡å‹

- æ€ä¹ˆæœ¬åœ°è¿è¡Œ
```python

from transformers import pipeline

get_completion = pipeline("ner", model="dslim/bert-base-NER")

def ner(input):
    output = get_completion(input)
    return {"text": input, "entities": output}

```

```python
API_URL = os.environ['HF_API_NER_BASE'] #NER endpoint
text = "My name is Andrew, I'm building DeepLearningAI and I live in California"
get_completion(text, parameters=None, ENDPOINT_URL= API_URL)
```
gr.interface()

- æ³¨æ„è¿™é‡Œçš„ input å’Œ output éƒ½ä¼ é€’äº†ä¸€ä¸ª[]ï¼Œå› ä¸ºner()ä¸æ­¢ä¸€ä¸ªè¾“å…¥è¾“å‡º
- è¾“å…¥è¾“å‡ºçš„ä¼ é€’è®°å¾—å¯¹åº”enr()å‡½æ•°å‚æ•°å’Œè¾“å‡º

```python
def ner(input):
    output = get_completion(input, parameters=None, ENDPOINT_URL=API_URL)
    return {"text": input, "entities": output}

gr.close_all()
demo = gr.Interface(fn=ner,
                    inputs=[gr.Textbox(label="Text to find entities", lines=2)],
                    outputs=[gr.HighlightedText(label="Text with entities")],
                    title="NER with dslim/bert-base-NER",
                    description="Find entities using the `dslim/bert-base-NER` model under the hood!",
                    allow_flagging="never",
                    #Here we introduce a new tag, examples, easy to use examples for your application
                    examples=["My name is Andrew and I live in California", "My name is Poli and work at HuggingFace"])
demo.launch(share=True, server_port=int(os.environ['PORT3']))
```
#### æ·»åŠ è¾…åŠ©å‡½æ•°åˆå¹¶token
```python
def merge_tokens(tokens):
    merged_tokens = []
    for token in tokens:
        if merged_tokens and token['entity'].startswith('I-') and merged_tokens[-1]['entity'].endswith(token['entity'][2:]):
            # If current token continues the entity of the last one, merge them
            last_token = merged_tokens[-1]
            last_token['word'] += token['word'].replace('##', '')
            last_token['end'] = token['end']
            last_token['score'] = (last_token['score'] + token['score']) / 2
        else:
            # Otherwise, add the token to the list
            merged_tokens.append(token)

    return merged_tokens

def ner(input):
    output = get_completion(input, parameters=None, ENDPOINT_URL=API_URL)
    merged_tokens = merge_tokens(output)
    return {"text": input, "entities": merged_tokens}

gr.close_all()
demo = gr.Interface(fn=ner,
                    inputs=[gr.Textbox(label="Text to find entities", lines=2)],
                    outputs=[gr.HighlightedText(label="Text with entities")],
                    title="NER with dslim/bert-base-NER",
                    description="Find entities using the `dslim/bert-base-NER` model under the hood!",
                    allow_flagging="never",
                    examples=["My name is Andrew, I'm building DeeplearningAI and I live in California", "My name is Poli, I live in Vienna and work at HuggingFace"])

demo.launch(share=True, server_port=int(os.environ['PORT4']))
```
```python
gr.close_all()
```
### æ€ä¹ˆè·å–è‡ªå·±çš„HF API Key

#### å®‰å…¨ä½¿ç”¨è‡ªå·±çš„key
- åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹åˆ›å»º`.env`æ–‡ä»¶
- æ–‡ä»¶åŒ…å«
`HF_API_KEY="ABC!@#XXX"`
- ä¿å­˜æ–‡ä»¶
- å®‰è£… `Python-dotenv`,ä½¿å¾—åœ¨jupyterä¸‹ç¬¬ä¸€æ¬¡cellè¿è¡Œ
```bash
pip install python-dotenv
```
## åˆ›å»ºå›¾ç‰‡æ•è·APP
```python
import os
import io
import IPython.display
from PIL import Image
import base64 
from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv()) # read local .env file
hf_api_key = os.environ['HF_API_KEY']
```
```python
# Helper functions
import requests, json

#Image-to-text endpoint
def get_completion(inputs, parameters=None, ENDPOINT_URL=os.environ['HF_API_ITT_BASE']):
    headers = {
      "Authorization": f"Bearer {hf_api_key}",
      "Content-Type": "application/json"
    }
    data = { "inputs": inputs }
    if parameters is not None:
        data.update({"parameters": parameters})
    response = requests.request("POST",
                                ENDPOINT_URL,
                                headers=headers,
                                data=json.dumps(data))
    return json.loads(response.content.decode("utf-8"))
```
### æ„å»ºå›¾ç‰‡æ•è·APP
`Salesforce/blip-image-captioning-base`ï¼Œä¸€ä¸ª14Må‚æ•°çš„æ¨¡å‹
```python
image_url = "https://free-images.com/sm/9596/dog_animal_greyhound_983023.jpg"
display(IPython.display.Image(url=image_url))
get_completion(image_url)
```

gr.Interface()

- ç±»å‹å‚æ•°æ˜¯FNå‡½æ•°æœŸæœ›æ¥æ”¶åˆ°å…¶è¾“å…¥çš„æ ¼å¼ã€‚å¦‚æœç±»å‹ä¸ºnumpyæˆ–pilï¼Œåˆ™Gr.imageï¼ˆï¼‰å°†å°†ä¸Šä¼ çš„æ–‡ä»¶è½¬æ¢ä¸ºæ­¤æ ¼å¼ï¼Œç„¶åå°†å…¶å‘é€åˆ°FNå‡½æ•°ã€‚

- å¦‚æœç±»å‹æ˜¯filepathï¼Œåˆ™Gr.imageï¼ˆï¼‰å°†ä¸´æ—¶å­˜å‚¨å›¾åƒï¼Œå¹¶ä¸ºè¯¥å›¾åƒä½ç½®æä¾›å­—ç¬¦ä¸²è·¯å¾„ä½œä¸ºFNå‡½æ•°çš„è¾“å…¥ã€‚

```python
import gradio as gr 

def image_to_base64_str(pil_image):
    byte_arr = io.BytesIO()
    pil_image.save(byte_arr, format='PNG')
    byte_arr = byte_arr.getvalue()
    return str(base64.b64encode(byte_arr).decode('utf-8'))

def captioner(image):
    base64_image = image_to_base64_str(image)
    result = get_completion(base64_image)
    return result[0]['generated_text']

gr.close_all()
demo = gr.Interface(fn=captioner,
                    inputs=[gr.Image(label="Upload image", type="pil")],
                    outputs=[gr.Textbox(label="Caption")],
                    title="Image Captioning with BLIP",
                    description="Caption any image using the BLIP model",
                    allow_flagging="never",
                    examples=["christmas_dog.jpeg", "bird_flight.jpeg", "cow.jpeg"])

demo.launch(share=True, server_port=int(os.environ['PORT1']))
```

## å›¾ç‰‡ç”ŸæˆAPP
```python
import os
import io
import IPython.display
from PIL import Image
import base64 
from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv()) # read local .env file
hf_api_key = os.environ['HF_API_KEY']
# Helper function
import requests, json

#Text-to-image endpoint
def get_completion(inputs, parameters=None, ENDPOINT_URL=os.environ['HF_API_TTI_BASE']):
    headers = {
      "Authorization": f"Bearer {hf_api_key}",
      "Content-Type": "application/json"
    }   
    data = { "inputs": inputs }
    if parameters is not None:
        data.update({"parameters": parameters})
    response = requests.request("POST",
                                ENDPOINT_URL,
                                headers=headers,
                                data=json.dumps(data))
    return json.loads(response.content.decode("utf-8"))
```
### æ„å»ºå›¾ç‰‡ç”ŸæˆAPP

`runwayml/stable-diffusion-v1-5`,`ğŸ§¨ diffusers`åº“é‡Œçš„

```python
prompt = "a dog in a park"

result = get_completion(prompt)
IPython.display.HTML(f'<img src="data:image/png;base64,{result}" />')
```

#### gr.Interface()

```python
import gradio as gr 

#A helper function to convert the PIL image to base64
#so you can send it to the API
def base64_to_pil(img_base64):
    base64_decoded = base64.b64decode(img_base64)
    byte_stream = io.BytesIO(base64_decoded)
    pil_image = Image.open(byte_stream)
    return pil_image

def generate(prompt):
    output = get_completion(prompt)
    result_image = base64_to_pil(output)
    return result_image

gr.close_all()
demo = gr.Interface(fn=generate,
                    inputs=[gr.Textbox(label="Your prompt")],
                    outputs=[gr.Image(label="Result")],
                    title="Image Generation with Stable Diffusion",
                    description="Generate any image with Stable Diffusion",
                    allow_flagging="never",
                    examples=["the spirit of a tamagotchi wandering in the city of Vienna","a mecha robot in a favela"])

demo.launch(share=True, server_port=int(os.environ['PORT1']))
```
#### é«˜çº§äº¤äº’
```python
import gradio as gr 

#A helper function to convert the PIL image to base64 
# so you can send it to the API
def base64_to_pil(img_base64):
    base64_decoded = base64.b64decode(img_base64)
    byte_stream = io.BytesIO(base64_decoded)
    pil_image = Image.open(byte_stream)
    return pil_image

def generate(prompt, negative_prompt, steps, guidance, width, height):
    params = {
        "negative_prompt": negative_prompt,
        "num_inference_steps": steps,
        "guidance_scale": guidance,
        "width": width,
        "height": height
    }
    
    output = get_completion(prompt, params)
    pil_image = base64_to_pil(output)
    return pil_image
```

#### gr.Slider()

- å¯è®¾ç½® minimum,maximum å’Œå¼€å§‹å€¼
- å¸Œæœ›æ»‘å—æ•´æ•°å¢åŠ ï¼Œå¯è®¾ç½® step=1

```python
gr.close_all()
demo = gr.Interface(fn=generate,
                    inputs=[
                        gr.Textbox(label="Your prompt"),
                        gr.Textbox(label="Negative prompt"),
                        gr.Slider(label="Inference Steps", minimum=1, maximum=100, value=25,
                                 info="In how many steps will the denoiser denoise the image?"),
                        gr.Slider(label="Guidance Scale", minimum=1, maximum=20, value=7, 
                                  info="Controls how much the text prompt influences the result"),
                        gr.Slider(label="Width", minimum=64, maximum=512, step=64, value=512),
                        gr.Slider(label="Height", minimum=64, maximum=512, step=64, value=512),
                    ],
                    outputs=[gr.Image(label="Result")],
                    title="Image Generation with Stable Diffusion",
                    description="Generate any image with Stable Diffusion",
                    allow_flagging="never"
                    )

demo.launch(share=True, server_port=int(os.environ['PORT2']))
```

#### gr.Blocks()
åœ¨gr.blocksï¼ˆï¼‰ä¸­ï¼Œæ‚¨å¯ä»¥å®šä¹‰å¤šä¸ªgr.rowï¼ˆï¼‰sæˆ–å¤šä¸ªgr.columnï¼ˆï¼‰sã€‚

è¯·æ³¨æ„ï¼Œå¦‚æœJupyterç¬”è®°æœ¬éå¸¸ç‹­çª„ï¼Œåˆ™å¸ƒå±€å¯èƒ½ä¼šæ›´æ”¹ä»¥æ›´å¥½åœ°æ˜¾ç¤ºå¯¹è±¡ã€‚å¦‚æœæ‚¨å®šä¹‰äº†ä¸¤åˆ—ä½†çœ‹ä¸åˆ°åº”ç”¨ç¨‹åºä¸­çš„ä¸¤ä¸ªåˆ—ï¼Œè¯·å°è¯•æ‰©å±•Webæµè§ˆå™¨çš„å®½åº¦ï¼Œä»¥åŠåŒ…å«æ­¤Jupyterç¬”è®°æœ¬çš„å±å¹•ã€‚

ä½¿ç”¨gr.blocksï¼ˆï¼‰æ—¶ï¼Œæ‚¨éœ€è¦ä½¿ç”¨gr.buttonï¼ˆï¼‰æ˜ç¡®å®šä¹‰â€œæäº¤â€æŒ‰é’®ï¼Œè€Œå½“ä½¿ç”¨gr.interfaceï¼ˆï¼‰æ—¶ï¼Œå°†è‡ªåŠ¨æ·»åŠ 'clear''å’Œ'æäº¤'æŒ‰é’®ã€‚

```python
with gr.Blocks() as demo:
    gr.Markdown("# Image Generation with Stable Diffusion")
    prompt = gr.Textbox(label="Your prompt")
    with gr.Row():
        with gr.Column():
            negative_prompt = gr.Textbox(label="Negative prompt")
            steps = gr.Slider(label="Inference Steps", minimum=1, maximum=100, value=25,
                      info="In many steps will the denoiser denoise the image?")
            guidance = gr.Slider(label="Guidance Scale", minimum=1, maximum=20, value=7,
                      info="Controls how much the text prompt influences the result")
            width = gr.Slider(label="Width", minimum=64, maximum=512, step=64, value=512)
            height = gr.Slider(label="Height", minimum=64, maximum=512, step=64, value=512)
            btn = gr.Button("Submit")
        with gr.Column():
            output = gr.Image(label="Result")

    btn.click(fn=generate, inputs=[prompt,negative_prompt,steps,guidance,width,height], outputs=[output])
gr.close_all()
demo.launch(share=True, server_port=int(os.environ['PORT3']))
```

#### è§„æ¨¡

è¦é€‰æ‹©è¦ç»™æ¯ä¸€åˆ—çš„ç›¸å¯¹å®½åº¦å¤šå°‘ï¼Œè¯·è®¾ç½®æ¯ä¸ªgr.columnï¼ˆï¼‰çš„æ¯”ä¾‹å‚æ•°ã€‚

å¦‚æœä¸€åˆ—çš„æ¯”ä¾‹= 4ï¼Œè€Œç¬¬äºŒåˆ—çš„æ¯”ä¾‹= 1ï¼Œåˆ™ç¬¬ä¸€åˆ—å æ€»å®½åº¦çš„4/5ï¼Œç¬¬äºŒåˆ—å æ€»å®½åº¦çš„1/5ã€‚

#### gr.Accordionï¼ˆï¼‰

gr.accordionï¼ˆï¼‰å¯ä»¥ä½¿ç”¨é¼ æ ‡å•å‡»æ˜¾ç¤º/éšè—åº”ç”¨ç¨‹åºé€‰é¡¹ã€‚

è®¾ç½®open = trueé»˜è®¤æƒ…å†µä¸‹æ˜¾ç¤ºæ‰‹é£ç´çš„å†…å®¹ï¼Œæˆ–falseé»˜è®¤å°†å…¶éšè—ã€‚

```python
with gr.Blocks() as demo:
    gr.Markdown("# Image Generation with Stable Diffusion")
    with gr.Row():
        with gr.Column(scale=4):
            prompt = gr.Textbox(label="Your prompt") #Give prompt some real estate
        with gr.Column(scale=1, min_width=50):
            btn = gr.Button("Submit") #Submit button side by side!
    with gr.Accordion("Advanced options", open=False): #Let's hide the advanced options!
            negative_prompt = gr.Textbox(label="Negative prompt")
            with gr.Row():
                with gr.Column():
                    steps = gr.Slider(label="Inference Steps", minimum=1, maximum=100, value=25,
                      info="In many steps will the denoiser denoise the image?")
                    guidance = gr.Slider(label="Guidance Scale", minimum=1, maximum=20, value=7,
                      info="Controls how much the text prompt influences the result")
                with gr.Column():
                    width = gr.Slider(label="Width", minimum=64, maximum=512, step=64, value=512)
                    height = gr.Slider(label="Height", minimum=64, maximum=512, step=64, value=512)
    output = gr.Image(label="Result") #Move the output up too
            
    btn.click(fn=generate, inputs=[prompt,negative_prompt,steps,guidance,width,height], outputs=[output])

gr.close_all()
demo.launch(share=True, server_port=int(os.environ['PORT4']))
```

## æ„å»ºæ¸¸æˆ
```python
import os
import io
from IPython.display import Image, display, HTML
from PIL import Image
import base64 

from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv()) # read local .env file
hf_api_key = os.environ['HF_API_KEY']

#### Helper function
import requests, json

#Here we are going to call multiple endpoints!
def get_completion(inputs, parameters=None, ENDPOINT_URL=""):
    headers = {
      "Authorization": f"Bearer {hf_api_key}",
      "Content-Type": "application/json"
    }   
    data = { "inputs": inputs }
    if parameters is not None:
        data.update({"parameters": parameters})
    response = requests.request("POST",
                                ENDPOINT_URL,
                                headers=headers,
                                data=json.dumps(data))
    return json.loads(response.content.decode("utf-8"))

#text-to-image
TTI_ENDPOINT = os.environ['HF_API_TTI_BASE']
#image-to-text
ITT_ENDPOINT = os.environ['HF_API_ITT_BASE']

```

### gr.Blocks()æ„å»ºæ¸¸æˆ
```python
#Bringing the functions from lessons 3 and 4!
def image_to_base64_str(pil_image):
    byte_arr = io.BytesIO()
    pil_image.save(byte_arr, format='PNG')
    byte_arr = byte_arr.getvalue()
    return str(base64.b64encode(byte_arr).decode('utf-8'))

def base64_to_pil(img_base64):
    base64_decoded = base64.b64decode(img_base64)
    byte_stream = io.BytesIO(base64_decoded)
    pil_image = Image.open(byte_stream)
    return pil_image

def captioner(image):
    base64_image = image_to_base64_str(image)
    result = get_completion(base64_image, None, ITT_ENDPOINT)
    return result[0]['generated_text']

def generate(prompt):
    output = get_completion(prompt, None, TTI_ENDPOINT)
    result_image = base64_to_pil(output)
    return result_image
```

#### é¦–æ¬¡å°è¯•æ•è·

```python
import gradio as gr 
with gr.Blocks() as demo:
    gr.Markdown("# Describe-and-Generate game ğŸ–ï¸")
    image_upload = gr.Image(label="Your first image",type="pil")
    btn_caption = gr.Button("Generate caption")
    caption = gr.Textbox(label="Generated caption")
    
    btn_caption.click(fn=captioner, inputs=[image_upload], outputs=[caption])

gr.close_all()
demo.launch(share=True, server_port=int(os.environ['PORT1']))
```

#### å¢åŠ ç”Ÿæˆ
```python
with gr.Blocks() as demo:
    gr.Markdown("# Describe-and-Generate game ğŸ–ï¸")
    image_upload = gr.Image(label="Your first image",type="pil")
    btn_caption = gr.Button("Generate caption")
    caption = gr.Textbox(label="Generated caption")
    btn_image = gr.Button("Generate image")
    image_output = gr.Image(label="Generated Image")
    btn_caption.click(fn=captioner, inputs=[image_upload], outputs=[caption])
    btn_image.click(fn=generate, inputs=[caption], outputs=[image_output])

gr.close_all()
demo.launch(share=True, server_port=int(os.environ['PORT2']))
```

#### ä¸€æ¬¡æ€§åšå®Œ
```python
def caption_and_generate(image):
    caption = captioner(image)
    image = generate(caption)
    return [caption, image]

with gr.Blocks() as demo:
    gr.Markdown("# Describe-and-Generate game ğŸ–ï¸")
    image_upload = gr.Image(label="Your first image",type="pil")
    btn_all = gr.Button("Caption and generate")
    caption = gr.Textbox(label="Generated caption")
    image_output = gr.Image(label="Generated Image")

    btn_all.click(fn=caption_and_generate, inputs=[image_upload], outputs=[caption, image_output])

gr.close_all()
demo.launch(share=True, server_port=int(os.environ['PORT3']))
```

## æ„å»º ChatBot

```python
import os
import io
import IPython.display
from PIL import Image
import base64 
import requests 
requests.adapters.DEFAULT_TIMEOUT = 60

from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv()) # read local .env file
hf_api_key = os.environ['HF_API_KEY']

# Helper function
import requests, json
from text_generation import Client

#FalcomLM-instruct endpoint on the text_generation library
client = Client(os.environ['HF_API_FALCOM_BASE'], headers={"Authorization": f"Basic {hf_api_key}"}, timeout=120)

```

### åœ¨ä»»ä½•LLMä¸Šæ„å»ºchatbot
`falcon-40b-instruct` 
```python
prompt = "Has math been invented or discovered?"
client.generate(prompt, max_new_tokens=256).generated_text
```
```python
#Back to Lesson 2, time flies!
import gradio as gr
def generate(input, slider):
    output = client.generate(input, max_new_tokens=slider).generated_text
    return output

demo = gr.Interface(fn=generate, 
                    inputs=[gr.Textbox(label="Prompt"), 
                            gr.Slider(label="Max new tokens", 
                                      value=20,  
                                      maximum=1024, 
                                      minimum=1)], 
                    outputs=[gr.Textbox(label="Completion")])

gr.close_all()
demo.launch(share=True, server_port=int(os.environ['PORT1']))
```

### gr.Chatbot()


- `gr.Chatbot()`å…è®¸æ‚¨ä¿å­˜ç”¨æˆ·å’ŒLLMä¹‹é—´çš„èŠå¤©å†å²ï¼Œå¹¶åœ¨åº”ç”¨ç¨‹åºä¸­æ˜¾ç¤ºå¯¹è¯ã€‚
- å®šä¹‰æ‚¨çš„`fn`ä»¥æ¥æ”¶ä¸€ä¸ª`gr.Chatbot()`å¯¹è±¡ã€‚
  - åœ¨æ‚¨å®šä¹‰çš„`fn`å‡½æ•°ä¸­ï¼Œæ·»åŠ ä¸€ä¸ªåŒ…å«ç”¨æˆ·æ¶ˆæ¯å’ŒLLMå“åº”çš„å…ƒç»„ï¼ˆæˆ–åˆ—è¡¨ï¼‰ï¼š
`chatbot_object.append( (user_message, llm_message) )`

- åœ¨åº”ç”¨ç¨‹åºçš„è¾“å…¥å’Œè¾“å‡ºä¸­éƒ½åŒ…å«èŠå¤©æœºå™¨äººå¯¹è±¡ã€‚

```python
import random

def respond(message, chat_history):
        #No LLM here, just respond with a random pre-made message
        bot_message = random.choice(["Tell me more about it", 
                                     "Cool, but I'm not interested", 
                                     "Hmmmm, ok then"]) 
        chat_history.append((message, bot_message))
        return "", chat_history

with gr.Blocks() as demo:
    chatbot = gr.Chatbot(height=240) #just to fit the notebook
    msg = gr.Textbox(label="Prompt")
    btn = gr.Button("Submit")
    clear = gr.ClearButton(components=[msg, chatbot], value="Clear console")

    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])
    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit

gr.close_all()
demo.launch(share=True, server_port=int(os.environ['PORT2']))
```

#### ä½¿ç”¨èŠå¤©å†å²æ ¼å¼åŒ–æç¤º

- æ‚¨å¯ä»¥ä½¿ç”¨forå¾ªç¯éå†èŠå¤©æœºå™¨äººå¯¹è±¡ã€‚
- æ¯ä¸ªé¡¹ç›®éƒ½æ˜¯ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«ç”¨æˆ·æ¶ˆæ¯å’ŒLLMçš„æ¶ˆæ¯ã€‚

```Python
for turn in chat_history:
    user_msg, bot_msg = turn
    ...
```

```python
def format_chat_prompt(message, chat_history):
    prompt = ""
    for turn in chat_history:
        user_message, bot_message = turn
        prompt = f"{prompt}\nUser: {user_message}\nAssistant: {bot_message}"
    prompt = f"{prompt}\nUser: {message}\nAssistant:"
    return prompt

def respond(message, chat_history):
        formatted_prompt = format_chat_prompt(message, chat_history)
        bot_message = client.generate(formatted_prompt,
                                     max_new_tokens=1024,
                                     stop_sequences=["\nUser:", "<|endoftext|>"]).generated_text
        chat_history.append((message, bot_message))
        return "", chat_history

with gr.Blocks() as demo:
    chatbot = gr.Chatbot(height=240) #just to fit the notebook
    msg = gr.Textbox(label="Prompt")
    btn = gr.Button("Submit")
    clear = gr.ClearButton(components=[msg, chatbot], value="Clear console")

    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])
    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit

gr.close_all()
demo.launch(share=True, server_port=int(os.environ['PORT3']))
```
#### æ·»åŠ é«˜çº§åŠŸèƒ½

```python
def format_chat_prompt(message, chat_history, instruction):
    prompt = f"System:{instruction}"
    for turn in chat_history:
        user_message, bot_message = turn
        prompt = f"{prompt}\nUser: {user_message}\nAssistant: {bot_message}"
    prompt = f"{prompt}\nUser: {message}\nAssistant:"
    return prompt
```

### æµå¼å¤„ç†

- å¦‚æœæ‚¨çš„LLMå¯ä»¥ä¸€æ¬¡æä¾›ä¸€ä¸ªä»¤ç‰Œæµï¼Œæ‚¨å¯ä»¥åœ¨èŠå¤©æœºå™¨äººå¯¹è±¡ä¸­ç´¯ç§¯è¿™äº›ä»¤ç‰Œã€‚
- ä¸‹é¢å‡½æ•°ä¸­çš„`for`å¾ªç¯éå†æµä¸­çš„æ‰€æœ‰ä»¤ç‰Œï¼Œå¹¶å°†å®ƒä»¬è¿½åŠ åˆ°èŠå¤©æœºå™¨äººæ¶ˆæ¯å†å²ä¸­æœ€è¿‘çš„å¯¹è¯è½®æ¬¡ä¸­ã€‚

```python
def respond(message, chat_history, instruction, temperature=0.7):
    prompt = format_chat_prompt(message, chat_history, instruction)
    chat_history = chat_history + [[message, ""]]
    stream = client.generate_stream(prompt,
                                      max_new_tokens=1024,
                                      stop_sequences=["\nUser:", "<|endoftext|>"],
                                      temperature=temperature)
                                      #stop_sequences to not generate the user answer
    acc_text = ""
    #Streaming the tokens
    for idx, response in enumerate(stream):
            text_token = response.token.text

            if response.details:
                return

            if idx == 0 and text_token.startswith(" "):
                text_token = text_token[1:]

            acc_text += text_token
            last_turn = list(chat_history.pop(-1))
            last_turn[-1] += acc_text
            chat_history = chat_history + [last_turn]
            yield "", chat_history
            acc_text = ""
```

```python
with gr.Blocks() as demo:
    chatbot = gr.Chatbot(height=240) #just to fit the notebook
    msg = gr.Textbox(label="Prompt")
    with gr.Accordion(label="Advanced options",open=False):
        system = gr.Textbox(label="System message", lines=2, value="A conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.")
        temperature = gr.Slider(label="temperature", minimum=0.1, maximum=1, value=0.7, step=0.1)
    btn = gr.Button("Submit")
    clear = gr.ClearButton(components=[msg, chatbot], value="Clear console")

    btn.click(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot])
    msg.submit(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot]) #Press enter to submit

gr.close_all()
demo.queue().launch(share=True, server_port=int(os.environ['PORT4'])) 
```